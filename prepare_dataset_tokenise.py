# -*- coding: utf-8 -*-
"""prepare_dataset_tokenise.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1cE3e5mxq9wRnz1vKUDhSLTIErnrEnBJF
"""

from google.colab import drive
# Mount Google Drive
drive.mount('/content/drive')

import torch
print(torch.cuda.is_available())  # Should return True
print(torch.cuda.current_device())  # Should return the device index
print(torch.cuda.get_device_name(torch.cuda.current_device()))  # Shou

# Commented out IPython magic to ensure Python compatibility.
# %%capture
# !pip install unsloth
# # Also get the latest nightly Unsloth!
# !pip uninstall unsloth -y && pip install --upgrade --no-cache-dir "unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git"

!export PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True

import os
os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"

!pip install -Uq flash_attn

!export TORCH_CUDA_ARCH_LIST="7.0;8.0"  # Replace with your architecture(s)
!pip install flash-attn --no-build-isolation

!pip install -q transformers==4.44.2
!pip install -Uq -q accelerate peft bitsandbytes trl dataset bitsandbytes
!pip install -q -U accelerate

!wget -c -q "https://huggingface.co/datasets/liuhaotian/LLaVA-Instruct-150K/resolve/main/llava_instruct_150k.json"

import os
import numpy as np
import torch
import torch.nn as nn
from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments, Trainer, WhisperProcessor, WhisperForConditionalGeneration, PreTrainedModel
from peft import prepare_model_for_kbit_training, LoraConfig, get_peft_model, TaskType
from datasets import Dataset, DatasetDict
from tqdm import tqdm
import json
import librosa
from dataclasses import dataclass
from typing import Any, Dict, List, Union

# Utility functions
def load_embeddings(file_path):
    data = np.load(file_path)
    print(f"Contents of {file_path}:")
    for key in data.files:
        print(f"- {key}: shape {data[key].shape}")

    if len(data.files) == 1:
        combined_data = data[data.files[0]]
        return {
            'ids': combined_data[:, 0],
            'embeddings': combined_data[:, 1:]
        }
    elif 'image_ids' in data and 'embeddings' in data:
        return {
            'ids': data['image_ids'],
            'embeddings': data['embeddings']
        }
    else:
        raise ValueError(f"Unexpected structure in {file_path}. Please check the file contents.")

# Model Definitions
class ProjectionBlock(nn.Module):
    def __init__(self, input_dim, output_dim):
        super().__init__()
        self.pre_norm = nn.LayerNorm(input_dim)
        self.proj = nn.Sequential(
            nn.Linear(input_dim, output_dim),
            nn.GELU(),
            nn.Linear(output_dim, output_dim)
        )
    def forward(self, x):
        x = self.pre_norm(x)
        return self.proj(x)

class MultimodalProjector(nn.Module):
    def __init__(self, image_input_dim, audio_input_dim, output_dim=512):
        super().__init__()
        self.image_proj = ProjectionBlock(image_input_dim, output_dim)
        self.audio_proj = ProjectionBlock(audio_input_dim, output_dim)

    def forward(self, image_embedding=None, audio_embedding=None):
        projected_image = self.image_proj(image_embedding) if image_embedding is not None else None
        projected_audio = self.audio_proj(audio_embedding) if audio_embedding is not None else None
        return projected_image, projected_audio

class Phi3WithProjector(PreTrainedModel):
    def __init__(self, config, phi3_model, projector):
        super().__init__(config)
        self.config = config
        self.phi3_model = phi3_model
        self.projector = projector

    def forward(
        self,
        input_ids=None,
        attention_mask=None,
        inputs_embeds=None,
        image_embeddings=None,
        audio_embeddings=None,  # Handle audio here
        labels=None,
        output_attentions=None,
        output_hidden_states=None,
        return_dict=None,
        **kwargs
    ):
        # Project both image and audio embeddings
        projected_image, projected_audio = self.projector(image_embeddings, audio_embeddings)

        if inputs_embeds is None:
            inputs_embeds = self.phi3_model.transformer.wte(input_ids)

        # Concatenate embeddings if available
        embeddings_to_concat = [inputs_embeds]
        if projected_image is not None:
            embeddings_to_concat.append(projected_image.unsqueeze(1))
        if projected_audio is not None:
            embeddings_to_concat.append(projected_audio.unsqueeze(1))

        combined_embeddings = torch.cat(embeddings_to_concat, dim=1)
        extended_attention_mask = torch.cat([attention_mask] +
                                            [torch.ones_like(attention_mask[:, :1])] * (len(embeddings_to_concat) - 1), dim=1)

        outputs = self.phi3_model(
            inputs_embeds=combined_embeddings,
            attention_mask=extended_attention_mask,
            labels=labels,
            output_attentions=output_attentions,
            output_hidden_states=output_hidden_states,
            return_dict=return_dict,
            **kwargs
        )
        return outputs

    def get_input_embeddings(self):
        return self.phi3_model.get_input_embeddings()

    def set_input_embeddings(self, value):
        self.phi3_model.set_input_embeddings(value)

    def get_output_embeddings(self):
        return self.phi3_model.get_output_embeddings()

    def set_output_embeddings(self, new_embeddings):
        self.phi3_model.set_output_embeddings(new_embeddings)

    def _set_gradient_checkpointing(self, module, value=False):
        if isinstance(module, (nn.Linear,)):
            module.gradient_checkpointing = value

    def prepare_inputs_for_generation(self, input_ids, past=None, attention_mask=None, **kwargs):
        inputs = self.phi3_model.prepare_inputs_for_generation(input_ids, past=past, attention_mask=attention_mask, **kwargs)
        inputs['image_embeddings'] = kwargs.get('image_embeddings', None)
        inputs['audio_embeddings'] = kwargs.get('audio_embeddings', None)  # Add audio embeddings here
        return inputs

    def _reorder_cache(self, past, beam_idx):
        return self.phi3_model._reorder_cache(past, beam_idx)

@dataclass
class MultimodalDataCollator:
    tokenizer: Any

    def __call__(self, features: List[Dict[str, Any]]) -> Dict[str, Any]:
        batch = {}

        input_ids = [feature["input_ids"] for feature in features]
        batch["input_ids"] = self.tokenizer.pad(
            {"input_ids": input_ids},
            padding=True,
            return_tensors="pt",
        )["input_ids"]

        batch["attention_mask"] = torch.ones_like(batch["input_ids"])
        batch["labels"] = batch["input_ids"].clone()

        if "image_embeddings" in features[0]:
            batch["image_embeddings"] = torch.stack([
                feature["image_embeddings"] for feature in features
            ])

        if "audio_embeddings" in features[0]:
            batch["audio_embeddings"] = torch.stack([
                feature["audio_embeddings"] for feature in features
            ])

        return batch

# Whisper Setup for Audio Transcription
whisper_model_name = "openai/whisper-small"
whisper_processor = WhisperProcessor.from_pretrained(whisper_model_name)
whisper_model = WhisperForConditionalGeneration.from_pretrained(whisper_model_name)

def transcribe_speech(audiopath):
    try:
        speech, rate = librosa.load(audiopath, sr=16000)
        audio_input = whisper_processor(speech, return_tensors="pt", sampling_rate=16000)

        with torch.no_grad():
            generated_ids = whisper_model.generate(audio_input["input_features"])

        transcription = whisper_processor.batch_decode(generated_ids, skip_special_tokens=True)[0]
    except Exception as e:
        print(f"Error transcribing audio: {e}")
        transcription = None  # Handle the case where transcription fails or is unavailable

    return transcription

def prepare_example(examples, tokenizer):
    image_embeddings = []
    audio_embeddings = []
    tokenized_inputs = []

    for idx, conv in enumerate(examples['conversation']):
        image_embedding = torch.tensor(examples['image_embedding'][idx]) if examples['image_embedding'][idx] is not None else None
        audio_embedding = None  # Initialize audio_embedding as None

        transcription = None  # Initialize transcription as None

        # Handle audio transcription if available
        if 'audio_path' in examples and examples['audio_path'][idx]:
            transcription = transcribe_speech(examples['audio_path'][idx])  # Transcribe audio
            audio_embedding = torch.tensor(examples['audio_embedding'][idx]) if 'audio_embedding' in examples else None

        for i in range(0, len(conv), 2):
            if i + 1 < len(conv):
                human_msg = conv[i]['value'].replace('<image>', '').replace('<audio>', '').strip()
                gpt_msg = conv[i + 1]['value']

                # Append transcription to human message if available
                if transcription:
                    human_msg += f"\nAudio Transcription: {transcription}"

                dialogue = [
                    {"role": "system", "content": "You are a helpful assistant."},
                    {"role": "user", "content": f"Given the following information, provide a detailed and accurate response:\n{human_msg}\n[Image is provided for this task.]\n"},
                    {"role": "assistant", "content": gpt_msg}
                ]

                tokenized_input = tokenizer.apply_chat_template(dialogue, return_tensors='pt', padding=True)
                tokenized_inputs.append(tokenized_input.squeeze(0))

                if image_embedding is not None:
                    image_embeddings.append(image_embedding)
                if audio_embedding is not None:
                    audio_embeddings.append(audio_embedding)

    max_length = max(input.shape[0] for input in tokenized_inputs)
    padded_inputs = [torch.nn.functional.pad(input, (0, max_length - input.shape[0])) for input in tokenized_inputs]

    result = {
        "input_ids": torch.stack(padded_inputs),
        "attention_mask": torch.stack(padded_inputs).ne(tokenizer.pad_token_id).long(),
        "labels": torch.stack(padded_inputs).clone()
    }

    if image_embeddings:
        result["image_embeddings"] = torch.stack(image_embeddings)
    if audio_embeddings:
        result["audio_embeddings"] = torch.stack(audio_embeddings)

    return result

def prepare_dataset(image_embeddings_path, dataset_path, cache_dir=None):
    # Load image embeddings
    try:
        image_embeddings = load_embeddings(image_embeddings_path)
    except Exception as e:
        print(f"Error loading image embeddings: {e}")
        print("Continuing without image embeddings...")
        image_embeddings = None

    # Load dataset
    with open(dataset_path, 'r') as f:
        data = json.load(f)

    # Create dataset
    processed_data = []
    for item in tqdm(data):
        processed_item = {'conversation': item['conversations']}

        if image_embeddings is not None and 'image' in item:
            image_id = item['image']
            if isinstance(image_embeddings['ids'][0], str):
                index = np.where(image_embeddings['ids'] == image_id)[0]
            else:
                index = np.where(image_embeddings['ids'] == int(image_id))[0]

            if len(index) > 0:
                processed_item['image_embedding'] = image_embeddings['embeddings'][index[0]]

        if 'audio' in item:
            processed_item['audio_path'] = item['audio']

        processed_data.append(processed_item)

    dataset = Dataset.from_dict({
        "conversation": [item['conversation'] for item in processed_data],
        "image_embedding": [item.get('image_embedding') for item in processed_data],
        "audio_path": [item.get('audio_path') for item in processed_data]
    })

    # Tokenizer for Phi-3
    tokenizer = AutoTokenizer.from_pretrained("microsoft/Phi-3-mini-128k-instruct", trust_remote_code=True)
    tokenizer.pad_token = tokenizer.eos_token
    tokenizer.padding_side = "right"

    # Set the chat template
    tokenizer.chat_template = """
    {% for message in messages %}
    {% if message.role == 'system' %}
    <|system|>{{message.content}}<|endoftext|>
    {% elif message.role == 'user' %}
    <|user|>{{message.content}}<|endoftext|>
    {% elif message.role == 'assistant' %}
    <|assistant|>{{message.content}}<|endoftext|>
    {% endif %}
    {% endfor %}
    """

    # Prepare dataset
    prepared_dataset = dataset.map(
        lambda examples: prepare_example(examples, tokenizer),
        batched=True,
        remove_columns=dataset.column_names,
        batch_size=1  # Process one item at a time to avoid memory issues
    ).with_format("torch")

    # Split dataset
    # train_test_split = prepared_dataset.train_test_split(test_size=0.2)
    # Create a DatasetDict object first to use train_valid_test_split
    dataset_dict_initial = DatasetDict({"data": prepared_dataset})
    train_valid_test_split = dataset_dict_initial["data"].train_test_split(test_size=0.2)  # Initial split
    test_valid_split = train_valid_test_split['test'].train_test_split(test_size=0.5)  # Split test and validation

    dataset_dict = DatasetDict({
        'train': train_valid_test_split['train'],
        'test': test_valid_split['test'],
        'validation': test_valid_split['train']
    })

    # Cache the prepared dataset if cache_dir is provided
    if cache_dir:
        os.makedirs(cache_dir, exist_ok=True)
        dataset_dict.save_to_disk(cache_dir)

    return dataset_dict, tokenizer

def train_model(dataset_dict, tokenizer, output_dir):
    # Load Phi-3 model
    phi3_model = AutoModelForCausalLM.from_pretrained(
        "microsoft/Phi-3-mini-128k-instruct",
        trust_remote_code=True,
        # # torch_dtype=torch.bfloat16,
        # device_map="auto"
    )
    phi3_model.gradient_checkpointing_enable()
    phi3_model = prepare_model_for_kbit_training(phi3_model)
    phi3_model.gradient_checkpointing_enable()
    phi3_model.enable_input_require_grads()
    # Initialize projector and combined model
    image_input_dim = 512  # Adjust this based on your image embeddings
    audio_input_dim = 512  # Adjust this for your audio embeddings
    projector = MultimodalProjector(image_input_dim=phi3_model.config.hidden_size, audio_input_dim=512)

    model = Phi3WithProjector(phi3_model.config, phi3_model, projector)

    # Set up LoRA
    lora_config = LoraConfig(
        r=16,
        lora_alpha=16,
        target_modules=["self_attn.qkv_proj", "self_attn.o_proj", "mlp.gate_up_proj", "mlp.down_proj"],  # Adjust based on Phi-3 architecture
        lora_dropout=0,
        bias="none",
        task_type=TaskType.CAUSAL_LM
    )
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")#
    with torch.device(device):
        phi3_model = AutoModelForCausalLM.from_pretrained(
            "microsoft/Phi-3-mini-128k-instruct",
            trust_remote_code=True,
            _attn_implementation = "eager",
            torch_dtype=torch.bfloat16,  # Optional: use bfloat16 for lower memory usage
            # device_map="auto"  # Optional: let Hugging Face handle device placement
        )



    model = get_peft_model(model, lora_config)
    # #Ensure model is on the GPU
    # model = model.to(device)

    # Enable gradient checkpointing on specific layers of the Phi3 model
    for name, module in phi3_model.named_modules():
        if isinstance(module, (nn.Linear,)):  # Target Linear layers for checkpointing
            model._set_gradient_checkpointing(module, value=True)


    # Move model to device
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model = model.to(device)



    # Training arguments
    training_args = TrainingArguments(
        output_dir=output_dir,
        num_train_epochs=3,
        per_device_train_batch_size=1,
        gradient_accumulation_steps=1,
        gradient_checkpointing=True,
        optim="paged_adamw_32bit",

        learning_rate=2e-4,
        fp16=True,
        save_total_limit=3,
        logging_steps=100,
        save_steps=500,
        eval_steps=500,
        evaluation_strategy="steps",
        load_best_model_at_end=True,
    )

    # Initialize trainer
    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=dataset_dict['train'],
        eval_dataset=dataset_dict['test'],
        tokenizer=tokenizer,
        gradient_checkpointing="unsloth",
        data_collator=MultimodalDataCollator(tokenizer),
    )
    # Before the training loop
    # Clear cache before training
    gc.collect()
    torch.cuda.empty_cache()
    # Start training
    trainer.train()

    # Save the fine-tuned model
    trainer.save_model(output_dir)

if __name__ == "__main__":
    image_embeddings_path = "/content/drive/MyDrive/LLM/Copy of clip_embeddings_instruct150k_15Oct_2024.npz"
    dataset_path = "/content/llava_instruct_150k.json"
    output_dir = "/content/drive/MyDrive/LLM_training_output"
    whisper_model_name = "openai/whisper-small"
    cache_dir = "/content/"


    # Prepare dataset (run this only once)
    dataset_dict, tokenizer = prepare_dataset(image_embeddings_path, dataset_path, cache_dir)

import torch
import gc
import torch
torch.cuda.empty_cache()
import gc
# del variables
gc.collect()
print(torch.cuda.memory_allocated())
print(torch.cuda.memory_reserved())
# Train model (can be run multiple times with different parameters)
train_model(dataset_dict, tokenizer, output_dir)



device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# Ensure model is on the GPU
model = model.to(device)

# Sample data (assuming batch is your input data from a dataloader)
input_ids = batch['input_ids'].to(device)  # Move input_ids to GPU
image_embeddings = batch['image_embeddings'].to(device) if 'image_embeddings' in batch else None
audio_embeddings = batch['audio_embeddings'].to(device) if 'audio_embeddings' in batch else None

# Confirm the device of input tensors
print(input_ids.device)  # Should print "cuda:0"

# !export PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True

torch.cuda.empty_cache()

import torch
print(torch.cuda.is_available())  # Should return True
print(torch.cuda.current_device())  # Should return the device index
print(torch.cuda.get_device_name(torch.cuda.current_device()))

# device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
# model = model.to(device)

import torch
print(f"Allocated memory: {torch.cuda.memory_allocated() / (1024**3)} GB")
print(f"Reserved memory: {torch.cuda.memory_reserved() / (1024**3)} GB")

import torch
torch.cuda.empty_cache()

import numpy as np

file_path = "/content/drive/MyDrive/LLM/Copy of clip_embeddings_instruct150k_15Oct_2024.npz"

data = np.load(file_path, allow_pickle=True)

# List all the keys in the .npz file
print("Keys in the .npz file:", data.files)

# For each key, print the shape of the stored arrays
for key in data.files:
    print(f"Key: {key}, Shape: {data[key].shape}")

data









# phi2_multimodal_finetuning.py

import os
import numpy as np
import torch
import torch.nn as nn
from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments, Trainer, WhisperProcessor, WhisperForConditionalGeneration, PreTrainedModel
from peft import prepare_model_for_kbit_training, LoraConfig, get_peft_model, TaskType
from datasets import Dataset, DatasetDict
from tqdm import tqdm
import json
import librosa
from dataclasses import dataclass
from typing import Any, Dict, List, Union

# Dependencies:
# pip install transformers peft datasets tqdm torch numpy

# Utility functions
def load_embeddings(file_path):
    data = np.load(file_path, allow_pickle=True)
    print(f"Contents of {file_path}:")
    for key in data.files:
        print(f"- {key}: shape {data[key].shape}")

    if len(data.files) == 1:
        combined_data = data[data.files[0]]
        return {
            'ids': combined_data[:, 0],
            'embeddings': combined_data[:, 1:]
        }
    elif 'ids' in data and 'embeddings' in data:
        return {
            'ids': data['ids'],
            'embeddings': data['embeddings']
        }
    else:
        raise ValueError(f"Unexpected structure in {file_path}. Please check the file contents.")

# Model Definitions
class ProjectionBlock(nn.Module):
    def __init__(self, input_dim, output_dim):
        super().__init__()
        self.pre_norm = nn.LayerNorm(input_dim)
        self.proj = nn.Sequential(
            nn.Linear(input_dim, output_dim),
            nn.GELU(),
            nn.Linear(output_dim, output_dim)
        )
    def forward(self, x):
        x = self.pre_norm(x)
        return self.proj(x)

class MultimodalProjector(nn.Module):
    def __init__(self, image_input_dim, output_dim):
        super().__init__()
        self.image_proj = ProjectionBlock(image_input_dim, output_dim)

    def forward(self, image_embedding=None):
        return self.image_proj(image_embedding) if image_embedding is not None else None

class Phi3WithProjector(PreTrainedModel):
    def __init__(self, config, phi3_model, projector):
        super().__init__(config)
        self.config = config
        self.phi3_model = phi3_model
        self.projector = projector

    def forward(
        self,
        input_ids=None,
        attention_mask=None,
        inputs_embeds=None,
        image_embeddings=None,
        labels=None,
        output_attentions=None,
        output_hidden_states=None,
        return_dict=None,
        **kwargs
    ):
        projected_embeddings = self.projector(image_embeddings)

        if inputs_embeds is None:
            inputs_embeds = self.phi3_model.transformer.wte(input_ids)

        if projected_embeddings is not None:
            combined_embeddings = torch.cat([inputs_embeds, projected_embeddings.unsqueeze(1)], dim=1)
            if attention_mask is not None:
                extended_attention_mask = torch.cat([attention_mask, torch.ones((attention_mask.shape[0], 1), device=attention_mask.device)], dim=1)
            else:
                extended_attention_mask = None
        else:
            combined_embeddings = inputs_embeds
            extended_attention_mask = attention_mask

        outputs = self.phi3_model(
            inputs_embeds=combined_embeddings,
            attention_mask=extended_attention_mask,
            labels=labels,
            output_attentions=output_attentions,
            output_hidden_states=output_hidden_states,
            return_dict=return_dict,
            **kwargs
        )
        return outputs

    def get_input_embeddings(self):
        return self.phi3_model.get_input_embeddings()

    def set_input_embeddings(self, value):
        self.phi3_model.set_input_embeddings(value)

    def get_output_embeddings(self):
        return self.phi3_model.get_output_embeddings()

    def set_output_embeddings(self, new_embeddings):
        self.phi3_model.set_output_embeddings(new_embeddings)

    def prepare_inputs_for_generation(self, input_ids, past=None, attention_mask=None, **kwargs):
        inputs = self.phi3_model.prepare_inputs_for_generation(input_ids, past=past, attention_mask=attention_mask, **kwargs)
        inputs['image_embeddings'] = kwargs.get('image_embeddings', None)
        return inputs

    def _reorder_cache(self, past, beam_idx):
        return self.phi3_model._reorder_cache(past, beam_idx)

@dataclass
class MultimodalDataCollator:
    tokenizer: Any

    def __call__(self, features: List[Dict[str, Any]]) -> Dict[str, Any]:
        batch = {}

        input_ids = [feature["input_ids"] for feature in features]
        batch["input_ids"] = self.tokenizer.pad(
            {"input_ids": input_ids},
            padding=True,
            return_tensors="pt",
        )["input_ids"]

        batch["attention_mask"] = torch.ones_like(batch["input_ids"])
        batch["labels"] = batch["input_ids"].clone()

        if "image_embeddings" in features[0]:
            batch["image_embeddings"] = torch.stack([
                feature["image_embeddings"] for feature in features
            ])

        return batch

whisper_model_name = "openai/whisper-base"
whisper_processor = WhisperProcessor.from_pretrained(whisper_model_name)
whisper_model = WhisperForConditionalGeneration.from_pretrained(whisper_model_name)

def transcribe_speech(audiopath):
    speech, rate = librosa.load(audiopath, sr=16000)
    audio_input = whisper_processor(speech, return_tensors="pt", sampling_rate=16000)

    with torch.no_grad():
        generated_ids = whisper_model.generate(audio_input["input_features"])

    transcription = whisper_processor.batch_decode(generated_ids, skip_special_tokens=True)[0]

    return transcription

def prepare_example(examples, tokenizer):
    image_embeddings = []
    tokenized_inputs = []

    for idx, conv in enumerate(examples['conversation']):
        image_embedding = torch.tensor(examples['image_embedding'][idx]) if examples['image_embedding'][idx] is not None else None

        for i in range(0, len(conv), 2):
            if i + 1 < len(conv):
                human_msg = conv[i]['value'].replace('<image>', '').replace('<audio>', '').strip()
                gpt_msg = conv[i + 1]['value']

                if 'audio_path' in examples and examples['audio_path'][idx]:
                    transcription = transcribe_speech(examples['audio_path'][idx])
                    human_msg += f"\nAudio Transcription: {transcription}"

                dialogue = [
                    {"role": "system", "content": "You are a helpful assistant."},
                    {"role": "user", "content": f"Given the following information, provide a detailed and accurate response:\n{human_msg}\n[Image is provided for this task.]\n"},
                    {"role": "assistant", "content": gpt_msg}
                ]

                tokenized_input = tokenizer.apply_chat_template(dialogue, return_tensors='pt', padding=True)
                tokenized_inputs.append(tokenized_input.squeeze(0))
                if image_embedding is not None:
                    image_embeddings.append(image_embedding)

    max_length = max(input.shape[0] for input in tokenized_inputs)
    padded_inputs = [torch.nn.functional.pad(input, (0, max_length - input.shape[0])) for input in tokenized_inputs]

    result = {
        "input_ids": torch.stack(padded_inputs),
        "attention_mask": torch.stack(padded_inputs).ne(tokenizer.pad_token_id).long(),
        "labels": torch.stack(padded_inputs).clone()
    }

    if image_embeddings:
        result["image_embeddings"] = torch.stack(image_embeddings)

    return result

def prepare_dataset(image_embeddings_path, dataset_path, cache_dir=None):
    # Load image embeddings
    try:
        image_embeddings = load_embeddings(image_embeddings_path)
    except Exception as e:
        print(f"Error loading image embeddings: {e}")
        print("Continuing without image embeddings...")
        image_embeddings = None

    # Load dataset
    with open(dataset_path, 'r') as f:
        data = json.load(f)

    # Create dataset
    processed_data = []
    for item in tqdm(data):
        processed_item = {'conversation': item['conversations']}

        if image_embeddings is not None and 'image' in item:
            image_id = item['image']
            if isinstance(image_embeddings['ids'][0], str):
                index = np.where(image_embeddings['ids'] == image_id)[0]
            else:
                index = np.where(image_embeddings['ids'] == int(image_id))[0]

            if len(index) > 0:
                processed_item['image_embedding'] = image_embeddings['embeddings'][index[0]]

        if 'audio' in item:
            processed_item['audio_path'] = item['audio']

        processed_data.append(processed_item)

    dataset = Dataset.from_dict({
        "conversation": [item['conversation'] for item in processed_data],
        "image_embedding": [item.get('image_embedding') for item in processed_data],
        "audio_path": [item.get('audio_path') for item in processed_data]
    })

    # Tokenizer for Phi-3
    tokenizer = AutoTokenizer.from_pretrained("microsoft/Phi-3-mini-128k-instruct", trust_remote_code=True)
    tokenizer.pad_token = tokenizer.eos_token
    tokenizer.padding_side = "right"

    # Set the chat template
    tokenizer.chat_template = """
    {% for message in messages %}
    {% if message.role == 'system' %}
    <|system|>{{message.content}}<|endoftext|>
    {% elif message.role == 'user' %}
    <|user|>{{message.content}}<|endoftext|>
    {% elif message.role == 'assistant' %}
    <|assistant|>{{message.content}}<|endoftext|>
    {% endif %}
    {% endfor %}
    """

    # Prepare dataset
    prepared_dataset = dataset.map(
        lambda examples: prepare_example(examples, tokenizer),
        batched=True,
        remove_columns=dataset.column_names,
        batch_size=1  # Process one item at a time to avoid memory issues
    ).with_format("torch")

    # Split dataset
    train_test_split = prepared_dataset.train_test_split(test_size=0.1)
    dataset_dict = DatasetDict({
        'train': train_test_split['train'],
        'test': train_test_split['test']
    })

    # Cache the prepared dataset if cache_dir is provided
    if cache_dir:
        os.makedirs(cache_dir, exist_ok=True)
        dataset_dict.save_to_disk(cache_dir)

    return dataset_dict, tokenizer

def train_model(dataset_dict, tokenizer, output_dir):
    # Load Phi-3 model
    phi3_model = AutoModelForCausalLM.from_pretrained(
        "microsoft/Phi-3-mini-128k-instruct",
        trust_remote_code=True,
        torch_dtype=torch.bfloat16,
        device_map="auto"
    )

    phi3_model = prepare_model_for_kbit_training(phi3_model)

    # Initialize projector and combined model
    image_embedding_dim = 1000  # Adjust this based on your image embeddings
    projector = MultimodalProjector(image_embedding_dim, phi3_model.config.hidden_size)
    model = Phi3WithProjector(phi3_model.config, phi3_model, projector)

    # Set up LoRA
    lora_config = LoraConfig(
        r=16,
        lora_alpha=32,
        target_modules=["self_attn.qkv_proj", "self_attn.o_proj", "mlp.gate_up_proj", "mlp.down_proj"],  # Adjust based on Phi-3 architecture
        lora_dropout=0.05,
        bias="none",
        task_type=TaskType.CAUSAL_LM
    )
    model = get_peft_model(model, lora_config)

    # Training arguments
    training_args = TrainingArguments(
        output_dir=output_dir,
        num_train_epochs=3,
        per_device_train_batch_size=4,
        gradient_accumulation_steps=4,
        learning_rate=2e-4,
        fp16=True,
        save_total_limit=3,
        logging_steps=100,
        save_steps=500,
        eval_steps=500,
        evaluation_strategy="steps",
        load_best_model_at_end=True,
    )

    # Initialize trainer
    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=dataset_dict['train'],
        eval_dataset=dataset_dict['test'],
        tokenizer=tokenizer,
        data_collator=MultimodalDataCollator(tokenizer),
    )

    # Start training
    trainer.train()

    # Save the fine-tuned model
    trainer.save_model(output_dir)

import torch
import torch.nn as nn
import librosa
from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments, Trainer, WhisperProcessor, WhisperForConditionalGeneration
from peft import prepare_model_for_kbit_training, LoraConfig, get_peft_model, TaskType
from datasets import Dataset, DatasetDict
import numpy as np
from tqdm import tqdm
import json
import os
from dataclasses import dataclass
from typing import Any, Dict, List, Union

# Load Whisper model and processor
whisper_model_name = "openai/whisper-base"  # You can change this to the specific model you want to use
whisper_processor = WhisperProcessor.from_pretrained(whisper_model_name)
whisper_model = WhisperForConditionalGeneration.from_pretrained(whisper_model_name)

def transcribe_speech(audiopath):
    speech, rate = librosa.load(audiopath, sr=16000)
    audio_input = whisper_processor(speech, return_tensors="pt", sampling_rate=16000)

    with torch.no_grad():
        generated_ids = whisper_model.generate(audio_input["input_features"])

    transcription = whisper_processor.batch_decode(generated_ids, skip_special_tokens=True)[0]

    return transcription

def getAudioArray(audio_path):
    speech, rate = librosa.load(audio_path, sr=16000)
    return speech

def process_audio(audio_path):
    audio_array = getAudioArray(audio_path)
    audio_input = whisper_processor(audio_array, return_tensors="pt", sampling_rate=16000)

    with torch.no_grad():
        audio_features = whisper_model.get_encoder()(audio_input.input_features)[0]

    return audio_features.mean(dim=0)  # Average over the time dimension

class ProjectionBlock(nn.Module):
    def __init__(self, input_dim, output_dim):
        super().__init__()
        self.pre_norm = nn.LayerNorm(input_dim)
        self.proj = nn.Sequential(
            nn.Linear(input_dim, output_dim),
            nn.GELU(),
            nn.Linear(output_dim, output_dim)
        )
    def forward(self, x):
        x = self.pre_norm(x)
        return self.proj(x)

class MultimodalProjector(nn.Module):
    def __init__(self, image_input_dim, audio_input_dim, output_dim):
        super().__init__()
        self.image_proj = ProjectionBlock(image_input_dim, output_dim)
        self.audio_proj = ProjectionBlock(audio_input_dim, output_dim)

    def forward(self, image_embedding=None, audio_embedding=None):
        outputs = []
        if image_embedding is not None:
            outputs.append(self.image_proj(image_embedding))
        if audio_embedding is not None:
            outputs.append(self.audio_proj(audio_embedding))
        return torch.stack(outputs).mean(dim=0) if outputs else None

class Phi3WithProjector(nn.Module):
    def __init__(self, config, phi3_model, projector):
        super().__init__()
        self.config = config
        self.phi3_model = phi3_model
        self.projector = projector

    def forward(
        self,
        input_ids=None,
        attention_mask=None,
        inputs_embeds=None,
        image_embeddings=None,
        audio_embeddings=None,
        labels=None,
        output_attentions=None,
        output_hidden_states=None,
        return_dict=None,
        **kwargs
    ):
        projected_embeddings = self.projector(image_embeddings, audio_embeddings)

        if inputs_embeds is None:
            inputs_embeds = self.phi3_model.transformer.wte(input_ids)

        if projected_embeddings is not None:
            combined_embeddings = torch.cat([inputs_embeds, projected_embeddings.unsqueeze(1)], dim=1)
            if attention_mask is not None:
                extended_attention_mask = torch.cat([attention_mask, torch.ones((attention_mask.shape[0], 1), device=attention_mask.device)], dim=1)
            else:
                extended_attention_mask = None
        else:
            combined_embeddings = inputs_embeds
            extended_attention_mask = attention_mask

        outputs = self.phi3_model(
            inputs_embeds=combined_embeddings,
            attention_mask=extended_attention_mask,
            labels=labels,
            output_attentions=output_attentions,
            output_hidden_states=output_hidden_states,
            return_dict=return_dict,
            **kwargs
        )
        return outputs

@dataclass
class MultimodalDataCollator:
    tokenizer: Any

    def __call__(self, features: List[Dict[str, Any]]) -> Dict[str, Any]:
        batch = {}

        input_ids = [feature["input_ids"] for feature in features]
        batch["input_ids"] = self.tokenizer.pad(
            {"input_ids": input_ids},
            padding=True,
            return_tensors="pt",
        )["input_ids"]

        batch["attention_mask"] = torch.ones_like(batch["input_ids"])
        batch["labels"] = batch["input_ids"].clone()

        if "image_embeddings" in features[0]:
            batch["image_embeddings"] = torch.stack([
                feature["image_embeddings"] for feature in features
            ])

        if "audio_embeddings" in features[0]:
            batch["audio_embeddings"] = torch.stack([
                feature["audio_embeddings"] for feature in features
            ])

        return batch

import torch
import librosa
from transformers import WhisperProcessor, WhisperForConditionalGeneration

whisper_model_name = "openai/whisper-base"
whisper_processor = WhisperProcessor.from_pretrained(whisper_model_name)
whisper_model = WhisperForConditionalGeneration.from_pretrained(whisper_model_name)

def transcribe_speech(audiopath):
    speech, rate = librosa.load(audiopath, sr=16000)
    audio_input = whisper_processor(speech, return_tensors="pt", sampling_rate=16000)

    with torch.no_grad():
        generated_ids = whisper_model.generate(audio_input["input_features"])

    transcription = whisper_processor.batch_decode(generated_ids, skip_special_tokens=True)[0]

    return transcription

def prepare_example(examples, tokenizer):
    image_embeddings = []
    tokenized_inputs = []

    for idx, conv in enumerate(examples['conversation']):
        image_embedding = torch.tensor(examples['image_embedding'][idx]) if examples['image_embedding'][idx] is not None else None

        for i in range(0, len(conv), 2):
            if i + 1 < len(conv):
                human_msg = conv[i]['value'].replace('<image>', '').replace('<audio>', '').strip()
                gpt_msg = conv[i + 1]['value']

                if 'audio_path' in examples and examples['audio_path'][idx]:
                    transcription = transcribe_speech(examples['audio_path'][idx])
                    human_msg += f"\nAudio Transcription: {transcription}"

                dialogue = [
                    {"role": "system", "content": "You are a helpful assistant."},
                    {"role": "user", "content": f"Given the following information, provide a detailed and accurate response:\n{human_msg}\n[Image is provided for this task.]\n"},
                    {"role": "assistant", "content": gpt_msg}
                ]

                tokenized_input = tokenizer.apply_chat_template(dialogue, return_tensors='pt', padding=True)
                tokenized_inputs.append(tokenized_input.squeeze(0))
                if image_embedding is not None:
                    image_embeddings.append(image_embedding)

    max_length = max(input.shape[0] for input in tokenized_inputs)
    padded_inputs = [torch.nn.functional.pad(input, (0, max_length - input.shape[0])) for input in tokenized_inputs]

    result = {
        "input_ids": torch.stack(padded_inputs),
        "attention_mask": torch.stack(padded_inputs).ne(tokenizer.pad_token_id).long(),
        "labels": torch.stack(padded_inputs).clone()
    }

    if image_embeddings:
        result["image_embeddings"] = torch.stack(image_embeddings)

    return result

def prepare_dataset(image_embeddings_path, dataset_path, cache_dir=None):
    try:
        image_embeddings = np.load(image_embeddings_path)
        print(f"Contents of {image_embeddings_path}:")
        for key in image_embeddings.files:
            print(f"- {key}: shape {image_embeddings[key].shape}")
    except Exception as e:
        print(f"Error loading image embeddings: {e}")
        print("Continuing without image embeddings...")
        image_embeddings = None

    with open(dataset_path, 'r') as f:
        data = json.load(f)

    processed_data = []
    for item in tqdm(data):
        processed_item = {'conversation': item['conversations']}

        if image_embeddings is not None and 'image' in item:
            image_id = item['image']
            if isinstance(image_embeddings['image_ids'][0], str):
                index = np.where(image_embeddings['image_ids'] == image_id)[0]
            else:
                index = np.where(image_embeddings['image_ids'] == int(image_id))[0]

            if len(index) > 0:
                processed_item['image_embedding'] = image_embeddings['embeddings'][index[0]]

        if 'audio' in item:
            processed_item['audio_path'] = item['audio']

        processed_data.append(processed_item)

    dataset = Dataset.from_dict({
        "conversation": [item['conversation'] for item in processed_data],
        "image_embedding": [item.get('image_embedding') for item in processed_data],
        "audio_path": [item.get('audio_path') for item in processed_data]
    })

    tokenizer = AutoTokenizer.from_pretrained("microsoft/Phi-3-mini-128k-instruct", trust_remote_code=True)
    tokenizer.pad_token = tokenizer.eos_token
    tokenizer.padding_side = "right"

    tokenizer.chat_template = """
    {% for message in messages %}
    {% if message.role == 'system' %}
    <|system|>{{message.content}}<|endoftext|>
    {% elif message.role == 'user' %}
    <|user|>{{message.content}}<|endoftext|>
    {% elif message.role == 'assistant' %}
    <|assistant|>{{message.content}}<|endoftext|>
    {% endif %}
    {% endfor %}
    """

    prepared_dataset = dataset.map(
        lambda examples: prepare_example(examples, tokenizer),
        batched=True,
        remove_columns=dataset.column_names,
        batch_size=1
    ).with_format("torch")

    train_test_split = prepared_dataset.train_test_split(test_size=0.1)
    dataset_dict = DatasetDict({
        'train': train_test_split['train'],
        'test': train_test_split['test']
    })

    if cache_dir:
        os.makedirs(cache_dir, exist_ok=True)
        dataset_dict.save_to_disk(cache_dir)

    return dataset_dict, tokenizer

def train_model(dataset_dict, tokenizer, output_dir):
    phi3_model = AutoModelForCausalLM.from_pretrained(
        "microsoft/Phi-3-mini-128k-instruct",
        trust_remote_code=True,
        torch_dtype=torch.bfloat16,
        device_map="auto"
    )

    phi3_model = prepare_model_for_kbit_training(phi3_model)

    image_embedding_dim = 1000  # Adjust based on your image embeddings
    audio_embedding_dim = whisper_model.config.hidden_size
    projector = MultimodalProjector(image_embedding_dim, audio_embedding_dim, phi3_model.config.hidden_size)
    model = Phi3WithProjector(phi3_model.config, phi3_model, projector)

    lora_config = LoraConfig(
        r=16,
        lora_alpha=32,
        target_modules=["self_attn.qkv_proj", "self_attn.o_proj", "mlp.gate_up_proj", "mlp.down_proj"],
        lora_dropout=0.05,
        bias="none",
        task_type=TaskType.CAUSAL_LM
    )
    model = get_peft_model(model, lora_config)

    training_args = TrainingArguments(
        output_dir=output_dir,
        num_train_epochs=3,
        per_device_train_batch_size=4,
        gradient_accumulation_steps=4,
        learning_rate=2e-4,
        fp16=True,
        save_total_limit=3,
        logging_steps=100,
        save_steps=500,
        eval_steps=500,
        evaluation_strategy="steps",
        load_best_model_at_end=True,
        optim="paged_adamw_8bit"
    )

    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=dataset_dict['train'],
        eval_dataset=dataset_dict['test'],
        tokenizer=tokenizer,
        data_collator=MultimodalDataCollator(tokenizer),
    )

    trainer.train()
    trainer.save_model(output_dir)

if __name__ == "__main__":
    image_embeddings_path = "/content/drive/MyDrive/LLM/clip_embeddings_instruct150k_15Oct_2024.npz"
    dataset_path = "/content/llava_instruct_150k.json"
    output_dir = "/content/drive/MyDrive/LLM_training_output"
    whisper_model_name = "openai/whisper-base"
    cache_dir = "/content/"

    dataset_dict, tokenizer = prepare_dataset(image_embeddings_path, dataset_path, cache_dir)

train_model(dataset_dict, tokenizer, output_dir)

































import torch
import torch.nn as nn
from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments, Trainer, WhisperFeatureExtractor, WhisperModel, PreTrainedModel
from peft import prepare_model_for_kbit_training, LoraConfig, get_peft_model, TaskType
from datasets import Dataset, DatasetDict
import numpy as np
from tqdm import tqdm
import json
import librosa
import os
from dataclasses import dataclass
from typing import Any, Dict, List, Union

# Dependencies:
# pip install transformers peft datasets tqdm torch numpy

# Utility functions (load_embeddings, process_audio, etc.)
def load_embeddings(file_path):
    data = np.load(file_path, allow_pickle=True)
    print(f"Contents of {file_path}:")
    for key in data.files:
        print(f"- {key}: shape {data[key].shape}")

    # Assuming the file contains a single array with both ids and embeddings
    if len(data.files) == 1:
        combined_data = data[data.files[0]]
        return {
            'ids': combined_data[:, 0],  # Assuming first column is ids
            'embeddings': combined_data[:, 1:]  # Assuming remaining columns are embeddings
        }
    elif 'ids' in data and 'embeddings' in data:
        return {
            'ids': data['ids'],
            'embeddings': data['embeddings']
        }
    else:
        raise ValueError(f"Unexpected structure in {file_path}. Please check the file contents.")

def process_audio(audio_path, feature_extractor, whisper_model):
    audio, sr = librosa.load(audio_path, sr=16000)
    inputs = feature_extractor(audio, sampling_rate=sr, return_tensors="pt")
    with torch.no_grad():
        outputs = whisper_model(**inputs)
    audio_embedding = outputs.last_hidden_state.mean(dim=1)
    return audio_embedding.squeeze()

# Cell 2: Model Definitions
class MultimodalProjector(nn.Module):
    def __init__(self, image_input_dim, audio_input_dim, output_dim):
        super().__init__()
        self.image_proj = nn.Linear(image_input_dim, output_dim)
        self.audio_proj = nn.Linear(audio_input_dim, output_dim)

    def forward(self, image_embedding=None, audio_embedding=None):
        outputs = []
        if image_embedding is not None:
            outputs.append(self.image_proj(image_embedding))
        if audio_embedding is not None:
            outputs.append(self.audio_proj(audio_embedding))
        return torch.stack(outputs).mean(dim=0) if outputs else None

class Phi3WithProjector(PreTrainedModel):
    def __init__(self, config, phi3_model, projector):
        super().__init__(config)
        self.config = config
        self.phi3_model = phi3_model
        self.projector = projector

    def forward(
        self,
        input_ids=None,
        attention_mask=None,
        inputs_embeds=None,
        image_embeddings=None,
        audio_embeddings=None,
        labels=None,
        output_attentions=None,
        output_hidden_states=None,
        return_dict=None,
        **kwargs
    ):
        projected_embeddings = self.projector(image_embeddings, audio_embeddings)

        if inputs_embeds is None:
            inputs_embeds = self.phi3_model.transformer.wte(input_ids)

        if projected_embeddings is not None:
            combined_embeddings = torch.cat([inputs_embeds, projected_embeddings], dim=1)
            if attention_mask is not None:
                extended_attention_mask = torch.cat([attention_mask, torch.ones((attention_mask.shape[0], projected_embeddings.shape[1]), device=attention_mask.device)], dim=1)
            else:
                extended_attention_mask = None
        else:
            combined_embeddings = inputs_embeds
            extended_attention_mask = attention_mask

        outputs = self.phi3_model(
            inputs_embeds=combined_embeddings,
            attention_mask=extended_attention_mask,
            labels=labels,
            output_attentions=output_attentions,
            output_hidden_states=output_hidden_states,
            return_dict=return_dict,
            **kwargs
        )
        return outputs

    def get_input_embeddings(self):
        return self.phi3_model.get_input_embeddings()

    def set_input_embeddings(self, value):
        self.phi3_model.set_input_embeddings(value)

    def get_output_embeddings(self):
        return self.phi3_model.get_output_embeddings()

    def set_output_embeddings(self, new_embeddings):
        self.phi3_model.set_output_embeddings(new_embeddings)

    def prepare_inputs_for_generation(self, input_ids, past=None, attention_mask=None, **kwargs):
        inputs = self.phi3_model.prepare_inputs_for_generation(input_ids, past=past, attention_mask=attention_mask, **kwargs)
        inputs['image_embeddings'] = kwargs.get('image_embeddings', None)
        inputs['audio_embeddings'] = kwargs.get('audio_embeddings', None)
        return inputs

    def _reorder_cache(self, past, beam_idx):
        return self.phi3_model._reorder_cache(past, beam_idx)

@dataclass
class MultimodalDataCollator:
    """
    Data collator that handles multimodal inputs (text, image, and audio).
    This collator will pad the sequences to the maximum length in the batch.
    """
    tokenizer: Any  # This should be the tokenizer you're using

    def __call__(self, features: List[Dict[str, Any]]) -> Dict[str, Any]:
        batch = {}

        # Handle text inputs
        input_ids = [feature["input_ids"] for feature in features]
        batch["input_ids"] = self.tokenizer.pad(
            {"input_ids": input_ids},
            padding=True,
            return_tensors="pt",
        )["input_ids"]

        batch["attention_mask"] = torch.ones_like(batch["input_ids"])
        batch["labels"] = batch["input_ids"].clone()

        # Handle image embeddings
        if "image_embeddings" in features[0]:
            batch["image_embeddings"] = torch.stack([
                feature["image_embeddings"] for feature in features
            ])

        # Handle audio embeddings
        if "audio_embeddings" in features[0]:
            batch["audio_embeddings"] = torch.stack([
                feature["audio_embeddings"] for feature in features
            ])

        return batch

# Cell 3: Dataset Preparation Function
def prepare_example(examples, tokenizer, feature_extractor, whisper_model):
    image_embeddings = []
    audio_embeddings = []
    tokenized_inputs = []

    for idx, conv in enumerate(examples['conversation']):
        image_embedding = torch.tensor(examples['image_embedding'][idx]) if examples['image_embedding'][idx] is not None else None

        # Process audio if available
        if 'audio_path' in examples and examples['audio_path'][idx]:
            audio_embedding = process_audio(examples['audio_path'][idx], feature_extractor, whisper_model)
        else:
            audio_embedding = None

        for i in range(0, len(conv), 2):
            if i + 1 < len(conv):  # Ensure we have a pair
                human_msg = conv[i]['value'].replace('<image>', '').replace('<audio>', '').strip()
                gpt_msg = conv[i + 1]['value']

                dialogue = [
                    {"role": "system", "content": "You are a helpful assistant."},
                    {"role": "user", "content": f"Given the following information, provide a detailed and accurate response:\n{human_msg}\n[Image and audio are provided for this task.]\n"},
                    {"role": "assistant", "content": gpt_msg}
                ]

                tokenized_input = tokenizer.apply_chat_template(dialogue, return_tensors='pt', padding=True)
                tokenized_inputs.append(tokenized_input.squeeze(0))
                if image_embedding is not None:
                    image_embeddings.append(image_embedding)
                if audio_embedding is not None:
                    audio_embeddings.append(audio_embedding)

    # Pad all tokenized inputs to the same length
    max_length = max(input.shape[0] for input in tokenized_inputs)
    padded_inputs = [torch.nn.functional.pad(input, (0, max_length - input.shape[0])) for input in tokenized_inputs]

    result = {
        "input_ids": torch.stack(padded_inputs),
        "attention_mask": torch.stack(padded_inputs).ne(tokenizer.pad_token_id).long(),
        "labels": torch.stack(padded_inputs).clone()
    }

    if image_embeddings:
        result["image_embeddings"] = torch.stack(image_embeddings)
    if audio_embeddings:
        result["audio_embeddings"] = torch.stack(audio_embeddings)

    return result

def prepare_dataset(image_embeddings_path, dataset_path, whisper_model_name, cache_dir=None):
    # Load image embeddings
    try:
        image_embeddings = load_embeddings(image_embeddings_path)
    except Exception as e:
        print(f"Error loading image embeddings: {e}")
        print("Continuing without image embeddings...")
        image_embeddings = None

    # Load dataset
    with open(dataset_path, 'r') as f:
        data = json.load(f)

    # Create dataset
    processed_data = []
    for item in tqdm(data):
        processed_item = {'conversation': item['conversations']}

        if image_embeddings is not None and 'image' in item:
            image_id = item['image']
            if isinstance(image_embeddings['ids'][0], str):
                index = np.where(image_embeddings['ids'] == image_id)[0]
            else:
                index = np.where(image_embeddings['ids'] == int(image_id))[0]

            if len(index) > 0:
                processed_item['image_embedding'] = image_embeddings['embeddings'][index[0]]

        if 'audio' in item:
            processed_item['audio_path'] = item['audio']

        processed_data.append(processed_item)

    dataset = Dataset.from_dict({
        "conversation": [item['conversation'] for item in processed_data],
        "image_embedding": [item.get('image_embedding') for item in processed_data],
        "audio_path": [item.get('audio_path') for item in processed_data]
    })

    # Load Whisper model and feature extractor
    feature_extractor = WhisperFeatureExtractor.from_pretrained(whisper_model_name)
    whisper_model = WhisperModel.from_pretrained(whisper_model_name)

    # Tokenizer for Phi-3
    tokenizer = AutoTokenizer.from_pretrained("microsoft/Phi-3-mini-128k-instruct", trust_remote_code=True)
    tokenizer.pad_token = tokenizer.eos_token
    tokenizer.padding_side = "right"

    # Set the chat template
    tokenizer.chat_template = """
    {% for message in messages %}
    {% if message.role == 'system' %}
    <|system|>{{message.content}}<|endoftext|>
    {% elif message.role == 'user' %}
    <|user|>{{message.content}}<|endoftext|>
    {% elif message.role == 'assistant' %}
    <|assistant|>{{message.content}}<|endoftext|>
    {% endif %}
    {% endfor %}
    """

    # Prepare dataset
    prepared_dataset = dataset.map(
        lambda examples: prepare_example(examples, tokenizer, feature_extractor, whisper_model),
        batched=True,
        remove_columns=dataset.column_names,
        batch_size=1  # Process one item at a time to avoid memory issues
    ).with_format("torch")

    # Split dataset
    train_test_split = prepared_dataset.train_test_split(test_size=0.1)
    dataset_dict = DatasetDict({
        'train': train_test_split['train'],
        'test': train_test_split['test']
    })

    # Cache the prepared dataset if cache_dir is provided
    if cache_dir:
        os.makedirs(cache_dir, exist_ok=True)
        dataset_dict.save_to_disk(cache_dir)

    return dataset_dict, tokenizer

# Cell 4: Model Training Function
def train_model(dataset_dict, tokenizer, output_dir):
    # Load Phi-3 model
    phi3_model = AutoModelForCausalLM.from_pretrained(
        "microsoft/Phi-3-mini-128k-instruct",
        trust_remote_code=True,
        device_map="auto",
        torch_dtype=torch.bfloat16,
    )

    # # Print model structure
    # print("Model structure:")
    # for name, module in phi3_model.named_modules():
    #     if isinstance(module, nn.Linear):
    #         print(f"Linear layer: {name}")
    # Initialize projector and combined model
    image_embedding_dim = 1000  # Adjust this based on your image embeddings
    audio_embedding_dim = 768  # Whisper base model hidden size
    projector = MultimodalProjector(image_embedding_dim, audio_embedding_dim, phi3_model.config.hidden_size)
    model = Phi3WithProjector(phi3_model.config, phi3_model, projector)

    # Set up LoRA
    lora_config = LoraConfig(
        r=16,
        lora_alpha=32,
        target_modules=["self_attn.qkv_proj", "self_attn.o_proj", "mlp.gate_up_proj", "mlp.down_proj"],
        lora_dropout=0.05,
        bias="none",
        task_type=TaskType.CAUSAL_LM
    )
    model = get_peft_model(model, lora_config)

    # Training arguments
    training_args = TrainingArguments(
        output_dir=output_dir,
        num_train_epochs=3,
        per_device_train_batch_size=4,
        gradient_accumulation_steps=4,
        learning_rate=2e-4,
        fp16=True,
        save_total_limit=3,
        logging_steps=100,
        save_steps=500,
        eval_steps=500,
        evaluation_strategy="steps",
        load_best_model_at_end=True,
         optim="paged_adamw_8bit"

    )

    # Initialize trainer
    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=dataset_dict['train'],
        eval_dataset=dataset_dict['test'],
        tokenizer=tokenizer,
        data_collator=MultimodalDataCollator(tokenizer),
    )

    # Start training
    trainer.train()

    # Save the fine-tuned model
    trainer.save_model(output_dir)

# Cell 5: Execution
if __name__ == "__main__":
    image_embeddings_path = "/content/drive/MyDrive/LLM/clip_embeddings_instruct150k_15Oct_2024.npz"
    dataset_path = "/content/llava_instruct_150k.json"
    output_dir = "/content/drive/MyDrive/LLM_training_output"
    whisper_model_name = "openai/whisper-base"
    cache_dir = "/content/"

    # Prepare dataset (run this only once)
    dataset_dict, tokenizer = prepare_dataset(image_embeddings_path, dataset_path, whisper_model_name, cache_dir)

# Train model (can be run multiple times with different parameters)
train_model(dataset_dict, tokenizer, output_dir)





























# import os
# import numpy as np
# import torch
# import torch.nn as nn
# from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments, Trainer
# from peft import prepare_model_for_kbit_training, LoraConfig, get_peft_model
# from datasets import Dataset, DatasetDict
# from tqdm import tqdm
# import json

# # Dependencies:
# # pip install transformers peft datasets tqdm torch numpy

# # 1. Load the .npz Image Embeddings
# def load_image_embeddings(file_path):
#     """Load image embeddings from .npz file."""
#     data = np.load(file_path)
#     return {
#         'ids': data['image_ids'],
#         'image_embeddings': data['embeddings']
#     }

# # 2. Preprocessing Steps
# def preprocess_embeddings(embeddings):
#     """Preprocess image embeddings if needed."""
#     # Example: Normalize embeddings
#     return embeddings / np.linalg.norm(embeddings, axis=1, keepdims=True)

# # 3. Fine-Tuning Process

# # Define the multimodal projector
# class MultimodalProjector(nn.Module):
#     def __init__(self, image_input_dim, output_dim):
#         super().__init__()
#         self.image_proj = nn.Linear(image_input_dim, output_dim)

#     def forward(self, image_embedding):
#         return self.image_proj(image_embedding)

# # Define the Phi-2 model with projector
# class Phi2WithProjector(nn.Module):
#     def __init__(self, phi2_model, projector):
#         super().__init__()
#         self.phi2_model = phi2_model
#         self.projector = projector

#     def forward(self, input_ids, attention_mask, image_embeddings=None):
#         if image_embeddings is not None:
#             projected_image = self.projector(image_embeddings)
#             text_embeddings = self.phi2_model.transformer.wte(input_ids)
#             combined_embeddings = torch.cat([text_embeddings, projected_image.unsqueeze(1)], dim=1)
#             extended_attention_mask = torch.cat([attention_mask, torch.ones((attention_mask.shape[0], 1), device=attention_mask.device)], dim=1)
#         else:
#             combined_embeddings = self.phi2_model.transformer.wte(input_ids)
#             extended_attention_mask = attention_mask

#         outputs = self.phi2_model(inputs_embeds=combined_embeddings, attention_mask=extended_attention_mask)
#         return outputs

# def prepare_dataset(examples, tokenizer):
#     """Prepare dataset for training."""
#     conversations = []
#     image_embeddings = []

#     for idx, conv in enumerate(examples['conversation']):
#         image_embedding = torch.tensor(examples['image_embedding'][idx])

#         messages = []
#         for turn in conv:
#             if turn['from'] == 'human':
#                 messages.append({"role": "user", "content": turn['value']})
#             elif turn['from'] == 'gpt':
#                 messages.append({"role": "assistant", "content": turn['value']})

#         conversations.append({"messages": messages})
#         image_embeddings.append(image_embedding)

#     tokenized_conversations = tokenizer.apply_chat_template(conversations, return_tensors='pt', padding=True)

#     return {
#         "input_ids": tokenized_conversations,
#         "attention_mask": torch.ones_like(tokenized_conversations),
#         "labels": tokenized_conversations.clone(),
#         "image_embeddings": torch.stack(image_embeddings)
#     }

# def fine_tune_phi2(image_embeddings_path, dataset_path, output_dir):
#     # Load image embeddings
#     image_embeddings = load_image_embeddings(image_embeddings_path)
#     preprocessed_embeddings = preprocess_embeddings(image_embeddings['image_embeddings'])

#     # Load dataset
#     with open(dataset_path, 'r') as f:
#         data = json.load(f)

#     # Create dataset
#     processed_data = []
#     for item in tqdm(data):
#         image_file = item['image']
#         if image_file in image_embeddings['ids']:
#             index = np.where(image_embeddings['ids'] == image_file)[0][0]
#             processed_data.append({
#                 'image': image_file,
#                 'image_embedding': preprocessed_embeddings[index].tolist(),
#                 'conversation': item['conversations']
#             })

#     dataset = Dataset.from_dict({
#         "image": [item['image'] for item in processed_data],
#         "image_embedding": [item['image_embedding'] for item in processed_data],
#         "conversation": [item['conversation'] for item in processed_data]
#     })

#     # Load Phi-2 model and tokenizer
#     model_name = "microsoft/phi-2"
#     tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
#     tokenizer.pad_token = tokenizer.eos_token
#     tokenizer.padding_side = "right"

#     # Set the chat template
#     tokenizer.chat_template = """
#     {% if system is defined %}
#     <|system|>{{system}}<|endoftext|>
#     {% endif %}
#     {% for message in messages %}
#     {% if message.role == 'user' %}
#     <|user|>{{message.content}}<|endoftext|>
#     {% elif message.role == 'assistant' %}
#     <|assistant|>{{message.content}}<|endoftext|>
#     {% endif %}
#     {% endfor %}<|assistant|>
#     """

#     phi2_model = AutoModelForCausalLM.from_pretrained(model_name, trust_remote_code=True)

#     # Initialize projector and combined model
#     image_embedding_dim = preprocessed_embeddings.shape[1]
#     projector = MultimodalProjector(image_embedding_dim, phi2_model.config.hidden_size)
#     model = Phi2WithProjector(phi2_model, projector)

#     # Prepare dataset
#     prepared_dataset = dataset.map(
#         lambda examples: prepare_dataset(examples, tokenizer),
#         batched=True,
#         remove_columns=dataset.column_names
#     ).with_format("torch")

#     # Split dataset
#     train_test_split = prepared_dataset.train_test_split(test_size=0.1)
#     dataset_dict = DatasetDict({
#         'train': train_test_split['train'],
#         'test': train_test_split['test']
#     })

#     # Set up LoRA
#     lora_config = LoraConfig(
#         r=16,
#         lora_alpha=32,
#         target_modules=["query_key_value"],
#         lora_dropout=0.05,
#         bias="none",
#         task_type="CAUSAL_LM"
#     )
#     model = get_peft_model(model, lora_config)

#     # Training arguments
#     training_args = TrainingArguments(
#         output_dir=output_dir,
#         num_train_epochs=3,
#         per_device_train_batch_size=4,
#         gradient_accumulation_steps=4,
#         learning_rate=2e-4,
#         fp16=True,
#         save_total_limit=3,
#         logging_steps=100,
#         save_steps=500,
#         eval_steps=500,
#         evaluation_strategy="steps",
#         load_best_model_at_end=True,
#     )

#     # Initialize trainer
#     trainer = Trainer(
#         model=model,
#         args=training_args,
#         train_dataset=dataset_dict['train'],
#         eval_dataset=dataset_dict['test'],
#         tokenizer=tokenizer,
#     )

#     # Start training
#     trainer.train()

#     # Save the fine-tuned model
#     trainer.save_model(output_dir)

# # phi2_multimodal_finetuning.py

# import os
# import numpy as np
# import torch
# import torch.nn as nn
# from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments, Trainer
# from peft import prepare_model_for_kbit_training, LoraConfig, get_peft_model
# from datasets import Dataset, DatasetDict
# from tqdm import tqdm
# import json

# # Dependencies:
# # pip install transformers peft datasets tqdm torch numpy

# # 1. Load the .npz Image Embeddings
# def load_image_embeddings(file_path):
#     """Load image embeddings from .npz file."""
#     data = np.load(file_path)
#     print(data)
#     return {
#         'ids': data['image_ids'],
#         'image_embeddings': data['embeddings']
#     }

# # 2. Preprocessing Steps
# def preprocess_embeddings(embeddings):
#     """Preprocess image embeddings if needed."""
#     # Example: Normalize embeddings
#     return embeddings / np.linalg.norm(embeddings, axis=1, keepdims=True)

# # 3. Fine-Tuning Process

# # Define the multimodal projector
# class MultimodalProjector(nn.Module):
#     def __init__(self, image_input_dim, output_dim):
#         super().__init__()
#         self.image_proj = nn.Linear(image_input_dim, output_dim)

#     def forward(self, image_embedding):
#         return self.image_proj(image_embedding)

# # Define the Phi-2 model with projector
# class Phi2WithProjector(nn.Module):
#     def __init__(self, phi2_model, projector):
#         super().__init__()
#         self.phi2_model = phi2_model
#         self.projector = projector

#     def forward(self, input_ids, attention_mask, image_embeddings=None):
#         if image_embeddings is not None:
#             projected_image = self.projector(image_embeddings)
#             text_embeddings = self.phi2_model.transformer.wte(input_ids)
#             combined_embeddings = torch.cat([text_embeddings, projected_image.unsqueeze(1)], dim=1)
#             extended_attention_mask = torch.cat([attention_mask, torch.ones((attention_mask.shape[0], 1), device=attention_mask.device)], dim=1)
#         else:
#             combined_embeddings = self.phi2_model.transformer.wte(input_ids)
#             extended_attention_mask = attention_mask

#         outputs = self.phi2_model(inputs_embeds=combined_embeddings, attention_mask=extended_attention_mask)
#         return outputs

# def prepare_dataset(examples, tokenizer):
#     image_embeddings = []
#     conversations = []

#     for idx, conv in enumerate(examples['conversation']):
#         image_embedding = torch.tensor(examples['image_embedding'][idx])
#         dialogue_pairs = []

#         for i in range(0, len(conv), 2):
#             if i + 1 < len(conv):  # Ensure we have a pair
#                 human_msg = conv[i]['value'].replace('<image>', '').strip()
#                 gpt_msg = conv[i + 1]['value']

#                 dialogue = [
#                     {"role": "system", "content": "You are a helpful assistant."},
#                     {"role": "user", "content": f"Given the following information, provide a detailed and accurate response:\n{human_msg}\n[An image is provided for this task.]\n"},
#                     {"role": "assistant", "content": gpt_msg}
#                 ]

#                 dialogue_pairs.append(dialogue)
#                 image_embeddings.append(image_embedding)

#         conversations.extend(dialogue_pairs)

#     image_embeddings = torch.stack(image_embeddings)

#     tokenized_conversations = tokenizer.apply_chat_template(conversations,
#                                                             return_tensors='pt', padding=True)

#     return {
#         "image_embeddings": image_embeddings,
#         "input_ids": tokenized_conversations,
#         "attention_mask": torch.ones_like(tokenized_conversations),
#         "labels": tokenized_conversations.clone()
#     }

# def fine_tune_phi2(image_embeddings_path, dataset_path, output_dir, tokenizer):
#     # Load image embeddings
#     image_embeddings = load_image_embeddings(image_embeddings_path)
#     preprocessed_embeddings = preprocess_embeddings(image_embeddings['image_embeddings'])

#     # Load dataset
#     with open(dataset_path, 'r') as f:
#         data = json.load(f)

#     # Create dataset
#     processed_data = []
#     for item in tqdm(data):
#         image_file = item['image']
#         if image_file in image_embeddings['ids']:
#             index = np.where(image_embeddings['ids'] == image_file)[0][0]
#             processed_data.append({
#                 'image': image_file,
#                 'image_embedding': preprocessed_embeddings[index],  # This is now a 1D array
#                 'conversation': item['conversations']
#             })

#     dataset = Dataset.from_dict({
#         "image": [item['image'] for item in processed_data],
#         "image_embedding": [item['image_embedding'] for item in processed_data],
#         "conversation": [item['conversation'] for item in processed_data]
#     })

#     # Load Phi-2 model and tokenizer
#     model_name = "microsoft/phi-2"
#     tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
#     tokenizer.pad_token = tokenizer.eos_token
#     tokenizer.padding_side = "right"

#     # Set the chat template
#     tokenizer.chat_template = """
#     {% if system is defined %}
#     <|system|>{{system}}<|endoftext|>
#     {% endif %}
#     {% for message in messages %}
#     {% if message.role == 'user' %}
#     <|user|>{{message.content}}<|endoftext|>
#     {% elif message.role == 'assistant' %}
#     <|assistant|>{{message.content}}<|endoftext|>
#     {% endif %}
#     {% endfor %}<|assistant|>
#     """

#     phi2_model = AutoModelForCausalLM.from_pretrained(model_name, trust_remote_code=True)

#     # Initialize projector and combined model
#     image_embedding_dim = preprocessed_embeddings.shape[1]  # Should be 1000
#     projector = MultimodalProjector(image_embedding_dim, phi2_model.config.hidden_size)
#     model = Phi2WithProjector(phi2_model, projector)

#      # Prepare dataset
#     prepared_dataset = dataset.map(
#         lambda examples: prepare_dataset(examples, tokenizer),
#         batched=True,
#         remove_columns=dataset.column_names,
#         batch_size=1  # Process one item at a time to avoid memory issues
#     ).with_format("torch")


#     # Split dataset
#     train_test_split = prepared_dataset.train_test_split(test_size=0.1)
#     dataset_dict = DatasetDict({
#         'train': train_test_split['train'],
#         'test': train_test_split['test']
#     })

#     # Set up LoRA
#     lora_config = LoraConfig(
#         r=16,
#         lora_alpha=32,
#         target_modules=["q_proj", "k_proj", "v_proj", "dense"],
#         lora_dropout=0.05,
#         bias="none",
#         task_type="CAUSAL_LM"
#     )
#     model = get_peft_model(model, lora_config)

#     # Training arguments
#     training_args = TrainingArguments(
#         output_dir=output_dir,
#         num_train_epochs=3,
#         per_device_train_batch_size=2,
#         gradient_accumulation_steps=4,
#         learning_rate=2e-4,
#         fp16=True,
#         save_total_limit=3,
#         logging_steps=100,
#         save_steps=500,
#         eval_steps=500,
#         evaluation_strategy="steps",
#         load_best_model_at_end=True,
#     )

#     # Initialize trainer
#     trainer = Trainer(
#         model=model,
#         args=training_args,
#         train_dataset=dataset_dict['train'],
#         eval_dataset=dataset_dict['test'],
#         tokenizer=tokenizer,
#     )

#     # Start training
#     trainer.train()

#     # Save the fine-tuned model
#     trainer.save_model(output_dir)

# # 5. Example Usage
# if __name__ == "__main__":
#     image_embeddings_path = "/content/drive/MyDrive/LLM/clip_embeddings_instruct150k_15Oct_2024.npz"
#     dataset_path = "/content/llava_instruct_150k.json"
#     output_dir = "/content/drive/MyDrive/LLM_training_output"
#     # Load the Phi-3 model and tokenizer
#     model_name = "microsoft/phi-2"
#     tokenizer = AutoTokenizer.from_pretrained(model_name,
#                                               padding_side="right",
#                                               add_eos_token=True,
#                                               trust_remote_code=True)
#     tokenizer.pad_token = tokenizer.eos_token
#     tokenizer.truncation_side = "left"

#     # fine_tune_phi2(image_embeddings_path, dataset_path, output_dir,tokenizer)

# Cell 3: Dataset Preparation Function
def prepare_example(examples, tokenizer, feature_extractor, whisper_model):
    image_embeddings = []
    audio_embeddings = []
    tokenized_inputs = []

    for idx, conv in enumerate(examples['conversation']):
        image_embedding = torch.tensor(examples['image_embedding'][idx]) if examples['image_embedding'][idx] is not None else None

        # Process audio if available
        if 'audio_path' in examples and examples['audio_path'][idx]:
            audio_embedding = process_audio(examples['audio_path'][idx], feature_extractor, whisper_model)
        else:
            audio_embedding = None

        for i in range(0, len(conv), 2):
            if i + 1 < len(conv):  # Ensure we have a pair
                human_msg = conv[i]['value'].replace('<image>', '').replace('<audio>', '').strip()
                gpt_msg = conv[i + 1]['value']

                dialogue = [
                    {"role": "system", "content": "You are a helpful assistant."},
                    {"role": "user", "content": f"Given the following information, provide a detailed and accurate response:\n{human_msg}\n[Image and audio are provided for this task.]\n"},
                    {"role": "assistant", "content": gpt_msg}
                ]

                tokenized_input = tokenizer.apply_chat_template(dialogue, return_tensors='pt', padding=True)
                tokenized_inputs.append(tokenized_input.squeeze(0))
                if image_embedding is not None:
                    image_embeddings.append(image_embedding)
                if audio_embedding is not None:
                    audio_embeddings.append(audio_embedding)

    # Pad all tokenized inputs to the same length
    max_length = max(input.shape[0] for input in tokenized_inputs)
    padded_inputs = [torch.nn.functional.pad(input, (0, max_length - input.shape[0])) for input in tokenized_inputs]

    result = {
        "input_ids": torch.stack(padded_inputs),
        "attention_mask": torch.stack(padded_inputs).ne(tokenizer.pad_token_id).long(),
        "labels": torch.stack(padded_inputs).clone()
    }

    if image_embeddings:
        result["image_embeddings"] = torch.stack(image_embeddings)
    if audio_embeddings:
        result["audio_embeddings"] = torch.stack(audio_embeddings)

    return result

def prepare_dataset(image_embeddings_path, dataset_path, whisper_model_name, cache_dir=None):
    # Load image embeddings
    image_embeddings = load_embeddings(image_embeddings_path)

    # Load dataset
    with open(dataset_path, 'r') as f:
        data = json.load(f)

    # Create dataset
    processed_data = []
    for item in tqdm(data):
        processed_item = {'conversation': item['conversations']}

        if 'image' in item and item['image'] in image_embeddings['ids']:
            index = np.where(image_embeddings['ids'] == item['image'])[0][0]
            processed_item['image_embedding'] = image_embeddings['embeddings'][index]

        if 'audio' in item:
            processed_item['audio_path'] = item['audio']

        processed_data.append(processed_item)

    dataset = Dataset.from_dict({
        "conversation": [item['conversation'] for item in processed_data],
        "image_embedding": [item.get('image_embedding') for item in processed_data],
        "audio_path": [item.get('audio_path') for item in processed_data]
    })

    # Load Whisper model and feature extractor
    feature_extractor = WhisperFeatureExtractor.from_pretrained(whisper_model_name)
    whisper_model = WhisperModel.from_pretrained(whisper_model_name)

    # Tokenizer for Phi-2
    tokenizer = AutoTokenizer.from_pretrained("microsoft/phi-2", trust_remote_code=True)
    tokenizer.pad_token = tokenizer.eos_token
    tokenizer.padding_side = "right"

    # Set the chat template
    tokenizer.chat_template = """
    {% if system is defined %}
    <|system|>{{system}}<|endoftext|>
    {% endif %}
    {% for message in messages %}
    {% if message.role == 'user' %}
    <|user|>{{message.content}}<|endoftext|>
    {% elif message.role == 'assistant' %}
    <|assistant|>{{message.content}}<|endoftext|>
    {% endif %}
    {% endfor %}<|assistant|>
    """

    # Prepare dataset
    prepared_dataset = dataset.map(
        lambda examples: prepare_example(examples, tokenizer, feature_extractor, whisper_model),
        batched=True,
        remove_columns=dataset.column_names,
        batch_size=1  # Process one item at a time to avoid memory issues
    ).with_format("torch")

    # Split dataset
    train_test_split = prepared_dataset.train_test_split(test_size=0.1)
    dataset_dict = DatasetDict({
        'train': train_test_split['train'],
        'test': train_test_split['test']
    })

    # Cache the prepared dataset if cache_dir is provided
    if cache_dir:
        os.makedirs(cache_dir, exist_ok=True)
        dataset_dict.save_to_disk(cache_dir)

    return dataset_dict, tokenizer

# Cell 4: Model Training Function
def train_model(dataset_dict, tokenizer, output_dir):
    # Load Phi-2 model with eager attention implementation
    phi2_model = AutoModelForCausalLM.from_pretrained(
        "microsoft/phi-2",
        trust_remote_code=True,
        attn_implementation="eager"
    )

    # Initialize projector and combined model
    image_embedding_dim = 1000  # Adjust this based on your image embeddings
    audio_embedding_dim = 768  # Whisper base model hidden size
    projector = MultimodalProjector(image_embedding_dim, audio_embedding_dim, phi2_model.config.hidden_size)
    model = Phi2WithProjector(phi2_model.config, phi2_model, projector)

    # Set up LoRA
    lora_config = LoraConfig(
        r=16,
        lora_alpha=32,
        target_modules=['k_proj', 'q_proj', 'v_proj', 'o_proj', "gate_proj", "down_proj", "up_proj"],
        lora_dropout=0.05,
        bias="none",
        task_type=TaskType.CAUSAL_LM
    )
    model = get_peft_model(model, lora_config)

    # Training arguments
    training_args = TrainingArguments(
        output_dir=output_dir,
        num_train_epochs=3,
        per_device_train_batch_size=2,
        gradient_accumulation_steps=4,
        learning_rate=2e-4,
        fp16=True,
        save_total_limit=3,
        logging_steps=100,
        save_steps=500,
        eval_steps=500,
        evaluation_strategy="steps",
        load_best_model_at_end=True,
    )

    # Initialize trainer
    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=dataset_dict['train'],
        eval_dataset=dataset_dict['test'],
        tokenizer=tokenizer,
        data_collator=MultimodalDataCollator(tokenizer),
    )

    # Start training
    trainer.train()

    # Save the fine-tuned model
    trainer.save_model(output_dir)

# fine_tune_phi2(image_embeddings_path, dataset_path, output_dir,tokenizer)

# phi2_multimodal_finetuning.py

import os
import numpy as np
import torch
import torch.nn as nn
from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments, Trainer, WhisperFeatureExtractor, WhisperModel, PreTrainedModel
from peft import prepare_model_for_kbit_training, LoraConfig, get_peft_model, TaskType
from datasets import Dataset, DatasetDict
from tqdm import tqdm
import json
import librosa
import os
from dataclasses import dataclass
from typing import Any, Dict, List, Union

# Dependencies:
# pip install transformers peft datasets tqdm torch numpy

# Utility functions (load_embeddings, process_audio, etc.)
def load_embeddings(file_path):
  data = np.load(file_path)
  print(f"Contents of {file_path}:")
  print(f"Keys: {data.files}")
  for key in data.files:
      print(f"- {key}: shape {data[key].shape}")
  return {
        'embeddings': data['embeddings'],
        'ids': data['image_ids']
    }

def process_audio(audio_path, feature_extractor, whisper_model):
    audio, sr = librosa.load(audio_path, sr=16000)
    inputs = feature_extractor(audio, sampling_rate=sr, return_tensors="pt")
    with torch.no_grad():
        outputs = whisper_model(**inputs)
    audio_embedding = outputs.last_hidden_state.mean(dim=1)
    return audio_embedding.squeeze()

# Cell 2: Model Definitions
class MultimodalProjector(nn.Module):
    def __init__(self,hidden_size, image_input_dim, audio_input_dim, output_dim):
        super().__init__()
        self.image_proj = nn.Linear(image_input_dim, output_dim)
        self.audio_proj = nn.Linear(audio_input_dim, output_dim)

    def forward(self, image_embedding, audio_embedding):
        return self.image_proj(image_embedding) + self.audio_proj(audio_embedding)

class Phi2WithProjector(PreTrainedModel):
    def __init__(self, config, phi2_model, projector):
        super().__init__(config)
        self.config = config
        self.phi2_model = phi2_model
        self.projector = projector

    def forward(self, input_ids=None, attention_mask=None, inputs_embeds=None, image_embeddings=None, audio_embeddings=None, labels=None): # Added inputs_embeds
        if input_ids is not None:
            # Original logic if input_ids are provided
            projected_embeddings = self.projector(image_embeddings, audio_embeddings)

            if projected_embeddings is not None:
                text_embeddings = self.phi2_model.transformer.wte(input_ids)
                combined_embeddings = torch.cat([text_embeddings, projected_embeddings], dim=1)
                extended_attention_mask = torch.cat([attention_mask, torch.ones((attention_mask.shape[0], projected_embeddings.shape[1]), device=attention_mask.device)], dim=1)
            else:
                combined_embeddings = self.phi2_model.transformer.wte(input_ids)
                extended_attention_mask = attention_mask

            outputs = self.phi2_model(inputs_embeds=combined_embeddings, attention_mask=extended_attention_mask, labels=labels)

        elif inputs_embeds is not None: # Handle the case where inputs_embeds are provided
            projected_embeddings = self.projector(image_embeddings, audio_embeddings)

            if projected_embeddings is not None:
                combined_embeddings = torch.cat([inputs_embeds, projected_embeddings], dim=1)
                extended_attention_mask = torch.cat([attention_mask, torch.ones((attention_mask.shape[0], projected_embeddings.shape[1]), device=attention_mask.device)], dim=1)
            else:
                combined_embeddings = inputs_embeds
                extended_attention_mask = attention_mask

            outputs = self.phi2_model(inputs_embeds=combined_embeddings, attention_mask=extended_attention_mask, labels=labels)

        else:
            raise ValueError("You have to specify either input_ids or inputs_embeds")

        return outputs

    def prepare_inputs_for_generation(self, input_ids, past=None, attention_mask=None, **kwargs):
        # This method is called during generation
        inputs = self.phi2_model.prepare_inputs_for_generation(input_ids, past=past, attention_mask=attention_mask, **kwargs)

        # Add image and audio embeddings if they're in kwargs
        inputs['image_embeddings'] = kwargs.get('image_embeddings', None)
        inputs['audio_embeddings'] = kwargs.get('audio_embeddings', None)

        return inputs

    def _reorder_cache(self, past, beam_idx):
        # This method is needed for beam search
        return self.phi2_model._reorder_cache(past, beam_idx)


@dataclass
class MultimodalDataCollator:
    """
    Data collator that handles multimodal inputs (text, image, and audio).
    This collator will pad the sequences to the maximum length in the batch.
    """
    tokenizer: Any  # This should be the tokenizer you're using

    def __call__(self, features: List[Dict[str, Any]]) -> Dict[str, Any]:
        batch = {}

        # Handle text inputs
        input_ids = [feature["input_ids"] for feature in features]
        batch["input_ids"] = self.tokenizer.pad(
            {"input_ids": input_ids},
            padding=True,
            return_tensors="pt",
        )["input_ids"]

        batch["attention_mask"] = torch.ones_like(batch["input_ids"])
        batch["labels"] = batch["input_ids"].clone()

        # Handle image embeddings
        if "image_embeddings" in features[0]:
            batch["image_embeddings"] = torch.stack([
                feature["image_embeddings"] for feature in features
            ])

        # Handle audio embeddings
        if "audio_embeddings" in features[0]:
            batch["audio_embeddings"] = torch.stack([
                feature["audio_embeddings"] for feature in features
            ])

        return batch

# Cell 5: Execution
if __name__ == "__main__":
    image_embeddings_path = "/content/drive/MyDrive/LLM/clip_embeddings_instruct150k_15Oct_2024.npz"
    dataset_path = "/content/llava_instruct_150k.json"
    output_dir = "/content/drive/MyDrive/LLM_training_output"
    whisper_model_name = "openai/whisper-base"
    cache_dir = "/content/"

    # Prepare dataset (run this only once)
    dataset_dict, tokenizer = prepare_dataset(image_embeddings_path, dataset_path, whisper_model_name, cache_dir)

# Train model (can be run multiple times with different parameters)
train_model(dataset_dict, tokenizer, output_dir)















"""Alo"""

import numpy as np
import requests
from tqdm import tqdm
import os, gc
import subprocess
import json
import random
### Download Phi-3 model
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments, BitsAndBytesConfig, PreTrainedModel
import torch.nn as nn
from transformers.trainer_callback import TrainerCallback
from datasets import Dataset, DatasetDict
# import joblib

from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training
from trl import SFTTrainer

# from phi3_with_projector import Phi3WithProjector, ImageProjector

# Check if CUDA is available and set the device
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"Using device: {device}")

"""### Download Phi-2 model"""

# Load the Phi-3 model and tokenizer
model_name = "microsoft/phi-2"
tokenizer = AutoTokenizer.from_pretrained(model_name,
                                           padding_side="right",
                                          add_eos_token=True,
                                           trust_remote_code=True)
tokenizer.pad_token = tokenizer.eos_token
tokenizer.truncation_side = "left"

# # Utility function to load processed data for training
# def load_processed_data(file_path):
#     try:
#         data = np.load(file_path)
#         return {
#             'ids': data['image_ids'],
#             'image_embeddings': data['embeddings']
#         }
#     except Exception as e:
#         print(f"Error loading processed data from {file_path}: {e}")
#         return None
# image_embeddings = load_processed_data("/content/drive/MyDrive/LLM/clip_embeddings_instruct150k_15Oct_2024.npz")
# image_embeddings

def prepare_dataset(examples):
    image_embeddings = []
    conversations = []

    for idx, conv in enumerate(examples['conversation']):
        image_embedding = torch.tensor(examples['image_embedding'][idx])
        dialogue_pairs = []

        for i in range(0, len(conv), 2):
            if i + 1 < len(conv):  # Ensure we have a pair
                human_msg = conv[i]['value'].replace('<image>', '').strip()
                gpt_msg = conv[i + 1]['value']

                dialogue = [
                    {"role": "system", "content": "You are a helpful assistant."},
                    {"role": "user", "content": f"Given the following information, provide a detailed and accurate response:\n{human_msg}\n[An image is provided for this task.]\n"},
                    {"role": "assistant", "content": gpt_msg}
                ]

                dialogue_pairs.append(dialogue)
                image_embeddings.append(image_embedding)

        conversations.extend(dialogue_pairs)

    image_embeddings = torch.stack(image_embeddings)

    tokenized_conversations = tokenizer.apply_chat_template(conversations,
                                                            return_tensors='pt', padding=True)

    return {
        "image_embeddings": image_embeddings,
        "input_ids": tokenized_conversations,
        "attention_mask": torch.ones_like(tokenized_conversations),
        "labels": tokenized_conversations.clone()
    }

# Test the prepare_dataset function with a real training example
def test_prepare_dataset(dataset):
    # Get a batch of examples from the dataset
    batch_size = 1  # You can adjust this as needed
    sample_batch = dataset[0:batch_size]

    print("Original conversations:")
    for message in sample_batch['conversation'][0]:
        print(f"{message['from']}: {message['value']}")

    # Process the sample batch
    result = prepare_dataset(sample_batch)

    # Print the structure of the result
    print("\nResult keys:", result.keys())
    print("Image embeddings shape:", result['image_embeddings'].shape)
    print("Input IDs shape:", result['input_ids'].shape)
    print("Attention mask shape:", result['attention_mask'].shape)
    print("Labels shape:", result['labels'].shape)

    # Decode and print the restructured conversations and labels
    for i in range(batch_size):
        decoded_input = tokenizer.decode(result['input_ids'][i])
        decoded_labels = tokenizer.decode(result['labels'][i])

        print(f"\nRestructured input for sample {i + 1}:")
        print(decoded_input)

        print(f"\nLabels for sample {i + 1}:")
        print(decoded_labels)

        # Optionally, you can print a more readable version of the labels
        print("\nReadable labels (non-padding tokens):")
        readable_labels = tokenizer.decode([token for token in result['labels'][i] if token != -100])
        print(readable_labels)

    # Optionally, you can print attention mask to see where it's applied
    print("\nAttention Mask:")
    print(result['attention_mask'][0])

# # Print embeddings and image names
# for image_name, embedding in image_embeddings.items():
#     print(f"Image: {image_name}")
#     print(f"Embedding shape: {embedding.shape}")
#     print(f"Embedding preview: {embedding[:100]}...")  # Print first 5 values
#     print("-" * 50)
#     break

# print(f"Total number of embeddings: {len(image_embeddings)}")

whisper_processor = WhisperProcessor.from_pretrained("openai/whisper-base")
whisper_model = WhisperModel.from_pretrained("openai/whisper-base")
whisper_projection = torch.nn.Linear(whisper_model.config.d_model, model.config.hidden_size).to(device)
def process_audio(audio_path):
    audio_input, _ = torchaudio.load(audio_path)
    input_features = whisper_processor(audio_input, return_tensors="pt").input_features
    with torch.no_grad():
        audio_features = whisper_model(input_features).last_hidden_state
    return whisper_projection(audio_features)

# Load the downloaded JSON file
json_file = "/content/llava_instruct_150k.json"
with open(json_file, 'r') as f:
    data = json.load(f)
def create_dataset():
    processed_data = []
    print("Processing data...")

    with tqdm(total=len(data)) as pbar:
        for item in data:
            image_file = item['image']

            # Check if image_file exists in image_embeddings['ids']
            if image_file in image_embeddings['ids']:
                index = np.where(image_embeddings['ids'] == image_file)[0]

                if index.size > 0:
                    # Ensure that image_embedding exists at the index
                    image_embedding = image_embeddings['image_embeddings'][index[0]]

                    if image_embedding is not None:
                        processed_data.append({
                            'image': image_file,
                            'image_embedding': image_embedding.tolist(),  # Convert embedding to list
                            'conversation': item['conversations']
                        })
                    else:
                        print(f"Warning: Missing image_embedding for image {image_file}")
                else:
                    print(f"Warning: No index found for image {image_file}")
            else:
                print(f"Warning: {image_file} not found in {image_embeddings['ids']}")

            pbar.update(1)

    print(f"Data processing completed. Total processed items: {len(processed_data)}")

    # Ensure image embeddings and conversations exist in processed_data
    if not processed_data:
        raise ValueError("No valid data processed. Check input data and image embeddings.")

    return Dataset.from_dict({
        "image": [item['image'] for item in processed_data],
        "image_embedding": [item['image_embedding'] for item in processed_data],
        "conversation": [item['conversation'] for item in processed_data]
    })

# Execute the function
print("Creating HuggingFace dataset...")
hf_dataset = create_dataset()

print("HuggingFace dataset creation completed.")
print(f"Total samples in dataset: {len(hf_dataset)}")

# # Load the downloaded JSON file
# json_file = "/content/llava_instruct_150k.json"
# with open(json_file, 'r') as f:
#     data = json.load(f)

# def create_dataset():
#     processed_data = []
#     print("Processing data...")
#     with tqdm(total=len(data)) as pbar:
#         for item in data:
#             # print(item['image'])
#             image_file =item['image']
#             if image_file in image_embeddings['ids']:

#                     index = np.where(image_embeddings['ids'] == image_file)[0]

#                     if index.size > 0:
#                         processed_data.append({
#                             'image': image_file,
#                             'image_embedding': image_embeddings['image_embeddings'][index[0]].tolist(),
#                             'conversation': item['conversations']
#                     })
#             pbar.update(1)

#     print(f"Data processing completed. Total processed items: {len(processed_data)}")

#     return Dataset.from_dict({
#         "image": [item['image'] for item in processed_data],
#         "image_embedding": [item['image_embedding'] for item in processed_data],
#         "conversation": [item['conversation'] for item in processed_data]
#     })

# print("Creating HuggingFace dataset...")
# hf_dataset = create_dataset()

# print("HuggingFace dataset creation completed.")
# print(f"Total samples in dataset: {len(hf_dataset)}")

# # Apply tokenization and prepare the dataset
# print("Applying tokenization and preparing the dataset...")

# import json
# with open("/content/processed_dataset.json", 'r') as f:
#   dataset_processed = json.load(f)

print("HuggingFace dataset creation completed.")
print(f"Total samples in dataset: {len(hf_dataset)}")

tokenizer.chat_template = """
{% if system is defined %}
<|system|>{{system}}<|endoftext|>
{% endif %}
{% for message in messages %}
{% if message.role == 'user' %}
<|user|>{{message.content}}<|endoftext|>
{% elif message.role == 'assistant' %}
<|assistant|>{{message.content}}<|endoftext|>
{% endif %}
{% endfor %}<|assistant|>
"""

#  Run the test
test_prepare_dataset(hf_dataset)


hf_dataset_mapped = hf_dataset.map(
    prepare_dataset,
    batched=True,
    remove_columns=hf_dataset.column_names,
    batch_size=1024  # Adjust based on your memory constraints
).with_format("torch")

# Split the dataset
train_test_split = hf_dataset_mapped.train_test_split(test_size=0.1)

# Create a DatasetDict
dataset_dict = DatasetDict({
    'train': train_test_split['train'],
    'test': train_test_split['test']
})

print(f"Train dataset size: {len(dataset_dict['train'])}")
print(f"Test dataset size: {len(dataset_dict['test'])}")


# Example of accessing an item:
sample = dataset_dict['train'][0]
print(f"Input IDs shape: {len(sample['input_ids'])}")
print(f"Attention mask shape: {len(sample['attention_mask'])}")
print(f"Labels shape: {len(sample['labels'])}")
print(f"Image embeddings: {sample['image_embeddings'].shape}")

dataset_dict

# from google.colab import drive
# drive.mount('/content/drive')

# pip install peft

print(dataset_dict['train'].features)

import os
import torch
import torch.nn as nn
from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments, Trainer, BitsAndBytesConfig
from peft import prepare_model_for_kbit_training
from transformers.trainer_callback import TrainerCallback

# Configuration
model_name = "microsoft/phi-2"
lora_r = 32
lora_alpha = 16
lora_dropout = 0.05
use_4bit = True
bnb_4bit_compute_dtype = "float16"
bnb_4bit_quant_type = "nf4"
use_nested_quant = True
output_dir = "./results"
num_train_epochs = 1
fp16 = False
bf16 = False
per_device_train_batch_size = 2
per_device_eval_batch_size = 4
gradient_accumulation_steps = 4
gradient_checkpointing = True
max_grad_norm = 0.3
learning_rate = 5e-4
weight_decay = 0.001
optim = "paged_adamw_32bit"
lr_scheduler_type = "constant"
max_steps = -1
warmup_ratio = 0.03
group_by_length = True
save_steps = 25
logging_steps = 25
eval_steps = 25
max_seq_length = 256
packing = False
device_map = {"": 0}

# Determine compute dtype
if torch.cuda.is_bf16_supported():
    compute_dtype = torch.bfloat16
else:
    compute_dtype = torch.float16

print(f"Using compute dtype: {compute_dtype}")

# Define the path for saving checkpoints
gdrive_checkpoint_dir = "/content/drive/MyDrive/llm/phi2_checkpoints"
os.makedirs(gdrive_checkpoint_dir, exist_ok=True)

# Custom callback for saving checkpoints
class SaveLatestCheckpointCallback(TrainerCallback):
    def on_save(self, args, state, control, **kwargs):
        if state.is_world_process_zero:
            checkpoint_dir = os.path.join(gdrive_checkpoint_dir, f"checkpoint-{state.global_step}")
            kwargs["model"].save_pretrained(checkpoint_dir)
            kwargs["tokenizer"].save_pretrained(checkpoint_dir)
            projector_path = os.path.join(checkpoint_dir, "image_projector.pth")
            torch.save(kwargs["model"].projector.state_dict(), projector_path)

            prev_checkpoint = os.path.join(gdrive_checkpoint_dir, f"checkpoint-{state.global_step - args.save_steps}")
            if os.path.exists(prev_checkpoint):
                import shutil
                shutil.rmtree(prev_checkpoint)

# Load Phi-2 model
bnb_config = BitsAndBytesConfig(
    load_in_4bit=use_4bit,
    bnb_4bit_quant_type=bnb_4bit_quant_type,
    bnb_4bit_compute_dtype=compute_dtype,
    bnb_4bit_use_double_quant=use_nested_quant,
)

phi2_model = AutoModelForCausalLM.from_pretrained(
    model_name,
    trust_remote_code=True,
    quantization_config=bnb_config,
    device_map='auto',
     attn_implementation="eager",
    torch_dtype=compute_dtype,
)

# Image Projector
class ImageProjector(nn.Module):
    def __init__(self, input_dim, output_dim):
        super().__init__()
        self.proj = nn.Linear(input_dim, output_dim)

    def forward(self, x):
        return self.proj(x)

# Assuming image_embedding_dim is known or can be determined from your dataset
image_embedding_dim = 512  # Example value, adjust as needed
projection_dim = phi2_model.config.hidden_size
projector = ImageProjector(image_embedding_dim, projection_dim).to(phi2_model.device)

projector

# class MultimodalDataCollator:
#     def __init__(self, tokenizer=None, padding='max_length', max_length=None):
#         self.tokenizer = tokenizer
#         self.padding = padding
#         self.max_length = max_length

#     def __call__(self, features):
#         batch = {k: [d[k] for d in features] for k in features[0].keys()}

#         # Stack image embeddings
#         batch['image_embeddings'] = torch.stack(batch['image_embeddings'])

#         # Pad the sequences
#         batch['input_ids'] = torch.nn.utils.rnn.pad_sequence(batch['input_ids'], batch_first=True, padding_value=self.tokenizer.pad_token_id)
#         batch['attention_mask'] = torch.nn.utils.rnn.pad_sequence(batch['attention_mask'], batch_first=True, padding_value=0)
#         batch['labels'] = torch.nn.utils.rnn.pad_sequence(batch['labels'], batch_first=True, padding_value=-100)

#         return batch

# updated code by chargpt
class MultimodalDataCollator:
    def __init__(self, tokenizer=None, padding='max_length', max_length=None):
        self.tokenizer = tokenizer
        self.padding = padding
        self.max_length = max_length

    def __call__(self, features):
        # Collect and organize batch data
        print(features)
        batch = {k: [d[k] for d in features] for k in features[0].keys()}

        # Stack image embeddings (already expected to be a tensor in each feature)
        batch['image_embedding'] = torch.stack([torch.tensor(emb) for emb in batch['image_embedding']])

        # Pad input_ids, attention_mask, and labels sequences to create a batch
        batch['input_ids'] = torch.nn.utils.rnn.pad_sequence(
            [torch.tensor(ids) for ids in batch['input_ids']],
            batch_first=True, padding_value=self.tokenizer.pad_token_id
        )
        batch['attention_mask'] = torch.nn.utils.rnn.pad_sequence(
            [torch.tensor(mask) for mask in batch['attention_mask']],
            batch_first=True, padding_value=0
        )
        batch['labels'] = torch.nn.utils.rnn.pad_sequence(
            [torch.tensor(lbl) for lbl in batch['labels']],
            batch_first=True, padding_value=-100
        )

        return batch

import torch
import torch.nn as nn
from transformers import PreTrainedModel
# discared due to
# AttributeError: 'ellipsis' object has no attribute 'config'
# class Phi2WithProjector(PreTrainedModel):

#     _supports_sdpa = False #added for attention mask
#     def __init__(self, phi2_model, projector):

#         super().__init__(phi2_model.config)
#         self.phi2_model = phi2_model
#         self.projector = projector
#         # Ensure the model knows about gradient checkpointing
#         self.supports_gradient_checkpointing = True

#     def forward(self, input_ids, attention_mask, image_embeddings):
#         projected_image = self.projector(image_embeddings)
#         # Combine text and image embeddings
#         text_embeddings = self.phi2_model.transformer.wte(input_ids)
#         combined_embeddings = torch.cat([text_embeddings, projected_image.unsqueeze(1)], dim=1)

#         # Adjust attention mask to account for the added image token
#         extended_attention_mask = torch.cat([attention_mask, torch.ones((attention_mask.shape[0], 1), device=attention_mask.device)], dim=1)

#         # Pass through the model
#         outputs = self.phi2_model(inputs_embeds=combined_embeddings, attention_mask=extended_attention_mask)
#         return outputs

    # def gradient_checkpointing_enable(self):
    #     self.phi2_model.gradient_checkpointing_enable()

    # def gradient_checkpointing_disable(self):
#     #     self.phi2_model.gradient_checkpointing_disable()

# # The rest of your setup remains the same
# model = Phi2WithProjector(phi2_model, projector)
# model = prepare_model_for_kbit_training(model)

# Training setup
training_args = TrainingArguments(
    output_dir=output_dir,
    num_train_epochs=num_train_epochs,
    per_device_train_batch_size=per_device_train_batch_size,
    gradient_accumulation_steps=gradient_accumulation_steps,
    learning_rate=learning_rate,
    fp16=fp16,
    bf16=bf16,
    gradient_checkpointing=True,
    max_grad_norm=max_grad_norm,
    weight_decay=weight_decay,
    optim=optim,
    lr_scheduler_type=lr_scheduler_type,
    max_steps=max_steps,
    warmup_ratio=warmup_ratio,
    group_by_length=group_by_length,
    save_steps=save_steps,
    logging_steps=logging_steps,
    eval_steps=eval_steps,
)

from transformers import Trainer, TrainingArguments



# Create your datasets and data collator
train_dataset = dataset_dict['train']  # Load your training dataset
eval_dataset = dataset_dict['test']   # Load your evaluation dataset
data_collator = MultimodalDataCollator()  # Your custom collator

# # Initialize Trainer
# trainer = Trainer(
#     model=model,
#     args=training_args,
#     train_dataset=train_dataset,
#     eval_dataset=eval_dataset,
#     data_collator=data_collator,
# )

# # Train the model
# trainer.train()

import torch
import torch.nn as nn
from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments, Trainer, BitsAndBytesConfig, PreTrainedModel
from peft import prepare_model_for_kbit_training

# ... (previous configurations remain the same)

# Load Phi-2 model with eager attention implementation
bnb_config = BitsAndBytesConfig(
    load_in_4bit=use_4bit,
    bnb_4bit_quant_type=bnb_4bit_quant_type,
    bnb_4bit_compute_dtype=compute_dtype,
    bnb_4bit_use_double_quant=use_nested_quant,
)

phi2_model = AutoModelForCausalLM.from_pretrained(
    model_name,
    trust_remote_code=True,
    quantization_config=bnb_config,
    device_map=device_map,
    torch_dtype=compute_dtype,
    attn_implementation="eager"  # Specify eager attention implementation
)

# ... (ImageProjector class remains the same)

class Phi2WithProjector(PreTrainedModel):
    def __init__(self, phi2_model, projector):
        super().__init__(phi2_model.config)
        self.phi2_model = phi2_model
        self.projector = projector
        self.config = phi2_model.config
        self.supports_gradient_checkpointing = True

    def forward(self, input_ids, attention_mask, image):
        projected_image = self.projector(image)
        text_embeddings = self.phi2_model.transformer.wte(input_ids)
        combined_embeddings = torch.cat([text_embeddings, projected_image.unsqueeze(1)], dim=1)

        extended_attention_mask = torch.cat([attention_mask, torch.ones((attention_mask.shape[0], 1), device=attention_mask.device)], dim=1)

        outputs = self.phi2_model(inputs_embeds=combined_embeddings, attention_mask=extended_attention_mask)
        return outputs

    # def gradient_checkpointing_enable(self):
    #     self.phi2_model.gradient_checkpointing_enable()

    # def gradient_checkpointing_disable(self):
    #     self.phi2_model.gradient_checkpointing_disable()

    # Add this method to ensure compatibility
    def get_input_embeddings(self):
        return self.phi2_model.get_input_embeddings()

    # Add this method to ensure compatibility
    def set_input_embeddings(self, value):
        self.phi2_model.set_input_embeddings(value)

# ... (rest of the setup remains the same)

model = Phi2WithProjector(phi2_model, projector)
model = prepare_model_for_kbit_training(model)

# ... (Training setup and execution remain the same)

import torch
import torch.nn as nn
from transformers import PreTrainedModel

# class Phi2WithProjector(PreTrainedModel):
#     def __init__(self, phi2_model, projector):
#         super().__init__(phi2_model.config)
#         self.phi2_model = phi2_model
#         self.projector = projector
#         # Ensure the model knows about gradient checkpointing
#         self.supports_gradient_checkpointing = True

#     def forward(self, input_ids, attention_mask, image):
#         projected_image = self.projector(image)
#         # Combine text and image embeddings
#         text_embeddings = self.phi2_model.transformer.wte(input_ids)
#         combined_embeddings = torch.cat([text_embeddings, projected_image.unsqueeze(1)], dim=1)

#         # Adjust attention mask to account for the added image token
#         extended_attention_mask = torch.cat([attention_mask, torch.ones((attention_mask.shape[0], 1), device=attention_mask.device)], dim=1)

#         # Pass through the model
#         outputs = self.phi2_model(inputs_embeds=combined_embeddings, attention_mask=extended_attention_mask)
#         return outputs

#     def gradient_checkpointing_enable(self, **kwargs):  # Accept arbitrary keyword arguments
#         self.phi2_model.gradient_checkpointing_enable()

#     def gradient_checkpointing_disable(self):
#         self.phi2_model.gradient_checkpointing_disable()

# The rest of your setup remains the same
model = Phi2WithProjector(phi2_model, projector)
model = prepare_model_for_kbit_training(model)

# Training setup
training_args = TrainingArguments(
    output_dir=output_dir,
    num_train_epochs=num_train_epochs,
    per_device_train_batch_size=per_device_train_batch_size,
    gradient_accumulation_steps=gradient_accumulation_steps,
    learning_rate=learning_rate,
    fp16=fp16,
    bf16=bf16,
    gradient_checkpointing=True,  # This will call gradient_checkpointing_enable
    max_grad_norm=max_grad_norm,
    weight_decay=weight_decay,
    optim=optim,
    lr_scheduler_type=lr_scheduler_type,
    max_steps=max_steps,
    warmup_ratio=warmup_ratio,
    group_by_length=group_by_length,
    save_steps=save_steps,
    logging_steps=logging_steps,
    eval_steps=eval_steps,
)

import torch
import torch.nn as nn
from transformers import PreTrainedModel

class Phi2WithProjector(PreTrainedModel):
    def __init__(self, phi2_model, projector):
        super().__init__(phi2_model.config)
        self.phi2_model = phi2_model
        self.projector = projector
        # Ensure the model knows about gradient checkpointing
        self.supports_gradient_checkpointing = True

    def forward(self, input_ids, attention_mask,  **kwargs):
        print(**kwargs)
        image_embeddings = kwargs.get('image_embeddings')
        print(f"image embeddings: {image_embeddings}")
        # Access image embeddings from kwargs, expecting the key 'image' or 'pixel_values'
        # This way it is compatible with both single image and multiple image inputs
        image_embeddings = kwargs.get('image', kwargs.get('pixel_values'))

        projected_image = self.projector(image_embeddings)
        # Combine text and image embeddingsimage_embeddings
        text_embeddings = self.phi2_model.transformer.wte(input_ids)
        combined_embeddings = torch.cat([text_embeddings, projected_image.unsqueeze(1)], dim=1)

        # Adjust attention mask to account for the added image token
        extended_attention_mask = torch.cat([attention_mask, torch.ones((attention_mask.shape[0], 1), device=attention_mask.device)], dim=1)

        # Pass through the model
        outputs = self.phi2_model(inputs_embeds=combined_embeddings, attention_mask=extended_attention_mask)
        return outputs

    # def gradient_checkpointing_enable(self, gradient_checkpointing_kwargs=None):
    #     # Only enable gradient checkpointing for the phi2_model
    #     if gradient_checkpointing_kwargs is not None:
    #         try:
    #             # Enable gradient checkpointing only for phi2_model
    #             self.phi2_model.gradient_checkpointing_enable(gradient_checkpointing_kwargs=gradient_checkpointing_kwargs)
    #         except TypeError:
    #             # If the phi2_model doesn't support the kwargs, enable it without arguments
    #             self.phi2_model.gradient_checkpointing_enable()
    #     else:
    #         self.phi2_model.gradient_checkpointing_enable()

    # def gradient_checkpointing_disable(self):
    #     # Disable gradient checkpointing only for phi2_model
    #     self.phi2_model.gradient_checkpointing_disable()

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=dataset_dict['train'],
    data_collator=data_collator,
    callbacks=[SaveLatestCheckpointCallback()],
)

# Start training
trainer.train()

# class Phi2WithProjector(PreTrainedModel):
#     def __init__(self, phi2_model, projector):
#         super().__init__(phi2_model.config)
#         self.phi2_model = phi2_model
#         self.projector = projector
#         self.supports_gradient_checkpointing = True

#     def forward(self, input_ids, attention_mask, image_embeddings=None):
#         if image_embeddings is not None:
#             projected_image = self.projector(image_embeddings)
#             text_embeddings = self.phi2_model.transformer.wte(input_ids)
#             combined_embeddings = torch.cat([text_embeddings, projected_image.unsqueeze(1)], dim=1)
#             extended_attention_mask = torch.cat([attention_mask, torch.ones((attention_mask.shape[0], 1), device=attention_mask.device)], dim=1)
#             outputs = self.phi2_model(inputs_embeds=combined_embeddings, attention_mask=extended_attention_mask)
#         else:
#             outputs = self.phi2_model(input_ids=input_ids, attention_mask=attention_mask)

#         return outputs

#     # def gradient_checkpointing_enable(self):
    #     # Enable gradient checkpointing in the phi2_model
    #     self.phi2_model.gradient_checkpointing_enable()

    # def gradient_checkpointing_disable(self):
    #     # Disable gradient checkpointing in the phi2_model
    #     self.phi2_model.gradient_checkpointing_disable()

# import torch
# import torch.nn as nn
# from transformers import PreTrainedModel

# class Phi2WithProjector(PreTrainedModel):
#     def __init__(self, phi2_model, projector):
#         super().__init__(phi2_model.config)
#         self.phi2_model = phi2_model
#         self.projector = projector
#         # Enable gradient checkpointing
#         self.supports_gradient_checkpointing = True

#     def forward(self, input_ids, attention_mask, image_embeddings=None):
#         # Process image embeddings if provided
#         if image_embeddings is not None:
#             # Apply the projection layer to the precomputed image embeddings
#             projected_image = self.projector(image_embeddings)

#             # Process text input to get text embeddings
#             text_embeddings = self.phi2_model.transformer.wte(input_ids)

#             # Combine image and text embeddings
#             combined_embeddings = torch.cat([text_embeddings, projected_image.unsqueeze(1)], dim=1)

#             # Adjust attention mask to account for the added image token
#             extended_attention_mask = torch.cat([attention_mask, torch.ones((attention_mask.shape[0], 1), device=attention_mask.device)], dim=1)

#             # Forward pass through Phi-2 model with combined embeddings
#             outputs = self.phi2_model(inputs_embeds=combined_embeddings, attention_mask=extended_attention_mask)
#         else:
#             # If no image embeddings are provided, just process the text input
#             outputs = self.phi2_model(input_ids=input_ids, attention_mask=attention_mask)

#         return outputs

#     def gradient_checkpointing_enable(self):
#         self.phi2_model.gradient_checkpointing_enable()

#     def gradient_checkpointing_disable(self):
#         self.phi2_model.gradient_checkpointing_disable()

























# """### QLoRA set up"""

# # new_model = "ms-phi3-custom"
# lora_r = 32
# lora_alpha = 16
# lora_dropout = 0.05
# use_4bit = True
# bnb_4bit_compute_dtype = "float16"
# bnb_4bit_quant_type = "nf4"
# use_nested_quant = False
# output_dir = "./results"
# num_train_epochs = 1
# fp16 = False
# bf16 = False
# per_device_train_batch_size = 8
# per_device_eval_batch_size = 4
# gradient_accumulation_steps = 8
# gradient_checkpointing = True
# max_grad_norm = 0.3
# learning_rate = 5e-4
# weight_decay = 0.001
# optim = "paged_adamw_32bit"
# lr_scheduler_type = "constant"
# max_steps = -1
# warmup_ratio = 0.03
# group_by_length = True
# save_steps = 25
# logging_steps = 25
# eval_steps = 25 # Evaluate every 25 steps
# max_seq_length = 256
# packing = False
# device_map = {"": 0}

# """### Model Training"""

# # Define the path in Google Drive where you want to save the checkpoints
# gdrive_checkpoint_dir = "/content/drive/MyDrive/multimodel_llm/phi3_checkpoints"

# # Ensure the directory exists
# os.makedirs(gdrive_checkpoint_dir, exist_ok=True)

# class SaveLatestCheckpointCallback(TrainerCallback):
#     def on_save(self, args, state, control, **kwargs):
#         if state.is_world_process_zero:
#             checkpoint_dir = os.path.join(gdrive_checkpoint_dir, f"checkpoint-{state.global_step}")

#             # Save the model and tokenizer
#             kwargs["model"].save_pretrained(checkpoint_dir)
#             kwargs["tokenizer"].save_pretrained(checkpoint_dir)

#             # Save the projector separately
#             projector_path = os.path.join(checkpoint_dir, "image_projector.pth")
#             torch.save(kwargs["model"].projector.state_dict(), projector_path)

#             # Remove previous checkpoint
#             prev_checkpoint = os.path.join(gdrive_checkpoint_dir, f"checkpoint-{state.global_step - args.save_steps}")
#             if os.path.exists(prev_checkpoint):
#                 import shutil
#                 shutil.rmtree(prev_checkpoint)

# if torch.cuda.is_bf16_supported():
#   compute_dtype = torch.bfloat16
# #   attn_implementation = 'flash_attention_2'
# else:
#   compute_dtype = torch.float16
# #   attn_implementation = 'sdpa'

# # print(attn_implementation)
# print(compute_dtype)

model_name

# !pip install -U bitsandbytes

# compute_dtype = getattr(torch, bnb_4bit_compute_dtype)
# bnb_config = BitsAndBytesConfig(
#     load_in_4bit=use_4bit,
#     bnb_4bit_quant_type=bnb_4bit_quant_type,
#     bnb_4bit_compute_dtype=compute_dtype,
#     bnb_4bit_use_double_quant=use_nested_quant,
# )
# # Load the model again for quantization
# ### Download Phi-3 model
# phi3_model = AutoModelForCausalLM.from_pretrained(
#     model_name,
#     trust_remote_code=True,
#     quantization_config=bnb_config,
#     device_map=device_map,
#     torch_dtype=compute_dtype,
#     # attn_implementation=attn_implementation
# )

# print(phi3_model)

# # Initialize the projector
# image_embedding_dim = len(hf_dataset[0]['image_embedding'])
# projection_dim = phi3_model.config.hidden_size  # Get dimension from the model
# projector = ImageProjector(image_embedding_dim, projection_dim).to(device)

# # Combine Phi-3 with the projector
# model = Phi3WithProjector(phi3_model, projector)
# # Prepare the model for k-bit training
# model = prepare_model_for_kbit_training(model)

# print(compute_dtype) , print(model)

# def print_trainable_parameters(model):
#     """
#     Prints the number of trainable parameters in the model.
#     """
#     trainable_params = 0
#     all_param = 0
#     for _, param in model.named_parameters():
#         all_param += param.numel()
#         if param.requires_grad:
#             trainable_params += param.numel()
#     print(
#         f"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}"
#     )

# # Define LoRA configuration
# lora_config = LoraConfig(
#     r=lora_r,
#     lora_alpha=lora_alpha,
#     target_modules=['k_proj', 'q_proj', 'v_proj', 'o_proj', "gate_proj", "down_proj", "up_proj"],
#     lora_dropout=lora_dropout,
#     bias="none",
#     task_type="CAUSAL_LM"
# )

# # Apply LoRA to the model
# model = get_peft_model(model, lora_config)
# print_trainable_parameters(model)

# print(model)

# """### Training"""

# gc.collect()
# torch.cuda.empty_cache()

# # Define training arguments
# training_args = TrainingArguments(
#     output_dir=output_dir,
#     num_train_epochs=num_train_epochs,
#     per_device_train_batch_size=per_device_train_batch_size,
#     per_device_eval_batch_size  = per_device_eval_batch_size,
#     gradient_accumulation_steps=gradient_accumulation_steps,
#     optim=optim,
#     save_steps=save_steps,
#     logging_steps=logging_steps,
#     learning_rate=learning_rate,
#     weight_decay=weight_decay,
#     fp16=fp16,
#     bf16=bf16,
#     max_grad_norm=max_grad_norm,
#     max_steps=max_steps,
#     warmup_ratio=warmup_ratio,
#     group_by_length=group_by_length,
#     lr_scheduler_type=lr_scheduler_type,
#     report_to="all",
#     eval_strategy="steps",
#     eval_steps=eval_steps, # Evaluate every 25 steps

#     # Enable gradient checkpointing
#     gradient_checkpointing=gradient_checkpointing,
#     # Disable data parallelism if not needed
#     ddp_find_unused_parameters=False,
#     save_total_limit=1,  # Keep only the latest checkpoint
# )

# # Custom data collator to handle pre-tokenized inputs
# def custom_data_collator(features):
#     batch = {k: [d[k] for d in features] for k in features[0].keys()}

#     # Stack image embeddings
#     batch['image_embeddings'] = torch.stack(batch['image_embeddings'])

#     # Pad the sequences
#     batch['input_ids'] = torch.nn.utils.rnn.pad_sequence(batch['input_ids'], batch_first=True, padding_value=tokenizer.pad_token_id)
#     batch['attention_mask'] = torch.nn.utils.rnn.pad_sequence(batch['attention_mask'], batch_first=True, padding_value=0)
#     batch['labels'] = torch.nn.utils.rnn.pad_sequence(batch['labels'], batch_first=True, padding_value=-100)

#     return batch

# # Function to select a random subset of the dataset
# def select_subset(dataset, fraction=0.05):
#     num_samples = int(len(dataset) * fraction)
#     indices = random.sample(range(len(dataset)), num_samples)
#     return dataset.select(indices)

# # Select 5% of the training and test datasets
# small_train_dataset = select_subset(dataset_dict['train'], fraction=0.05)
# small_test_dataset = select_subset(dataset_dict['test'], fraction=0.05)

# # Create a new DatasetDict with the smaller datasets
# small_dataset_dict = DatasetDict({
#     'train': small_train_dataset,
#     'test': small_test_dataset
# })

# print(f"Small train dataset size: {len(small_dataset_dict['train'])}")
# print(f"Small test dataset size: {len(small_dataset_dict['test'])}")

# # Initialize the SFTTrainer
# trainer = SFTTrainer(
#     model=model,
#     args=training_args,
#     # train_dataset=dataset_dict['train'],
#     # eval_dataset=dataset_dict['test'],
#     train_dataset=small_dataset_dict['train'],
#     eval_dataset=small_dataset_dict['test'],
#     tokenizer=tokenizer,
#     data_collator=custom_data_collator,
#     peft_config=lora_config,
#     max_seq_length=max_seq_length,
#     packing=packing,
#     callbacks=[SaveLatestCheckpointCallback()],  # Add the custom callback
# )

# # Start training
# trainer.train()

# # Save the fine-tuned model
# # trainer.model.save_pretrained(new_model)
# # Save the final fine-tuned model
# final_model_path = os.path.join(gdrive_checkpoint_dir, "final_model")
# trainer.model.save_pretrained(final_model_path)
# tokenizer.save_pretrained(final_model_path)

# """## sample inference code"""

# gc.collect()
# torch.cuda.empty_cache()

# # Create a custom text generation class
# class CustomTextGenerator:
#     def __init__(self, model, tokenizer):
#         self.model = model
#         self.tokenizer = tokenizer

#     def generate(self, input_text, image_embedding, **generate_kwargs):
#         # Tokenize the input text
#         inputs = self.tokenizer(input_text, return_tensors="pt")
#         input_ids = inputs["input_ids"].to(self.model.device)
#         attention_mask = inputs["attention_mask"].to(self.model.device)

#         # Ensure image_embedding is a tensor and move it to the correct device
#         if not isinstance(image_embedding, torch.Tensor):
#             image_embedding = torch.tensor(image_embedding)
#         image_embedding = image_embedding.to(self.model.device)

#         # Adjust attention_mask to account for the image embedding token
#         image_attention = torch.ones((1, 1), dtype=torch.long, device=self.model.device)
#         attention_mask = torch.cat([image_attention, attention_mask], dim=1)

#         # Generate text
#         outputs = self.model.generate(
#             input_ids=input_ids,
#             attention_mask=attention_mask,
#             image_embeddings=image_embedding.unsqueeze(0),  # Add batch dimension
#             **generate_kwargs
#         )

#         # Decode the generated text
#         generated_text = self.tokenizer.decode(outputs[0], skip_special_tokens=True)
#         return generated_text

# # Initialize the custom text generator
# generator = CustomTextGenerator(model=model, tokenizer=tokenizer)

# # Get a sample from the validation set
# sample = dataset_dict['test'][0]
# image_embedding = sample['image_embeddings']


# def get_first_user_input(decoded_text):
#     # Find the position of the first <|assistant|> tag
#     assistant_pos = decoded_text.find('<|assistant|>')

#     # If <|assistant|> is found, truncate the text
#     if assistant_pos != -1:
#         return decoded_text[:assistant_pos].strip()
#     else:
#         return decoded_text.strip()

# # Decode the input_ids
# full_text = tokenizer.decode(sample['input_ids'], skip_special_tokens=True)

# # Extract only the first user input
# input_text = get_first_user_input(full_text)

# # Generate text
# generated_text = generator.generate(
#     input_text,
#     image_embedding=image_embedding,
#     # max_length=200,
#     # num_return_sequences=1,
#     # do_sample=True,
#     # temperature=0.7,
#     # top_k=50,
#     # top_p=0.95,
#     max_new_tokens=150,
#     num_return_sequences=1,
#     do_sample=True,
#     temperature=0.8,
#     top_k=40,
#     top_p=0.9,
#     repetition_penalty=1.2,
#     no_repeat_ngram_size=3,
# )

# print("Input text:")
# print(input_text)
# print("\nGenerated text:")
print(generated_text)

# """### merge models and save in gdrive"""

# gc.collect()
# torch.cuda.empty_cache()

# # Save the projector
# projector_path = '/content/drive/MyDrive/multimodel_llm/image_projector.pth'
# os.makedirs(os.path.dirname(projector_path), exist_ok=True)
# torch.save(model.projector.state_dict(), projector_path)
# print(f"Projector saved to: {projector_path}")

# # Merge the fine-tuned adapter with the base model
# from peft import AutoPeftModelForCausalLM
# from peft import PeftModel

# # Load the fine-tuned model with the LoRA adapter
# # Reload model in FP16 and merge it with LoRA weights
# base_model = AutoModelForCausalLM.from_pretrained(
#     model_name,
#     low_cpu_mem_usage=True,
#     return_dict=True,
#     torch_dtype=torch.float16,
#     device_map=device_map,
# )
# model = PeftModel.from_pretrained(base_model, final_model_path)

# # Merge the LoRA adapter with the base model
# merged_model = model.merge_and_unload()

# # Define the path to save the merged model in Google Drive
# merged_model_path = '/content/drive/MyDrive/multimodel_llm/merged_phi3_llava_model'

# # Save the merged model
# # merged_model.save_pretrained(merged_model_path)

# # Initialize the projector
# image_embedding_dim = len(hf_dataset[0]['image_embedding'])
# projection_dim = merged_model.config.hidden_size  # Get dimension from the model
# projector = ImageProjector(image_embedding_dim, projection_dim).to(device)
# projector.load_state_dict(torch.load(projector_path))

# # Combine Phi-3 with the projector
# phi3_with_projector = Phi3WithProjector(merged_model, projector)

# # Save the merged model with the projector
# phi3_with_projector.save_pretrained(merged_model_path)

# # Save the tokenizer
# # Reload tokenizer to save it
# tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
# tokenizer.pad_token = tokenizer.eos_token
# tokenizer.padding_side = "right"
# tokenizer.save_pretrained(merged_model_path)

# print(f"Merged model and tokenizer saved to: {merged_model_path}")

# from huggingface_hub import HfApi

# api = HfApi()
# api.upload_folder(
#     folder_path=merged_model_path,
#     repo_id="seemGoel/multimodal-phi3-4k-instruct-llava",
#     repo_type="model",
#     delete_patterns = "*.safetensors",
# )
# print("Model uploaded to Hugging Face Hub")



