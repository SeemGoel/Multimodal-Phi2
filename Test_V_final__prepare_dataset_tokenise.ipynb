{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vDpVn5-1YsXu",
        "outputId": "a3337a68-3e77-46bc-ca1a-dcbd22a9c382"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install xformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wB7yn-zqhtgG",
        "outputId": "c0480b9f-5377-40cd-f2e0-2c489d7487d5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting xformers\n",
            "  Downloading xformers-0.0.28.post2-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (1.0 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from xformers) (1.26.4)\n",
            "Requirement already satisfied: torch==2.5.0 in /usr/local/lib/python3.10/dist-packages (from xformers) (2.5.0+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch==2.5.0->xformers) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch==2.5.0->xformers) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch==2.5.0->xformers) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch==2.5.0->xformers) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch==2.5.0->xformers) (2024.6.1)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch==2.5.0->xformers) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch==2.5.0->xformers) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch==2.5.0->xformers) (3.0.2)\n",
            "Downloading xformers-0.0.28.post2-cp310-cp310-manylinux_2_28_x86_64.whl (16.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.7/16.7 MB\u001b[0m \u001b[31m76.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: xformers\n",
            "Successfully installed xformers-0.0.28.post2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# %%capture\n",
        "# !pip install unsloth\n",
        "# # Also get the latest nightly Unsloth!\n",
        "# !pip uninstall unsloth -y && pip install --upgrade --no-cache-dir \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\""
      ],
      "metadata": {
        "id": "mzhugctAtX0Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wo3qnGrgzDuQ"
      },
      "outputs": [],
      "source": [
        "# !export PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5UNV0dDVzFXb"
      },
      "outputs": [],
      "source": [
        "# import os\n",
        "# os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DbXNHaW9xfk2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "818e2c32-a6c3-4204-de89-c66efa83e018"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/2.6 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m2.6/2.6 MB\u001b[0m \u001b[31m163.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.6/2.6 MB\u001b[0m \u001b[31m73.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: Operation cancelled by user\u001b[0m\u001b[31m\n",
            "\u001b[0m  Building wheel for flash_attn (setup.py) ... \u001b[?25l\u001b[?25h"
          ]
        }
      ],
      "source": [
        "# !pip install -Uq flash_attn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J2QyvO9o2POW",
        "outputId": "03157d7a-dd3c-4634-af58-c56aab015987"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting flash-attn\n",
            "  Using cached flash_attn-2.6.3.tar.gz (2.6 MB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from flash-attn) (2.5.0+cu121)\n",
            "Requirement already satisfied: einops in /usr/local/lib/python3.10/dist-packages (from flash-attn) (0.8.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn) (2024.6.1)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch->flash-attn) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->flash-attn) (3.0.2)\n",
            "Building wheels for collected packages: flash-attn\n",
            "  Building wheel for flash-attn (setup.py) ... \u001b[?25l\u001b[?25hcanceled\n",
            "\u001b[31mERROR: Operation cancelled by user\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "# !export TORCH_CUDA_ARCH_LIST=\"7.0;8.0\"  # Replace with your architecture(s)\n",
        "# !pip install flash-attn --no-build-isolation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "evPMjLN3TkE6",
        "outputId": "51e3dfe9-a4d5-4f71-985c-f175d075293d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/330.9 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m330.9/330.9 kB\u001b[0m \u001b[31m20.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m320.7/320.7 kB\u001b[0m \u001b[31m26.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m122.4/122.4 MB\u001b[0m \u001b[31m18.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m316.6/316.6 kB\u001b[0m \u001b[31m23.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m233.2/233.2 kB\u001b[0m \u001b[31m19.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m78.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m109.8/109.8 kB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m472.7/472.7 kB\u001b[0m \u001b[31m35.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.6/78.6 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m17.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "ipython-sql 0.5.0 requires sqlalchemy>=2.0, but you have sqlalchemy 1.4.54 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "# !pip install -q transformers==4.44.2\n",
        "# !pip install -Uq -q accelerate peft bitsandbytes trl dataset bitsandbytes\n",
        "# !pip install -q -U accelerate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IqaPm4JaYQVG"
      },
      "outputs": [],
      "source": [
        "# !wget -c -q \"https://huggingface.co/datasets/liuhaotian/LLaVA-Instruct-150K/resolve/main/llava_instruct_150k.json\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"prepare_dataset_tokenise.py - Optimized for Multimodal Fine-tuning\"\"\"\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments, Trainer, WhisperProcessor, WhisperForConditionalGeneration, PreTrainedModel,BitsAndBytesConfig\n",
        "from peft import prepare_model_for_kbit_training, LoraConfig, get_peft_model, TaskType\n",
        "from datasets import Dataset, DatasetDict\n",
        "from tqdm import tqdm\n",
        "import json\n",
        "import librosa\n",
        "from dataclasses import dataclass\n",
        "from typing import Any, Dict, List, Union\n",
        "import gc\n",
        "\n",
        "# Initialize Whisper components for audio transcription\n",
        "whisper_model_name = \"openai/whisper-small\"\n",
        "whisper_processor = WhisperProcessor.from_pretrained(whisper_model_name)\n",
        "whisper_model = WhisperForConditionalGeneration.from_pretrained(whisper_model_name)\n",
        "\n",
        "# Load embeddings with error handling\n",
        "def load_embeddings(file_path):\n",
        "    try:\n",
        "        data = np.load(file_path)\n",
        "        if 'image_ids' in data and 'embeddings' in data:\n",
        "            return {'ids': data['image_ids'], 'embeddings': data['embeddings']}\n",
        "        else:\n",
        "            raise ValueError(f\"Unexpected structure in {file_path}.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading embeddings: {e}\")\n",
        "        return None\n",
        "\n",
        "# Process audio files\n",
        "def transcribe_speech(audiopath):\n",
        "    try:\n",
        "        speech, rate = librosa.load(audiopath, sr=16000)\n",
        "        audio_input = whisper_processor(speech, return_tensors=\"pt\", sampling_rate=16000)\n",
        "        with torch.no_grad():\n",
        "            generated_ids = whisper_model.generate(audio_input[\"input_features\"])\n",
        "        return whisper_processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
        "    except Exception as e:\n",
        "        print(f\"Error transcribing audio: {e}\")\n",
        "        return None\n"
      ],
      "metadata": {
        "id": "FNJr2eQK8kqI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define multimodal projector class\n",
        "class ProjectionBlock(nn.Module):\n",
        "    def __init__(self, input_dim, output_dim):\n",
        "        super().__init__()\n",
        "        self.pre_norm = nn.LayerNorm(input_dim)\n",
        "        self.proj = nn.Sequential(nn.Linear(input_dim, output_dim), nn.GELU(), nn.Linear(output_dim, output_dim))\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.proj(self.pre_norm(x))\n",
        "\n",
        "class MultimodalProjector(nn.Module):\n",
        "    def __init__(self, image_input_dim, audio_input_dim, output_dim):\n",
        "        super().__init__()\n",
        "        self.image_proj = ProjectionBlock(image_input_dim, output_dim)\n",
        "        self.audio_proj = ProjectionBlock(audio_input_dim, output_dim)\n",
        "\n",
        "    def forward(self, image_embedding=None, audio_embedding=None):\n",
        "        projected_image = self.image_proj(image_embedding) if image_embedding is not None else None\n",
        "        projected_audio = self.audio_proj(audio_embedding) if audio_embedding is not None else None\n",
        "        return projected_image, projected_audio\n",
        "\n",
        "\n",
        "# Dataset preparation with better error handling and modularization\n",
        "def prepare_dataset(image_embeddings_path, dataset_path, cache_dir=None):\n",
        "    image_embeddings = load_embeddings(image_embeddings_path)\n",
        "    with open(dataset_path, 'r') as f:\n",
        "        data = json.load(f)\n",
        "    processed_data = [{\"conversation\": item[\"conversations\"], \"image_embedding\": image_embeddings['embeddings'][np.where(image_embeddings['ids'] == item['image'])[0][0]] if image_embeddings and \"image\" in item else None, \"audio_path\": item.get(\"audio\")} for item in data]\n",
        "    dataset = Dataset.from_dict({\"conversation\": [item[\"conversation\"] for item in processed_data], \"image_embedding\": [item.get(\"image_embedding\") for item in processed_data], \"audio_path\": [item.get(\"audio_path\") for item in processed_data]})\n",
        "    tokenizer = AutoTokenizer.from_pretrained(\"microsoft/Phi-3-mini-128k-instruct\", trust_remote_code=True)\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "    tokenizer.padding_side = \"right\"\n",
        "    tokenizer.chat_template = \"\"\"\n",
        "    {% for message in messages %}\n",
        "    {% if message.role == 'system' %}<|system|>{{message.content}}<|endoftext|>{% elif message.role == 'user' %}<|user|>{{message.content}}<|endoftext|>{% elif message.role == 'assistant' %}<|assistant|>{{message.content}}<|endoftext|>{% endif %}{% endfor %}\n",
        "    \"\"\"\n",
        "    prepared_dataset = dataset.map(lambda examples: prepare_example(examples, tokenizer), batched=True, remove_columns=dataset.column_names, batch_size=1).with_format(\"torch\")\n",
        "    dataset_dict = DatasetDict({\"train\": prepared_dataset.train_test_split(test_size=0.1)[\"train\"], \"test\": prepared_dataset.train_test_split(test_size=0.1)[\"test\"]})\n",
        "    if cache_dir:\n",
        "        os.makedirs(cache_dir, exist_ok=True)\n",
        "        dataset_dict.save_to_disk(cache_dir)\n",
        "    return dataset_dict, tokenizer\n",
        "\n",
        "# Example preparation for dataset rows\n",
        "def prepare_example(examples, tokenizer):\n",
        "    image_embeddings, audio_embeddings, tokenized_inputs = [], [], []\n",
        "    for idx, conv in enumerate(examples[\"conversation\"]):\n",
        "        image_embedding = torch.tensor(examples[\"image_embedding\"][idx]) if examples[\"image_embedding\"][idx] is not None else None\n",
        "        transcription = transcribe_speech(examples[\"audio_path\"][idx]) if \"audio_path\" in examples and examples[\"audio_path\"][idx] else None\n",
        "        for i in range(0, len(conv), 2):\n",
        "            if i + 1 < len(conv):\n",
        "                human_msg = conv[i][\"value\"].replace(\"<image>\", \"\").replace(\"<audio>\", \"\").strip()\n",
        "                if transcription:\n",
        "                    human_msg += f\"\\nAudio Transcription: {transcription}\"\n",
        "                gpt_msg = conv[i + 1][\"value\"]\n",
        "                tokenized_input = tokenizer.apply_chat_template([{\"role\": \"system\", \"content\": \"You are a helpful assistant.\"}, {\"role\": \"user\", \"content\": f\"{human_msg}\"}, {\"role\": \"assistant\", \"content\": gpt_msg}], return_tensors=\"pt\", padding=True)\n",
        "                tokenized_inputs.append(tokenized_input.squeeze(0))\n",
        "                if image_embedding is not None:\n",
        "                    image_embeddings.append(image_embedding)\n",
        "    max_length = max(input.shape[0] for input in tokenized_inputs)\n",
        "    padded_inputs = [torch.nn.functional.pad(input, (0, max_length - input.shape[0])) for input in tokenized_inputs]\n",
        "    result = {\"input_ids\": torch.stack(padded_inputs), \"attention_mask\": torch.stack(padded_inputs).ne(tokenizer.pad_token_id).long(), \"labels\": torch.stack(padded_inputs).clone()}\n",
        "    if image_embeddings:\n",
        "        result[\"image_embeddings\"] = torch.stack(image_embeddings)\n",
        "    if audio_embeddings:\n",
        "        result[\"audio_embeddings\"] = torch.stack(audio_embeddings)\n",
        "    return result\n"
      ],
      "metadata": {
        "id": "WCl3YUIo-SQv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.checkpoint import checkpoint_sequential"
      ],
      "metadata": {
        "id": "4Fn5cp323TtZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Phi3WithProjector(PreTrainedModel):\n",
        "    def __init__(self, config, phi3_model, projector):\n",
        "        super().__init__(config)\n",
        "        self.phi3_model = phi3_model\n",
        "        self.projector = projector\n",
        "\n",
        "    def forward(self, input_ids=None, attention_mask=None, inputs_embeds=None, image_embeddings=None, audio_embeddings=None, labels=None, **kwargs):\n",
        "        # Use get_input_embeddings() to retrieve the embeddings layer\n",
        "        if inputs_embeds is None:\n",
        "            inputs_embeds = self.phi3_model.get_input_embeddings()(input_ids)\n",
        "\n",
        "        # Project both image and audio embeddings to the appropriate dimension\n",
        "        projected_image, projected_audio = self.projector(image_embeddings, audio_embeddings)\n",
        "\n",
        "        # Concatenate the embeddings\n",
        "        embeddings_to_concat = [inputs_embeds]\n",
        "        if projected_image is not None:\n",
        "            embeddings_to_concat.append(projected_image.unsqueeze(1))\n",
        "        if projected_audio is not None:\n",
        "            embeddings_to_concat.append(projected_audio.unsqueeze(1))\n",
        "\n",
        "        combined_embeddings = torch.cat(embeddings_to_concat, dim=1)\n",
        "\n",
        "        # Modify how the attention mask is extended\n",
        "        extended_attention_mask = attention_mask.clone()  # Start with a copy\n",
        "\n",
        "        # Extend for image and audio, if present\n",
        "        if projected_image is not None:\n",
        "            extended_attention_mask = torch.cat([extended_attention_mask, torch.ones_like(extended_attention_mask[:, :1])], dim=1)\n",
        "        if projected_audio is not None:\n",
        "            extended_attention_mask = torch.cat([extended_attention_mask, torch.ones_like(extended_attention_mask[:, :1])], dim=1)\n",
        "\n",
        "        # Adjust labels to match the extended input sequence length\n",
        "        if labels is not None:\n",
        "            # Pad labels with -100 to ignore the added tokens in the loss calculation\n",
        "            num_added_tokens = sum(1 for emb in [projected_image, projected_audio] if emb is not None)\n",
        "            labels = torch.cat([labels, torch.full((labels.shape[0], num_added_tokens), -100, dtype=labels.dtype, device=labels.device)], dim=1)\n",
        "\n",
        "        return self.phi3_model(\n",
        "            inputs_embeds=combined_embeddings,\n",
        "            attention_mask=extended_attention_mask,\n",
        "            labels=labels,\n",
        "            **kwargs\n",
        "        )\n",
        "\n",
        "    def get_input_embeddings(self):\n",
        "        \"\"\"Returns the model's input embeddings.\"\"\"\n",
        "        return self.phi3_model.get_input_embeddings()\n",
        "\n",
        "    def set_input_embeddings(self, value):\n",
        "        \"\"\"Sets the model's input embeddings.\"\"\"\n",
        "        self.phi3_model.set_input_embeddings(value)\n"
      ],
      "metadata": {
        "id": "Of1wqy7998iT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(dataset_dict, tokenizer, output_dir):\n",
        "    from transformers import BitsAndBytesConfig\n",
        "\n",
        "    # Configure quantization\n",
        "    quantization_config = BitsAndBytesConfig(\n",
        "        load_in_8bit=True,\n",
        "        load_in_8bit_fp32_cpu_offload=True\n",
        "    )\n",
        "    # Explicitly define device_map to load the model onto the GPU\n",
        "    device_map = {\"\": 0}  # Load the model to the first GPU (index 0\n",
        "\n",
        "    # Load the main model with dynamic quantization configuration\n",
        "    phi3_model = AutoModelForCausalLM.from_pretrained(\n",
        "        \"microsoft/Phi-3-mini-128k-instruct\",\n",
        "        quantization_config=quantization_config,\n",
        "        torch_dtype=torch.float16,\n",
        "        device_map=device_map\n",
        "    )\n",
        "\n",
        "    phi3_model = prepare_model_for_kbit_training(phi3_model)\n",
        "\n",
        "    # Set output_dim dynamically based on the model's hidden size\n",
        "    output_dim = phi3_model.config.hidden_size\n",
        "\n",
        "    # Initialize MultimodalProjector with the dynamically set output_dim\n",
        "    projector = MultimodalProjector(\n",
        "        image_input_dim=512,     # Replace with actual image embedding dimension if different\n",
        "        audio_input_dim=512,     # Replace with actual audio embedding dimension if different\n",
        "        output_dim=output_dim    # Dynamic output dimension based on model's hidden size\n",
        "    )\n",
        "\n",
        "    # Initialize Phi3WithProjector model\n",
        "    model = Phi3WithProjector(phi3_model.config, phi3_model, projector )\n",
        "\n",
        "    # Configure LoRA\n",
        "    lora_config = LoraConfig(\n",
        "        r=8,\n",
        "        lora_alpha=16,\n",
        "        target_modules=[\"self_attn.qkv_proj\", \"self_attn.o_proj\"],\n",
        "        lora_dropout=0.05,\n",
        "        bias=\"none\",\n",
        "        task_type=TaskType.CAUSAL_LM\n",
        "    )\n",
        "    model = get_peft_model(model, lora_config)\n",
        "\n",
        "\n",
        "    # Training arguments\n",
        "    training_args = TrainingArguments(\n",
        "        output_dir=output_dir,\n",
        "        num_train_epochs=1/1000,\n",
        "        per_device_train_batch_size=2,\n",
        "        gradient_accumulation_steps=8,\n",
        "        learning_rate=2e-4,\n",
        "        fp16=True,\n",
        "        save_total_limit=2,\n",
        "        logging_steps=100,\n",
        "        save_steps=500,\n",
        "        eval_steps=500,\n",
        "        eval_strategy=\"steps\",\n",
        "        load_best_model_at_end=True,\n",
        "        remove_unused_columns=False\n",
        "\n",
        "    )\n",
        "\n",
        "    # Trainer setup\n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        train_dataset=dataset_dict['train'],\n",
        "        eval_dataset=dataset_dict['test'],\n",
        "        tokenizer=tokenizer,\n",
        "        data_collator=MultimodalDataCollator(tokenizer),\n",
        "        callbacks=[EarlyStoppingCallback(early_stopping_patience=3)]\n",
        "    )\n",
        "\n",
        "\n",
        "\n",
        "    trainer.train()\n",
        "    trainer.save_model(output_dir)\n"
      ],
      "metadata": {
        "id": "1eXm2wzTtWpI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    image_embeddings_path = \"/content/drive/MyDrive/LLM/Copy of clip_embeddings_instruct150k_15Oct_2024.npz\"\n",
        "    dataset_path = \"/content/llava_instruct_150k.json\"\n",
        "    output_dir = \"/content/drive/MyDrive/LLM_training_output\"\n",
        "    whisper_model_name = \"openai/whisper-small\"\n",
        "    cache_dir = \"/content/\"\n",
        "    #dataset_dict, tokenizer = prepare_dataset(image_embeddings_path, dataset_path, cache_dir)\n",
        "    if os.path.exists(cache_dir):\n",
        "        dataset_dict = DatasetDict.load_from_disk(cache_dir)\n",
        "    else:\n",
        "      dataset_dict, tokenizer = prepare_dataset(image_embeddings_path, dataset_path, cache_dir)\n"
      ],
      "metadata": {
        "id": "ME81wbO-8knI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the EarlyStoppingCallback\n",
        "from transformers import EarlyStoppingCallback"
      ],
      "metadata": {
        "id": "iO5lJw2S0dL5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0epwsOtA4NLT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "yGxLK04EY6UB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_model(dataset_dict, tokenizer, output_dir)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 214,
          "referenced_widgets": [
            "cea6e51eaa0441b3a44d94a066719677",
            "8243bb91b37141f4bf959f579e2ae34e",
            "61270edc331b4540b6cb018e77c08544",
            "bae445ab51874749abea2a752d53df30",
            "5104700260ef4337879f98a1a278bc71",
            "a691720901564d93acaa5bd1f9b02bb5",
            "cf48609992f94b01bc0ac474423825eb",
            "4797a5b7839e4c0bbdcdf60a04ed7f6e",
            "d201dd7371bc4f0597190dca047cca8b",
            "2caa1b6b144c47db99524afcd1d9f8f1",
            "1dc8548faf2042e3ab6e1921b35a6e5f"
          ]
        },
        "id": "DGFeFzVe8kkI",
        "outputId": "9ac955bb-c643-4cbe-db8f-ee3b97e3548b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Unused kwargs: ['load_in_8bit_fp32_cpu_offload']. These kwargs are not used in <class 'transformers.utils.quantization_config.BitsAndBytesConfig'>.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "cea6e51eaa0441b3a44d94a066719677"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  return fn(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/autograd/_functions.py:316: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
            "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='21' max='21' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [21/21 06:58, Epoch 0/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CLkgKkWqf4c2"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "12cZDCuIf4Sv"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "cea6e51eaa0441b3a44d94a066719677": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_8243bb91b37141f4bf959f579e2ae34e",
              "IPY_MODEL_61270edc331b4540b6cb018e77c08544",
              "IPY_MODEL_bae445ab51874749abea2a752d53df30"
            ],
            "layout": "IPY_MODEL_5104700260ef4337879f98a1a278bc71"
          }
        },
        "8243bb91b37141f4bf959f579e2ae34e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a691720901564d93acaa5bd1f9b02bb5",
            "placeholder": "​",
            "style": "IPY_MODEL_cf48609992f94b01bc0ac474423825eb",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "61270edc331b4540b6cb018e77c08544": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4797a5b7839e4c0bbdcdf60a04ed7f6e",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_d201dd7371bc4f0597190dca047cca8b",
            "value": 2
          }
        },
        "bae445ab51874749abea2a752d53df30": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2caa1b6b144c47db99524afcd1d9f8f1",
            "placeholder": "​",
            "style": "IPY_MODEL_1dc8548faf2042e3ab6e1921b35a6e5f",
            "value": " 2/2 [00:05&lt;00:00,  2.81s/it]"
          }
        },
        "5104700260ef4337879f98a1a278bc71": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a691720901564d93acaa5bd1f9b02bb5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cf48609992f94b01bc0ac474423825eb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4797a5b7839e4c0bbdcdf60a04ed7f6e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d201dd7371bc4f0597190dca047cca8b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "2caa1b6b144c47db99524afcd1d9f8f1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1dc8548faf2042e3ab6e1921b35a6e5f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}