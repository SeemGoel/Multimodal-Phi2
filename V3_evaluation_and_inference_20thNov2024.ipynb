{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SeemGoel/Multimodal-Phi2/blob/experiments/V3_evaluation_and_inference_20thNov2024.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vDpVn5-1YsXu",
        "outputId": "bd56ee15-a6df-4073-8ade-f1b9579999f0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M99_nQ2ajIEH"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vYAgi7K_7rCT"
      },
      "outputs": [],
      "source": [
        "# import torch\n",
        "# print(torch.cuda.is_available())  # Should return True\n",
        "# print(torch.cuda.current_device())  # Should return the device index\n",
        "# print(torch.cuda.get_device_name(torch.cuda.current_device()))  # Shou"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jhSXNLpY5P9f"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xJU5LK5LIclR",
        "outputId": "4ecc553e-ee35-4149-8d6c-1c9015e7b106"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/Cap_project\n"
          ]
        }
      ],
      "source": [
        "cd /content/drive/MyDrive/Cap_project"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o7pAVAoGl-Xa"
      },
      "outputs": [],
      "source": [
        "# !pip install -q -U flash-attn #--no-build-isolation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xAl3COh8fmxs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7e1cc386-bece-4fe5-b434-4b7369aa758a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: bitsandbytes in /usr/local/lib/python3.10/dist-packages (0.44.1)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from bitsandbytes) (2.5.1+cu121)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from bitsandbytes) (1.26.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (2024.9.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch->bitsandbytes) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->bitsandbytes) (3.0.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install -U bitsandbytes\n",
        "!pip install -Uq -q accelerate peft bitsandbytes trl dataset bitsandbytes\n",
        "!pip install -q -U accelerate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_YxllywLcFil"
      },
      "outputs": [],
      "source": [
        "# !pip install datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2wAkZA4V6bFW"
      },
      "outputs": [],
      "source": [
        "# !pip install -r requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "np5SkhnGjjUe",
        "outputId": "be0cf98d-0a1e-413c-d7ff-c749bfbb9881"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "12.1\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "print(torch.version.cuda)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mzhugctAtX0Y"
      },
      "outputs": [],
      "source": [
        "# %%capture\n",
        "# !pip install unsloth\n",
        "# # Also get the latest nightly Unsloth!\n",
        "# !pip uninstall unsloth -y && pip install --upgrade --no-cache-dir \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lb9OXMVb2yNM"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wo3qnGrgzDuQ"
      },
      "outputs": [],
      "source": [
        "!export PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "61T4utikN_mK"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5UNV0dDVzFXb"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import datasets\n",
        "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IqaPm4JaYQVG"
      },
      "outputs": [],
      "source": [
        "!wget -c -q \"https://huggingface.co/datasets/liuhaotian/LLaVA-Instruct-150K/resolve/main/llava_instruct_150k.json\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4Fn5cp323TtZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "33127650-4e21-41d8-d6fc-f9bd7395e333"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "from torch.utils.checkpoint import checkpoint_sequential\n",
        "#from modal import ProjectionBlock,MultimodalProjector,Phi3WithProjector\n",
        "from modal import ProjectionBlock, EnhancedMultimodalProjector,Phi3WithProjector\n",
        "from features import load_embeddings,transcribe_speech, MultimodalDataCollator, prepare_dataset, prepare_example"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q6kWdX694Xv6"
      },
      "outputs": [],
      "source": [
        "from config import TrainingConfig"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1eXm2wzTtWpI"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from peft import LoraConfig, TaskType, get_peft_model\n",
        "from transformers.trainer_utils import PREFIX_CHECKPOINT_DIR\n",
        "import torch\n",
        "from torch.utils.data import DataLoader, RandomSampler\n",
        "from config import TrainingConfig\n",
        "from transformers import AutoModelForCausalLM\n",
        "from peft import LoraConfig, TaskType, get_peft_model, prepare_model_for_kbit_training\n",
        "\n",
        "\n",
        "from transformers import TrainerCallback, TrainerState, TrainerControl\n",
        "from transformers import Trainer, TrainingArguments, EarlyStoppingCallback # Import EarlyStoppingCallback\n",
        "\n",
        "import torch\n",
        "import gc\n",
        "from typing import Dict, Union\n",
        "import os\n",
        "\n",
        "class OOMSafeCallback(TrainerCallback):\n",
        "    \"\"\"Custom callback to handle OOM errors and ensure safe model saving\"\"\"\n",
        "    def __init__(self, save_dir: str):\n",
        "        self.save_dir = save_dir\n",
        "        self.best_loss = float('inf')\n",
        "\n",
        "    def on_step_end(self, args, state: TrainerState, control: TrainerControl, **kwargs):\n",
        "        \"\"\"Check memory usage after each step\"\"\"\n",
        "        if torch.cuda.is_available():\n",
        "            # Monitor GPU memory\n",
        "            gpu_memory = torch.cuda.memory_allocated() / 1024**2  # Convert to MB\n",
        "            if gpu_memory > 0.95 * torch.cuda.get_device_properties(0).total_memory / 1024**2:\n",
        "                print(f\"Warning: High GPU memory usage detected: {gpu_memory:.2f}MB\")\n",
        "\n",
        "    def on_save(self, args, state: TrainerState, control: TrainerControl, **kwargs):\n",
        "        \"\"\"Handle safe model saving\"\"\"\n",
        "        try:\n",
        "            # Clear memory before saving\n",
        "            gc.collect()\n",
        "            torch.cuda.empty_cache()\n",
        "\n",
        "            # Get model from kwargs\n",
        "            model = kwargs.get('model')\n",
        "            if model is None:\n",
        "                return\n",
        "\n",
        "            # Move model to CPU for saving if needed\n",
        "            device = next(model.parameters()).device\n",
        "            if device.type != 'cpu':\n",
        "                model.to('cpu')\n",
        "\n",
        "            # Save in try-except block\n",
        "            try:\n",
        "                checkpoint_dir = os.path.join(\n",
        "                    self.save_dir,\n",
        "                    f\"checkpoint-{state.global_step}\"\n",
        "                )\n",
        "                os.makedirs(checkpoint_dir, exist_ok=True)\n",
        "\n",
        "                # Save model state separately\n",
        "                model.save_pretrained(checkpoint_dir)\n",
        "\n",
        "                # Save training state\n",
        "                state_dict = {\n",
        "                    \"step\": state.global_step,\n",
        "                    \"epoch\": state.epoch,\n",
        "                    \"best_loss\": self.best_loss\n",
        "                }\n",
        "                torch.save(state_dict, os.path.join(checkpoint_dir, \"trainer_state.bin\"))\n",
        "\n",
        "            finally:\n",
        "                # Move model back to original device\n",
        "                if device.type != 'cpu':\n",
        "                    model.to(device)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Warning: Error during saving: {str(e)}\")\n",
        "            # Attempt emergency save\n",
        "            try:\n",
        "                emergency_save_path = os.path.join(self.save_dir, \"emergency_save\")\n",
        "                os.makedirs(emergency_save_path, exist_ok=True)\n",
        "\n",
        "                # Save in 8-bit format to reduce memory\n",
        "                model.save_pretrained(\n",
        "                    emergency_save_path,\n",
        "                    save_in_8bit=True,\n",
        "                    max_shard_size=\"500MB\"\n",
        "                )\n",
        "                print(f\"Emergency save successful at: {emergency_save_path}\")\n",
        "            except Exception as e2:\n",
        "                print(f\"Emergency save failed: {str(e2)}\")\n",
        "\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    \"\"\"Compute BLEU score for model evaluation\"\"\"\n",
        "    predictions = eval_pred.predictions\n",
        "    labels = eval_pred.label_ids\n",
        "\n",
        "    # Ensure predictions and labels are decoded to text first\n",
        "    # This depends on your tokenizer and data format\n",
        "    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
        "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
        "\n",
        "    bleu_scores = []\n",
        "    for pred, label in zip(decoded_preds, decoded_labels):\n",
        "        # Convert strings to tokens for BLEU calculation\n",
        "        pred_tokens = pred.split()\n",
        "        label_tokens = label.split()\n",
        "        bleu_scores.append(sentence_bleu([label_tokens], pred_tokens))\n",
        "\n",
        "    avg_bleu = sum(bleu_scores) / len(bleu_scores) if bleu_scores else 0\n",
        "    return {\"bleu\": avg_bleu}\n",
        "\n",
        "\n",
        "def train_model(dataset_dict, tokenizer, output_dir):\n",
        "    from transformers import BitsAndBytesConfig\n",
        "    from transformers import logging as transformers_logging\n",
        "\n",
        "    config = TrainingConfig()\n",
        "\n",
        "    # Set logging verbosity\n",
        "    transformers_logging.set_verbosity_info()  # Or set_verbosity_debug() for even more detailed output\n",
        "\n",
        "\n",
        "    # Configure quantization commented on 20 november\n",
        "    # quantization_config = BitsAndBytesConfig(\n",
        "    #     load_in_8bit=True,\n",
        "    #     load_in_8bit_fp32_cpu_offload=True,\n",
        "    #     llm_int8_threshold=6.0,\n",
        "    #     llm_int8_has_fp16_weight=False,\n",
        "    #     bnb_4bit_compute_dtype=torch.float16\n",
        "    # )\n",
        "    quantization_config = BitsAndBytesConfig(\n",
        "        load_in_4bit=True,\n",
        "        bnb_4bit_quant_storage=\"float32\",\n",
        "        bnb_4bit_compute_dtype=\"bfloat16\",\n",
        "        bnb_4bit_use_double_quant=True\n",
        "    )\n",
        "    # Explicitly define device_map to load the model onto the GPU\n",
        "    device_map = {\"\": 0}  # Load the model to the first GPU (index 0\n",
        "\n",
        "    # Load the main model with dynamic quantization configuration\n",
        "    # phi3_model = AutoModelForCausalLM.from_pretrained(\n",
        "    #     \"microsoft/Phi-3-mini-128k-instruct\",\n",
        "    #     quantization_config=quantization_config,\n",
        "    #     torch_dtype=torch.float16,\n",
        "    #     device_map=device_map\n",
        "    # )\n",
        "\n",
        "    # Clear GPU memory before loading model\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.empty_cache()\n",
        "        gc.collect()\n",
        "\n",
        "    try:\n",
        "        phi3_model = AutoModelForCausalLM.from_pretrained(\n",
        "            \"microsoft/Phi-3-mini-128k-instruct\",\n",
        "            quantization_config=quantization_config,\n",
        "            torch_dtype=torch.float16,\n",
        "            device_map=device_map  # Let transformers handle device mapping\n",
        "        )\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading model: {str(e)}\")\n",
        "        raise\n",
        "\n",
        "    phi3_model = prepare_model_for_kbit_training(phi3_model)\n",
        "\n",
        "\n",
        "    # Set output_dim dynamically based on the model's hidden size\n",
        "    output_dim = phi3_model.config.hidden_size\n",
        "\n",
        "    # Initialize MultimodalProjector with the dynamically set output_dim\n",
        "    projector = EnhancedMultimodalProjector(\n",
        "        image_input_dim=512,\n",
        "        audio_input_dim=512,\n",
        "        output_dim=output_dim    # Dynamic output dimension based on model's hidden size\n",
        "    )\n",
        "\n",
        "    # Initialize Phi3WithProjector model\n",
        "    model = Phi3WithProjector(phi3_model.config, phi3_model, projector )\n",
        "\n",
        "    # Configure LoRA commented to solve the problem of features as out put in prompt\n",
        "    # lora_config = LoraConfig(\n",
        "    #     r=8,\n",
        "    #     lora_alpha=16,\n",
        "    #     target_modules=[\"self_attn.q_proj\",\n",
        "    #     \"self_attn.k_proj\",\n",
        "    #     \"self_attn.v_proj\",\n",
        "    #     \"self_attn.o_proj\",\n",
        "    #     \"mlp.gate_proj\",\n",
        "    #     \"mlp.down_proj\",\n",
        "    #     \"mlp.up_proj\"  ],\n",
        "    #     lora_dropout=0.05,\n",
        "    #     bias=\"none\",\n",
        "    #     # save_embedding_layers=True,\n",
        "    #     task_type=TaskType.CAUSAL_LM\n",
        "    # )\n",
        "\n",
        "    # Configure LoRA Updated on 17th Nov 2024\n",
        "    lora_config = LoraConfig(\n",
        "        r=8, # updaeted from 8 to 16\n",
        "        lora_alpha=16, # updaeted from 16 to 32\n",
        "        target_modules=[\"self_attn.q_proj\",\n",
        "        \"self_attn.k_proj\",\n",
        "        \"self_attn.v_proj\",\n",
        "        \"self_attn.o_proj\",\n",
        "        \"mlp.gate_proj\",\n",
        "        \"mlp.down_proj\",\n",
        "        \"mlp.up_proj\"  ],\n",
        "        lora_dropout=0.1,  # updaeted from .05 to .1\n",
        "        bias=\"none\",\n",
        "        # save_embedding_layers=True,\n",
        "        task_type=TaskType.CAUSAL_LM\n",
        "    )\n",
        "\n",
        "\n",
        "\n",
        "    lora_config.save_embedding_layers = True\n",
        "    model = get_peft_model(model, lora_config)\n",
        "    num_samples_per_epoch = 500\n",
        "    sampler = RandomSampler(dataset_dict['train'],\n",
        "                            replacement=True,\n",
        "                            num_samples=num_samples_per_epoch\n",
        "                            ) # Use replacement for smaller datasets\n",
        "\n",
        "\n",
        "    # Add the OOMSafeCallback to the trainer's callbacks\n",
        "    callbacks = [\n",
        "        EarlyStoppingCallback(early_stopping_patience=3),\n",
        "        OOMSafeCallback(save_dir=output_dir)\n",
        "    ]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    # Training arguments optimised\n",
        "    training_args = TrainingArguments(\n",
        "        output_dir=config.output_dir,\n",
        "\n",
        "      #@num_train_epochs=config.num_train_epochs,\n",
        "        num_train_epochs=1/90,\n",
        "        per_device_train_batch_size=config.per_device_train_batch_size,\n",
        "        gradient_accumulation_steps=config.gradient_accumulation_steps,\n",
        "        learning_rate=config.learning_rate,\n",
        "        #fp16=config.fp16,\n",
        "        bf16=True,\n",
        "        warmup_ratio=config.warmup_ratio,\n",
        "        lr_scheduler_type=config.lr_scheduler_type,\n",
        "        save_total_limit=config.save_total_limit,\n",
        "        logging_steps=config.logging_steps,\n",
        "        # max_grad_norm=config.max_grad_norm,\n",
        "        report_to=config.report_to,\n",
        "        save_steps=config.save_steps,\n",
        "        evaluation_strategy=config.evaluation_strategy,\n",
        "        eval_steps=config.eval_steps,\n",
        "        # logging_dir=config.logging_dir,\n",
        "        # logging_strategy=config.logging_strategy,\n",
        "        disable_tqdm=False,\n",
        "        eval_strategy=config.eval_strategy,\n",
        "        load_best_model_at_end=config.load_best_model_at_end,\n",
        "        save_strategy=config.save_strategy,\n",
        "        push_to_hub=config.push_to_hub,\n",
        "        # run_name=config.run_name,\n",
        "        # logging_first_step=config.logging_first_step,\n",
        "        remove_unused_columns=config.remove_unused_columns,\n",
        "        optim=\"adamw_torch\",#config.optim,\n",
        "        gradient_checkpointing=True,#config.gradient_checkpointing,\n",
        "        gradient_checkpointing_kwargs={\"use_reentrant\": False},\n",
        "        resume_from_checkpoint=config.resume_from_checkpoint,\n",
        "\n",
        "         dataloader_pin_memory=False,  # Reduce CPU memory usage\n",
        "        # torch_compile=True,  # Enable torch.compile\n",
        "        # deepspeed=config.deepspeed,\n",
        "        # seed=config.seed,\n",
        "\n",
        "    )\n",
        "    # Create DataLoader with the sampler\n",
        "    train_dataloader = DataLoader(\n",
        "        dataset_dict['train'],\n",
        "        batch_size=training_args.per_device_train_batch_size,\n",
        "        sampler=sampler,\n",
        "        collate_fn=MultimodalDataCollator(tokenizer),\n",
        "        num_workers=1,  # Consider adjusting for optimal performance\n",
        "    )\n",
        "\n",
        "\n",
        "    # Trainer setup\n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        train_dataset=dataset_dict['train'],\n",
        "        eval_dataset=dataset_dict['validation'],\n",
        "        processing_class=tokenizer,\n",
        "        compute_metrics=compute_metrics,\n",
        "        data_collator=MultimodalDataCollator(tokenizer),\n",
        "        callbacks=[EarlyStoppingCallback(early_stopping_patience=3)]\n",
        "    )\n",
        "\n",
        "    # # Check for existing checkpoints and load if available\n",
        "    # if os.path.exists(os.path.join(output_dir, PREFIX_CHECKPOINT_DIR)):\n",
        "    #     latest_checkpoint = max(\n",
        "    #         [os.path.join(output_dir, d) for d in os.listdir(output_dir) if d.startswith(PREFIX_CHECKPOINT_DIR)],\n",
        "    #         key=os.path.getmtime,\n",
        "    #     )\n",
        "    #     print(f\"Loading model from checkpoint: {latest_checkpoint}\")\n",
        "    #     model = Phi3WithProjector.from_pretrained(latest_checkpoint)  # Load your custom model\n",
        "    # else:\n",
        "\n",
        "    #     train_result = trainer.train()\n",
        "\n",
        "\n",
        "\n",
        "    # # Save final model and metrics\n",
        "    # trainer.save_model(output_dir)\n",
        "    # trainer.save_state()\n",
        "\n",
        "    # # Log and save metrics\n",
        "    # metrics = train_result.metrics\n",
        "    # trainer.log_metrics(\"train\", metrics)\n",
        "    # trainer.save_metrics(\"train\", metrics)\n",
        "\n",
        "    # # Run final evaluation\n",
        "    # eval_metrics = trainer.evaluate()\n",
        "    # trainer.log_metrics(\"eval\", eval_metrics)\n",
        "    # trainer.save_metrics(\"eval\", eval_metrics)\n",
        "\n",
        "    #     # Free up memory\n",
        "\n",
        "    # return model, trainer\n",
        "\n",
        "\n",
        "    train_result = None\n",
        "    # Wrap the training in a try-except block\n",
        "    try:\n",
        "        if os.path.exists(os.path.join(output_dir, PREFIX_CHECKPOINT_DIR)):\n",
        "            latest_checkpoint = max(\n",
        "                [os.path.join(output_dir, d) for d in os.listdir(output_dir)\n",
        "                 if d.startswith(PREFIX_CHECKPOINT_DIR)],\n",
        "                key=os.path.getmtime,\n",
        "            )\n",
        "            print(f\"Loading model from checkpoint: {latest_checkpoint}\")\n",
        "            model = Phi3WithProjector.from_pretrained(latest_checkpoint)\n",
        "        else:\n",
        "            train_result = trainer.train()\n",
        "\n",
        "    except RuntimeError as e:\n",
        "        if \"out of memory\" in str(e):\n",
        "            print(\"OOM error encountered. Attempting to save current state...\")\n",
        "\n",
        "            # Clear memory\n",
        "            if torch.cuda.is_available():\n",
        "                torch.cuda.empty_cache()\n",
        "            gc.collect()\n",
        "\n",
        "            # Save emergency checkpoint\n",
        "            emergency_dir = os.path.join(output_dir, \"emergency_checkpoint\")\n",
        "            trainer.save_model(emergency_dir)\n",
        "            print(f\"Emergency checkpoint saved to {emergency_dir}\")\n",
        "\n",
        "            # Optionally, you can exit here or continue with reduced batch size\n",
        "            raise e\n",
        "\n",
        "    # Save final model and metrics\n",
        "    try:\n",
        "        trainer.save_model(output_dir)\n",
        "        trainer.save_state()\n",
        "\n",
        "        if  train_result is not None:\n",
        "            metrics = train_result.metrics\n",
        "            trainer.log_metrics(\"train\", metrics)\n",
        "            trainer.save_metrics(\"train\", metrics)\n",
        "\n",
        "            eval_metrics = trainer.evaluate()\n",
        "            trainer.log_metrics(\"eval\", eval_metrics)\n",
        "            trainer.save_metrics(\"eval\", eval_metrics)\n",
        "    except Exception as e:\n",
        "        print(f\"Error during final save: {str(e)}\")\n",
        "        # Attempt one final emergency save\n",
        "        emergency_final_dir = os.path.join(output_dir, \"final_emergency_save\")\n",
        "        model.save_pretrained(emergency_final_dir, save_in_8bit=True)\n",
        "\n",
        "    return model, trainer\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 145,
          "referenced_widgets": [
            "6d7aaa66181c47a3a255e918f1a351f9",
            "35261735059b46cb9e665b3f57ed1638",
            "d33360c826ca410687d10fe9518df695",
            "9e27d5f3f7074e86ba6ced840ebed2e2",
            "1b410077c5f840e7929918c32ab31918",
            "7f78c19d9fd841b6ac3caac2b8c36016",
            "974ba1bb9bfa4e6da7228c81db6b7041",
            "109d967faa57475c852b50ce0bd48fe9",
            "257f06a762a543c9ad5d7fc4f4f0c1db",
            "2e7aa4b0ee9c4fe2a188cdee5907bdb4",
            "e516c005ea924537a61b04fa1dcd7da5",
            "27acfcb2e9da42f1927d52ba0cc09ed2",
            "8223db5e9dd04e38b8beb2aea02e6ddd",
            "8fdd458d1a7242aab26f42a621aedc58",
            "87adba404ada475cb0c3e558ffae7a8d",
            "ed0e7c7375244bbb8f587ede64107a9a",
            "4065d5015ffd404789d578c95080346d",
            "12b08aa13ecb493581bd67f03f52cdcb",
            "dc93a8ab89284a63836ca637d5509656",
            "f902895480d44332895686d0c654ba2b",
            "101583b995d94c6c89e4e6948f14f63b",
            "bcbbdea3604f4c7b8e158d832e477762",
            "80e63d9e28c4486383e45322fd57d967",
            "93e98e54b69f467d8d4a134e0832a546",
            "25411fb908c6492c87d7fcf08c9425f6",
            "a79067d66eba458f941c6d5967d8c9d8",
            "9c3a08b3ea3b4ee4a36cd7c5d7b09128",
            "18ae8b4391de49b2a6ab988b913d11f2",
            "ce02ac39215d4db9bf10759dd84aeb7c",
            "69bcfd7076fa4a849ebc4af42f62af7d",
            "59288ec589a642be90d624c205b37b13",
            "fbc60ee4041f4d0a82b5cd423a104512",
            "c86f05b3af0f48a9bc44a83c5c18197c",
            "380cb338697b44f3b734d4f5ffd986dd",
            "dc669d09a8ab40a1994e5a56dd6f3ba7",
            "569b1719b9af4daab79436de0e8ed30d",
            "e5463e0057cf4d6aa55c9b176dda660d",
            "eecb00c38bab4ecb84704f87afae668a",
            "75c699d676f54d328f69e3c3cf625aa3",
            "0fe44843bb8144ff83d38f43e4956b3f",
            "1bae64225ade46688df8510b1a813c92",
            "ea8701c80ab64db2b22ac28bf6c20212",
            "c63ed592ec98462baafe69fcb3ffc577",
            "0d491d3d6e554ddf800dc29e26c8b0eb"
          ]
        },
        "id": "ME81wbO-8knI",
        "outputId": "ebc58346-b308-4b28-d1fe-b8a0caa05e15"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/157712 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6d7aaa66181c47a3a255e918f1a351f9"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Saving the dataset (0/3 shards):   0%|          | 0/289128 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "27acfcb2e9da42f1927d52ba0cc09ed2"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Saving the dataset (0/1 shards):   0%|          | 0/36142 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "80e63d9e28c4486383e45322fd57d967"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Saving the dataset (0/1 shards):   0%|          | 0/36141 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "380cb338697b44f3b734d4f5ffd986dd"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "if __name__ == \"__main__\":\n",
        "    image_embeddings_path = \"/content/drive/MyDrive/LLM/Copy of clip_embeddings_instruct150k_15Oct_2024.npz\"\n",
        "    dataset_path = \"/content/drive/MyDrive/Cap_project/llava_instruct_150k.json\"\n",
        "    output_dir = \"/content/drive/MyDrive/LLM_training_19November\"\n",
        "    whisper_model_name = \"openai/whisper-small\"\n",
        "    cache_dir = \"/content/\"\n",
        "    dataset_dict, tokenizer = prepare_dataset(image_embeddings_path, dataset_path, cache_dir)\n",
        "    # if os.path.exists(cache_dir):\n",
        "    #     dataset_dict = DatasetDict.load_from_disk(cache_dir)\n",
        "    # else:\n",
        "    #   dataset_dict, tokenizer = prepare_dataset(image_embeddings_path, dataset_path, cache_dir)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "92b375a579344c9ebd1a80d284271593",
            "db1950791ccb409d95711555b96ef4c9",
            "298a568c21dc4255b13d48b83497bfd1",
            "d1b42be7b37c47a38c4e964e16b7792d",
            "0473a81c8318486ba60fb09416158070",
            "bf3906f632754f3f8c2b0cffe09f6879",
            "506754b3ca194e929036d56b94f11c00",
            "b39e9fc259284395933afc3596b1742f",
            "f9b5626218d94eafbd67ab2a542f6654",
            "d4f58827f6654a65b34b33deb78a9c5e",
            "1b463739325a4c1b8357b33c94ed4686",
            "d454bddf8abf477498f4a9959723bc54",
            "85f9c7c79d2d478aa290870dc0772306",
            "0f3778db19224d0d83e3ba1d315760d4",
            "185284391bbb4415a572d75cde220789",
            "dcf2040b003f4b5f9890432af29e2571",
            "06a085b848ac411a971f2b0f5e4d2bd4",
            "5b5dcef51bb1444da05a393718a91e0c",
            "bbfe0737db6c42dca56c45807dbb72ce",
            "abf0ce9be78040cf849ddada9acc6c36",
            "9f08ca5f4a334f7bb63fa5da4e1648a5",
            "1234608895db412abf4340dd4708d30a",
            "6677bda76c3f4446bb4d3be8ea18a028",
            "4f4653cf13cb4b0bb6d4ea2e1d102a8e",
            "a7d8c2e126754a7f9ef0bee2e5b8fce6",
            "ddd36600c7894886992c36a03db0e2fa",
            "072dc8800efb4211857dba238342b72c",
            "6be18d89ea4149b982a12ecbc8fb3d5d",
            "054bda0db6914fb28fc260f67da4a647",
            "b5b5a8697c7f4745b2ce5cdecaba6eb1",
            "cbce0649a05e4111a8d7d14b6b5babf7",
            "5f23d9f673804fd4a8d6f817786edd78",
            "8bbcbe35315341e3a3b06a520ba4d310",
            "02af28b834d04222a53730e148744830",
            "43a9388b06f3472eb3f86643f1caf242",
            "f18bde59b7684a0ba4528f7adcd11f5f",
            "4a91dc8bdb85419c8cd0fc3bb6fcd0bd",
            "861fb7e0707c4c6fa3efd8320611fe36",
            "d7b509ba810a46229b145e838dccda7e",
            "794ef5cb56964e7f9a6fb070397baae0",
            "047d593c11854733919685c03b0a3f13",
            "64445af2c7f3453885bf92628e68f5fe",
            "9bfe1302849f45bc8df53ce3d22ab95a",
            "c60017a43cca4521b673ab9f40ebd530",
            "b864e58b82044b1592c7c79a3aa694aa",
            "ee1f54c6145e46a082deeba8e0104305",
            "b4673dd7a901469994e483310509d593",
            "3de779a2189a4fa0abac212ecb8882c5",
            "e57dfca132634c8b962c2975434d5ad8",
            "451af528edef4907b63bad2a3e3d081f",
            "ef1e863d054a457690b8b6322ab369f4",
            "e7a00d5e83bb4fd08f1eea5296088121",
            "4b01f89280134b5187b2a5bba0c7c50b",
            "06d22b21b7ab43dca5fb71624eb1a3e7",
            "d7478d58fcbf4501bca807a02215e112",
            "da474d34a373411f812d58b8a55cd71f",
            "271d5224832e4b8482d2edd0bcc7745e",
            "f5bde914c781436bb8e555fb92836cd3",
            "785f755459ae4f2abddf0b9b2d6c30bc",
            "eb7b0ac5494745af89e3ef6835208757",
            "1d7764b1bdbd4a1dace248ec0d0a6b6e",
            "c4a809c2126a44e5a7c91bf4af841c3e",
            "619cd281a8064a3b8ddd250bc0ffa6ec",
            "92b65507dbc2471caac95719d19a37a7",
            "ef8232e1e6e9408b9d11de087ff4c052",
            "0c7277954f8b4eb8b611f299515b75c8",
            "be61f4a5ab2e4e7192116cfd73787fac",
            "47838e803bda45f1ad93011806c84fae",
            "54cce308c82a4917a6c74c808b2bef91",
            "42ec8c1be3e24e63a9f7b6daff39ad66",
            "f39ac95894374f3da2ef259d2a536ed0",
            "af2a5096fd0d4d08a02025e7bd0c95de",
            "15c993dafa074a569dcea64196b4a75a",
            "2995dcb310ea4b359acd0d3903cf5ce9",
            "c3dbab463d3b433aa2dbbf99443f9325",
            "7f227a4e23e74dad9cf78f0e57d58064",
            "df56346994f343ee923b2f8c46111144"
          ]
        },
        "id": "KT035O4sa1Dz",
        "outputId": "83bf33b8-786a-4291-bcab-67b3cf493697"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "92b375a579344c9ebd1a80d284271593",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/3.48k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--microsoft--Phi-3-mini-128k-instruct/snapshots/a90b62ae09941edff87a90ced39ba5807e6b2ade/config.json\n",
            "Model config Phi3Config {\n",
            "  \"_name_or_path\": \"microsoft/Phi-3-mini-128k-instruct\",\n",
            "  \"architectures\": [\n",
            "    \"Phi3ForCausalLM\"\n",
            "  ],\n",
            "  \"attention_bias\": false,\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"auto_map\": {\n",
            "    \"AutoConfig\": \"microsoft/Phi-3-mini-128k-instruct--configuration_phi3.Phi3Config\",\n",
            "    \"AutoModelForCausalLM\": \"microsoft/Phi-3-mini-128k-instruct--modeling_phi3.Phi3ForCausalLM\"\n",
            "  },\n",
            "  \"bos_token_id\": 1,\n",
            "  \"embd_pdrop\": 0.0,\n",
            "  \"eos_token_id\": 32000,\n",
            "  \"hidden_act\": \"silu\",\n",
            "  \"hidden_size\": 3072,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 8192,\n",
            "  \"max_position_embeddings\": 131072,\n",
            "  \"model_type\": \"phi3\",\n",
            "  \"num_attention_heads\": 32,\n",
            "  \"num_hidden_layers\": 32,\n",
            "  \"num_key_value_heads\": 32,\n",
            "  \"original_max_position_embeddings\": 4096,\n",
            "  \"pad_token_id\": 32000,\n",
            "  \"resid_pdrop\": 0.0,\n",
            "  \"rms_norm_eps\": 1e-05,\n",
            "  \"rope_scaling\": {\n",
            "    \"long_factor\": [\n",
            "      1.0700000524520874,\n",
            "      1.1200000047683716,\n",
            "      1.149999976158142,\n",
            "      1.4199999570846558,\n",
            "      1.5699999332427979,\n",
            "      1.7999999523162842,\n",
            "      2.129999876022339,\n",
            "      2.129999876022339,\n",
            "      3.009999990463257,\n",
            "      5.910000324249268,\n",
            "      6.950000286102295,\n",
            "      9.070000648498535,\n",
            "      9.930000305175781,\n",
            "      10.710000038146973,\n",
            "      11.130000114440918,\n",
            "      14.609999656677246,\n",
            "      15.409998893737793,\n",
            "      19.809999465942383,\n",
            "      37.279998779296875,\n",
            "      38.279998779296875,\n",
            "      38.599998474121094,\n",
            "      40.12000274658203,\n",
            "      46.20000457763672,\n",
            "      50.940006256103516,\n",
            "      53.66000747680664,\n",
            "      54.9373893737793,\n",
            "      56.89738845825195,\n",
            "      57.28738784790039,\n",
            "      59.98738479614258,\n",
            "      60.86738586425781,\n",
            "      60.887386322021484,\n",
            "      61.71739196777344,\n",
            "      62.91739273071289,\n",
            "      62.957393646240234,\n",
            "      63.41739273071289,\n",
            "      63.8173942565918,\n",
            "      63.83739471435547,\n",
            "      63.897396087646484,\n",
            "      63.93739700317383,\n",
            "      64.06739807128906,\n",
            "      64.11434936523438,\n",
            "      64.12435150146484,\n",
            "      64.15435028076172,\n",
            "      64.19435119628906,\n",
            "      64.24435424804688,\n",
            "      64.57435607910156,\n",
            "      64.69000244140625,\n",
            "      64.76000213623047\n",
            "    ],\n",
            "    \"short_factor\": [\n",
            "      1.1,\n",
            "      1.1,\n",
            "      1.1,\n",
            "      1.3000000000000003,\n",
            "      1.3500000000000003,\n",
            "      1.3500000000000003,\n",
            "      1.4000000000000004,\n",
            "      1.5500000000000005,\n",
            "      2.000000000000001,\n",
            "      2.000000000000001,\n",
            "      2.000000000000001,\n",
            "      2.000000000000001,\n",
            "      2.000000000000001,\n",
            "      2.000000000000001,\n",
            "      2.000000000000001,\n",
            "      2.000000000000001,\n",
            "      2.000000000000001,\n",
            "      2.000000000000001,\n",
            "      2.000000000000001,\n",
            "      2.000000000000001,\n",
            "      2.000000000000001,\n",
            "      2.000000000000001,\n",
            "      2.000000000000001,\n",
            "      2.000000000000001,\n",
            "      2.000000000000001,\n",
            "      2.0500000000000007,\n",
            "      2.0500000000000007,\n",
            "      2.0500000000000007,\n",
            "      2.0500000000000007,\n",
            "      2.0500000000000007,\n",
            "      2.0500000000000007,\n",
            "      2.1000000000000005,\n",
            "      2.1000000000000005,\n",
            "      2.1500000000000004,\n",
            "      2.25,\n",
            "      2.25,\n",
            "      2.25,\n",
            "      2.25,\n",
            "      2.25,\n",
            "      2.3999999999999995,\n",
            "      2.4499999999999993,\n",
            "      2.499999999999999,\n",
            "      2.6999999999999984,\n",
            "      2.6999999999999984,\n",
            "      2.7499999999999982,\n",
            "      2.799999999999998,\n",
            "      2.8999999999999977,\n",
            "      3.049999999999997\n",
            "    ],\n",
            "    \"type\": \"longrope\"\n",
            "  },\n",
            "  \"rope_theta\": 10000.0,\n",
            "  \"sliding_window\": 262144,\n",
            "  \"tie_word_embeddings\": false,\n",
            "  \"torch_dtype\": \"float16\",\n",
            "  \"transformers_version\": \"4.46.2\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 32064\n",
            "}\n",
            "\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d454bddf8abf477498f4a9959723bc54",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors.index.json:   0%|          | 0.00/16.3k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--microsoft--Phi-3-mini-128k-instruct/snapshots/a90b62ae09941edff87a90ced39ba5807e6b2ade/model.safetensors.index.json\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6677bda76c3f4446bb4d3be8ea18a028",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "02af28b834d04222a53730e148744830",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model-00001-of-00002.safetensors:   0%|          | 0.00/4.97G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b864e58b82044b1592c7c79a3aa694aa",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model-00002-of-00002.safetensors:   0%|          | 0.00/2.67G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Instantiating Phi3ForCausalLM model under default dtype torch.float16.\n",
            "Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 1,\n",
            "  \"eos_token_id\": 32000,\n",
            "  \"pad_token_id\": 32000\n",
            "}\n",
            "\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "da474d34a373411f812d58b8a55cd71f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "All model checkpoint weights were used when initializing Phi3ForCausalLM.\n",
            "\n",
            "All the weights of Phi3ForCausalLM were initialized from the model checkpoint at microsoft/Phi-3-mini-128k-instruct.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use Phi3ForCausalLM for predictions without further training.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "be61f4a5ab2e4e7192116cfd73787fac",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/181 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "loading configuration file generation_config.json from cache at /root/.cache/huggingface/hub/models--microsoft--Phi-3-mini-128k-instruct/snapshots/a90b62ae09941edff87a90ced39ba5807e6b2ade/generation_config.json\n",
            "Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 1,\n",
            "  \"eos_token_id\": [\n",
            "    32000,\n",
            "    32001,\n",
            "    32007\n",
            "  ],\n",
            "  \"pad_token_id\": 32000\n",
            "}\n",
            "\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n",
            "PyTorch: setting up devices\n",
            "Using auto half precision backend\n",
            "***** Running training *****\n",
            "  Num examples = 289,128\n",
            "  Num Epochs = 1\n",
            "  Instantaneous batch size per device = 2\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
            "  Gradient Accumulation steps = 8\n",
            "  Total optimization steps = 201\n",
            "  Number of trainable parameters = 4,456,448\n",
            "You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='143' max='201' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [143/201 2:21:27 < 58:11, 0.02 it/s, Epoch 0.01/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='202' max='201' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [201/201 3:22:30, Epoch 0.01/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table><p>\n",
              "    <div>\n",
              "      \n",
              "      <progress value='4' max='4518' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [   4/4518 00:36 < 15:11:38, 0.08 it/s]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Saving model checkpoint to /content/drive/MyDrive/LLM_training_output_1Nov2024_V3/checkpoint-201\n",
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--microsoft--Phi-3-mini-128k-instruct/snapshots/a90b62ae09941edff87a90ced39ba5807e6b2ade/config.json\n",
            "Model config Phi3Config {\n",
            "  \"_name_or_path\": \"Phi-3-mini-128k-instruct\",\n",
            "  \"architectures\": [\n",
            "    \"Phi3ForCausalLM\"\n",
            "  ],\n",
            "  \"attention_bias\": false,\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"auto_map\": {\n",
            "    \"AutoConfig\": \"microsoft/Phi-3-mini-128k-instruct--configuration_phi3.Phi3Config\",\n",
            "    \"AutoModelForCausalLM\": \"microsoft/Phi-3-mini-128k-instruct--modeling_phi3.Phi3ForCausalLM\"\n",
            "  },\n",
            "  \"bos_token_id\": 1,\n",
            "  \"embd_pdrop\": 0.0,\n",
            "  \"eos_token_id\": 32000,\n",
            "  \"hidden_act\": \"silu\",\n",
            "  \"hidden_size\": 3072,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 8192,\n",
            "  \"max_position_embeddings\": 131072,\n",
            "  \"model_type\": \"phi3\",\n",
            "  \"num_attention_heads\": 32,\n",
            "  \"num_hidden_layers\": 32,\n",
            "  \"num_key_value_heads\": 32,\n",
            "  \"original_max_position_embeddings\": 4096,\n",
            "  \"pad_token_id\": 32000,\n",
            "  \"resid_pdrop\": 0.0,\n",
            "  \"rms_norm_eps\": 1e-05,\n",
            "  \"rope_scaling\": {\n",
            "    \"long_factor\": [\n",
            "      1.0700000524520874,\n",
            "      1.1200000047683716,\n",
            "      1.149999976158142,\n",
            "      1.4199999570846558,\n",
            "      1.5699999332427979,\n",
            "      1.7999999523162842,\n",
            "      2.129999876022339,\n",
            "      2.129999876022339,\n",
            "      3.009999990463257,\n",
            "      5.910000324249268,\n",
            "      6.950000286102295,\n",
            "      9.070000648498535,\n",
            "      9.930000305175781,\n",
            "      10.710000038146973,\n",
            "      11.130000114440918,\n",
            "      14.609999656677246,\n",
            "      15.409998893737793,\n",
            "      19.809999465942383,\n",
            "      37.279998779296875,\n",
            "      38.279998779296875,\n",
            "      38.599998474121094,\n",
            "      40.12000274658203,\n",
            "      46.20000457763672,\n",
            "      50.940006256103516,\n",
            "      53.66000747680664,\n",
            "      54.9373893737793,\n",
            "      56.89738845825195,\n",
            "      57.28738784790039,\n",
            "      59.98738479614258,\n",
            "      60.86738586425781,\n",
            "      60.887386322021484,\n",
            "      61.71739196777344,\n",
            "      62.91739273071289,\n",
            "      62.957393646240234,\n",
            "      63.41739273071289,\n",
            "      63.8173942565918,\n",
            "      63.83739471435547,\n",
            "      63.897396087646484,\n",
            "      63.93739700317383,\n",
            "      64.06739807128906,\n",
            "      64.11434936523438,\n",
            "      64.12435150146484,\n",
            "      64.15435028076172,\n",
            "      64.19435119628906,\n",
            "      64.24435424804688,\n",
            "      64.57435607910156,\n",
            "      64.69000244140625,\n",
            "      64.76000213623047\n",
            "    ],\n",
            "    \"short_factor\": [\n",
            "      1.1,\n",
            "      1.1,\n",
            "      1.1,\n",
            "      1.3000000000000003,\n",
            "      1.3500000000000003,\n",
            "      1.3500000000000003,\n",
            "      1.4000000000000004,\n",
            "      1.5500000000000005,\n",
            "      2.000000000000001,\n",
            "      2.000000000000001,\n",
            "      2.000000000000001,\n",
            "      2.000000000000001,\n",
            "      2.000000000000001,\n",
            "      2.000000000000001,\n",
            "      2.000000000000001,\n",
            "      2.000000000000001,\n",
            "      2.000000000000001,\n",
            "      2.000000000000001,\n",
            "      2.000000000000001,\n",
            "      2.000000000000001,\n",
            "      2.000000000000001,\n",
            "      2.000000000000001,\n",
            "      2.000000000000001,\n",
            "      2.000000000000001,\n",
            "      2.000000000000001,\n",
            "      2.0500000000000007,\n",
            "      2.0500000000000007,\n",
            "      2.0500000000000007,\n",
            "      2.0500000000000007,\n",
            "      2.0500000000000007,\n",
            "      2.0500000000000007,\n",
            "      2.1000000000000005,\n",
            "      2.1000000000000005,\n",
            "      2.1500000000000004,\n",
            "      2.25,\n",
            "      2.25,\n",
            "      2.25,\n",
            "      2.25,\n",
            "      2.25,\n",
            "      2.3999999999999995,\n",
            "      2.4499999999999993,\n",
            "      2.499999999999999,\n",
            "      2.6999999999999984,\n",
            "      2.6999999999999984,\n",
            "      2.7499999999999982,\n",
            "      2.799999999999998,\n",
            "      2.8999999999999977,\n",
            "      3.049999999999997\n",
            "    ],\n",
            "    \"type\": \"longrope\"\n",
            "  },\n",
            "  \"rope_theta\": 10000.0,\n",
            "  \"sliding_window\": 262144,\n",
            "  \"tie_word_embeddings\": false,\n",
            "  \"torch_dtype\": \"bfloat16\",\n",
            "  \"transformers_version\": \"4.46.2\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 32064\n",
            "}\n",
            "\n",
            "tokenizer config file saved in /content/drive/MyDrive/LLM_training_output_1Nov2024_V3/checkpoint-201/tokenizer_config.json\n",
            "Special tokens file saved in /content/drive/MyDrive/LLM_training_output_1Nov2024_V3/checkpoint-201/special_tokens_map.json\n",
            "Deleting older checkpoint [/content/drive/MyDrive/LLM_training_output_1Nov2024_V3/checkpoint-201] due to args.save_total_limit\n",
            "\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 36141\n",
            "  Batch size = 8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "OOM error encountered. Attempting to save current state...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Saving model checkpoint to /content/drive/MyDrive/LLM_training_19Nov_test/emergency_checkpoint\n",
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--microsoft--Phi-3-mini-128k-instruct/snapshots/a90b62ae09941edff87a90ced39ba5807e6b2ade/config.json\n",
            "Model config Phi3Config {\n",
            "  \"_name_or_path\": \"Phi-3-mini-128k-instruct\",\n",
            "  \"architectures\": [\n",
            "    \"Phi3ForCausalLM\"\n",
            "  ],\n",
            "  \"attention_bias\": false,\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"auto_map\": {\n",
            "    \"AutoConfig\": \"microsoft/Phi-3-mini-128k-instruct--configuration_phi3.Phi3Config\",\n",
            "    \"AutoModelForCausalLM\": \"microsoft/Phi-3-mini-128k-instruct--modeling_phi3.Phi3ForCausalLM\"\n",
            "  },\n",
            "  \"bos_token_id\": 1,\n",
            "  \"embd_pdrop\": 0.0,\n",
            "  \"eos_token_id\": 32000,\n",
            "  \"hidden_act\": \"silu\",\n",
            "  \"hidden_size\": 3072,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 8192,\n",
            "  \"max_position_embeddings\": 131072,\n",
            "  \"model_type\": \"phi3\",\n",
            "  \"num_attention_heads\": 32,\n",
            "  \"num_hidden_layers\": 32,\n",
            "  \"num_key_value_heads\": 32,\n",
            "  \"original_max_position_embeddings\": 4096,\n",
            "  \"pad_token_id\": 32000,\n",
            "  \"resid_pdrop\": 0.0,\n",
            "  \"rms_norm_eps\": 1e-05,\n",
            "  \"rope_scaling\": {\n",
            "    \"long_factor\": [\n",
            "      1.0700000524520874,\n",
            "      1.1200000047683716,\n",
            "      1.149999976158142,\n",
            "      1.4199999570846558,\n",
            "      1.5699999332427979,\n",
            "      1.7999999523162842,\n",
            "      2.129999876022339,\n",
            "      2.129999876022339,\n",
            "      3.009999990463257,\n",
            "      5.910000324249268,\n",
            "      6.950000286102295,\n",
            "      9.070000648498535,\n",
            "      9.930000305175781,\n",
            "      10.710000038146973,\n",
            "      11.130000114440918,\n",
            "      14.609999656677246,\n",
            "      15.409998893737793,\n",
            "      19.809999465942383,\n",
            "      37.279998779296875,\n",
            "      38.279998779296875,\n",
            "      38.599998474121094,\n",
            "      40.12000274658203,\n",
            "      46.20000457763672,\n",
            "      50.940006256103516,\n",
            "      53.66000747680664,\n",
            "      54.9373893737793,\n",
            "      56.89738845825195,\n",
            "      57.28738784790039,\n",
            "      59.98738479614258,\n",
            "      60.86738586425781,\n",
            "      60.887386322021484,\n",
            "      61.71739196777344,\n",
            "      62.91739273071289,\n",
            "      62.957393646240234,\n",
            "      63.41739273071289,\n",
            "      63.8173942565918,\n",
            "      63.83739471435547,\n",
            "      63.897396087646484,\n",
            "      63.93739700317383,\n",
            "      64.06739807128906,\n",
            "      64.11434936523438,\n",
            "      64.12435150146484,\n",
            "      64.15435028076172,\n",
            "      64.19435119628906,\n",
            "      64.24435424804688,\n",
            "      64.57435607910156,\n",
            "      64.69000244140625,\n",
            "      64.76000213623047\n",
            "    ],\n",
            "    \"short_factor\": [\n",
            "      1.1,\n",
            "      1.1,\n",
            "      1.1,\n",
            "      1.3000000000000003,\n",
            "      1.3500000000000003,\n",
            "      1.3500000000000003,\n",
            "      1.4000000000000004,\n",
            "      1.5500000000000005,\n",
            "      2.000000000000001,\n",
            "      2.000000000000001,\n",
            "      2.000000000000001,\n",
            "      2.000000000000001,\n",
            "      2.000000000000001,\n",
            "      2.000000000000001,\n",
            "      2.000000000000001,\n",
            "      2.000000000000001,\n",
            "      2.000000000000001,\n",
            "      2.000000000000001,\n",
            "      2.000000000000001,\n",
            "      2.000000000000001,\n",
            "      2.000000000000001,\n",
            "      2.000000000000001,\n",
            "      2.000000000000001,\n",
            "      2.000000000000001,\n",
            "      2.000000000000001,\n",
            "      2.0500000000000007,\n",
            "      2.0500000000000007,\n",
            "      2.0500000000000007,\n",
            "      2.0500000000000007,\n",
            "      2.0500000000000007,\n",
            "      2.0500000000000007,\n",
            "      2.1000000000000005,\n",
            "      2.1000000000000005,\n",
            "      2.1500000000000004,\n",
            "      2.25,\n",
            "      2.25,\n",
            "      2.25,\n",
            "      2.25,\n",
            "      2.25,\n",
            "      2.3999999999999995,\n",
            "      2.4499999999999993,\n",
            "      2.499999999999999,\n",
            "      2.6999999999999984,\n",
            "      2.6999999999999984,\n",
            "      2.7499999999999982,\n",
            "      2.799999999999998,\n",
            "      2.8999999999999977,\n",
            "      3.049999999999997\n",
            "    ],\n",
            "    \"type\": \"longrope\"\n",
            "  },\n",
            "  \"rope_theta\": 10000.0,\n",
            "  \"sliding_window\": 262144,\n",
            "  \"tie_word_embeddings\": false,\n",
            "  \"torch_dtype\": \"bfloat16\",\n",
            "  \"transformers_version\": \"4.46.2\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 32064\n",
            "}\n",
            "\n",
            "tokenizer config file saved in /content/drive/MyDrive/LLM_training_19Nov_test/emergency_checkpoint/tokenizer_config.json\n",
            "Special tokens file saved in /content/drive/MyDrive/LLM_training_19Nov_test/emergency_checkpoint/special_tokens_map.json\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Emergency checkpoint saved to /content/drive/MyDrive/LLM_training_19Nov_test/emergency_checkpoint\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "OutOfMemoryError",
          "evalue": "CUDA out of memory. Tried to allocate 518.00 MiB. GPU 0 has a total capacity of 14.75 GiB of which 163.06 MiB is free. Process 45524 has 14.59 GiB memory in use. Of the allocated memory 13.61 GiB is allocated by PyTorch, and 862.66 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-26-811908d6d30d>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0moutput_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"/content/drive/MyDrive/LLM_training_19Nov_test\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodal\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-25-a8c185b0ce61>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(dataset_dict, tokenizer, output_dir)\u001b[0m\n\u001b[1;32m    357\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    358\u001b[0m             \u001b[0;31m# Optionally, you can exit here or continue with reduced batch size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 359\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    360\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    361\u001b[0m     \u001b[0;31m# Save final model and metrics\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-25-a8c185b0ce61>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(dataset_dict, tokenizer, output_dir)\u001b[0m\n\u001b[1;32m    340\u001b[0m             \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPhi3WithProjector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlatest_checkpoint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    341\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 342\u001b[0;31m             \u001b[0mtrain_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    343\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mRuntimeError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2121\u001b[0m                 \u001b[0mhf_hub_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable_progress_bars\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2122\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2123\u001b[0;31m             return inner_training_loop(\n\u001b[0m\u001b[1;32m   2124\u001b[0m                 \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2125\u001b[0m                 \u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m_inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2571\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2572\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrol\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallback_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_epoch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2573\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_log_save_evaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtr_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_norm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrial\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_keys_for_eval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2574\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2575\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mDebugOption\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTPU_METRICS_DEBUG\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m_maybe_log_save_evaluate\u001b[0;34m(self, tr_loss, grad_norm, model, trial, epoch, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   3002\u001b[0m         \u001b[0mmetrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3003\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_evaluate\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3004\u001b[0;31m             \u001b[0mmetrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_evaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_keys_for_eval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3005\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3006\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_save\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m_evaluate\u001b[0;34m(self, trial, ignore_keys_for_eval, skip_scheduler)\u001b[0m\n\u001b[1;32m   2956\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2957\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_evaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrial\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_keys_for_eval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mskip_scheduler\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2958\u001b[0;31m         \u001b[0mmetrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mignore_keys\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mignore_keys_for_eval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2959\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_report_to_hp_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglobal_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2960\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, eval_dataset, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[1;32m   3973\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3974\u001b[0m         \u001b[0meval_loop\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprediction_loop\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_legacy_prediction_loop\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluation_loop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3975\u001b[0;31m         output = eval_loop(\n\u001b[0m\u001b[1;32m   3976\u001b[0m             \u001b[0meval_dataloader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3977\u001b[0m             \u001b[0mdescription\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Evaluation\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mevaluation_loop\u001b[0;34m(self, dataloader, description, prediction_loss_only, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[1;32m   4167\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4168\u001b[0m             \u001b[0;31m# Prediction step\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4169\u001b[0;31m             \u001b[0mlosses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprediction_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprediction_loss_only\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_keys\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mignore_keys\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4170\u001b[0m             \u001b[0mmain_input_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"main_input_name\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"input_ids\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4171\u001b[0m             inputs_decode = (\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mprediction_step\u001b[0;34m(self, model, inputs, prediction_loss_only, ignore_keys)\u001b[0m\n\u001b[1;32m   4383\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mhas_labels\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mloss_without_labels\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4384\u001b[0m                     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_loss_context_manager\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4385\u001b[0;31m                         \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4386\u001b[0m                     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4387\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mcompute_loss\u001b[0;34m(self, model, inputs, return_outputs, num_items_in_batch)\u001b[0m\n\u001b[1;32m   3631\u001b[0m                 \u001b[0mloss_kwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"num_items_in_batch\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnum_items_in_batch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3632\u001b[0m             \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mloss_kwargs\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3633\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3634\u001b[0m         \u001b[0;31m# Save past state if it exists\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3635\u001b[0m         \u001b[0;31m# TODO: this needs to be fixed and made cleaner later.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/accelerate/utils/operations.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    821\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    822\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 823\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mmodel_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    824\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    825\u001b[0m     \u001b[0;31m# To act like a decorator so that it can be popped when doing `extract_model_from_parallel`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/accelerate/utils/operations.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    809\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    810\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 811\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mconvert_to_fp32\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    812\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    813\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getstate__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/amp/autocast_mode.py\u001b[0m in \u001b[0;36mdecorate_autocast\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_autocast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mautocast_instance\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m     \u001b[0mdecorate_autocast\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__script_unsupported\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"@autocast() decorator is not supported in script mode\"\u001b[0m  \u001b[0;31m# type: ignore[attr-defined]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/peft/peft_model.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict, task_ids, **kwargs)\u001b[0m\n\u001b[1;32m   1642\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_enable_peft_forward_hooks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1643\u001b[0m                 \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspecial_peft_forward_args\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1644\u001b[0;31m                 return self.base_model(\n\u001b[0m\u001b[1;32m   1645\u001b[0m                     \u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1646\u001b[0m                     \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/peft/tuners/tuners_utils.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    195\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 197\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_pre_injection_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mPeftConfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madapter_name\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/drive/MyDrive/Cap_project/modal.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, inputs_embeds, image_embeddings, audio_embeddings, labels, **kwargs)\u001b[0m\n\u001b[1;32m    203\u001b[0m             \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfull\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_added_tokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    204\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 205\u001b[0;31m         return self.phi3_model(\n\u001b[0m\u001b[1;32m    206\u001b[0m             \u001b[0minputs_embeds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcombined_embeddings\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mextended_attention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py\u001b[0m in \u001b[0;36mnew_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    168\u001b[0m                 \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_old_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 170\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_old_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    171\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_hf_hook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpost_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/phi3/modeling_phi3.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, cache_position, num_logits_to_keep, **loss_kwargs)\u001b[0m\n\u001b[1;32m   1277\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1278\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1279\u001b[0;31m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mloss_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1280\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1281\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mreturn_dict\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/loss/loss_utils.py\u001b[0m in \u001b[0;36mForCausalLMLoss\u001b[0;34m(logits, labels, vocab_size, num_items_in_batch, ignore_index, **kwargs)\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[0;31m# Enable model parallelism\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m     \u001b[0mshift_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mshift_labels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshift_logits\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfixed_cross_entropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshift_logits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshift_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_items_in_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/loss/loss_utils.py\u001b[0m in \u001b[0;36mfixed_cross_entropy\u001b[0;34m(source, target, num_items_in_batch, ignore_index, **kwargs)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mfixed_cross_entropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_items_in_batch\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0mreduction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"sum\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mnum_items_in_batch\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"mean\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctional\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcross_entropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mignore_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mreduction\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"sum\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mnum_items_in_batch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[1;32m   3477\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msize_average\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mreduce\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3478\u001b[0m         \u001b[0mreduction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_get_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize_average\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3479\u001b[0;31m     return torch._C._nn.cross_entropy_loss(\n\u001b[0m\u001b[1;32m   3480\u001b[0m         \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3481\u001b[0m         \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 518.00 MiB. GPU 0 has a total capacity of 14.75 GiB of which 163.06 MiB is free. Process 45524 has 14.59 GiB memory in use. Of the allocated memory 13.61 GiB is allocated by PyTorch, and 862.66 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
          ]
        }
      ],
      "source": [
        "output_dir = \"/content/drive/MyDrive/LLM_training_19Nov_test\"\n",
        "modal, trainer = train_model(dataset_dict, tokenizer, output_dir)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.evaluate()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        },
        "id": "6EPw7o8Lnd0o",
        "outputId": "a783f2ea-a641-4146-f59e-3c9e5bbcedef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'trainer' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-34-f245b31d31e3>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'trainer' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0nxPdMqR8tUw"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "import torch\n",
        "import gc\n",
        "import os\n",
        "\n",
        "def load_and_save_model_from_checkpoint(checkpoint_path, target_save_path, device='cuda', load_in_8bit=False):\n",
        "    \"\"\"\n",
        "    Load model from checkpoint and save it with memory optimization techniques.\n",
        "\n",
        "    Args:\n",
        "        checkpoint_path: Path to the checkpoint directory\n",
        "        target_save_path: Where to save the final model\n",
        "        device: 'cuda' or 'cpu'\n",
        "        load_in_8bit: Whether to load the model in 8-bit precision\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Clear GPU memory before loading\n",
        "        if torch.cuda.is_available():\n",
        "            torch.cuda.empty_cache()\n",
        "            gc.collect()\n",
        "\n",
        "        # Load configuration separately first\n",
        "        tokenizer = AutoTokenizer.from_pretrained(checkpoint_path)\n",
        "\n",
        "        # Configure loading options\n",
        "        load_options = {\n",
        "            \"torch_dtype\": torch.float16 if device == 'cuda' else torch.float32,\n",
        "            \"low_cpu_mem_usage\": True,\n",
        "        }\n",
        "\n",
        "        if load_in_8bit:\n",
        "            load_options[\"load_in_8bit\"] = True\n",
        "\n",
        "        # Load model with memory optimizations\n",
        "        model = AutoModelForCausalLM.from_pretrained(\n",
        "            checkpoint_path,\n",
        "            **load_options\n",
        "        )\n",
        "\n",
        "        # Save model and tokenizer\n",
        "        model.save_pretrained(target_save_path)\n",
        "        tokenizer.save_pretrained(target_save_path)\n",
        "\n",
        "        return True, \"Model successfully loaded and saved\"\n",
        "\n",
        "    except Exception as e:\n",
        "        return False, f\"Error: {str(e)}\"\n",
        "\n",
        "def load_model_in_chunks(checkpoint_path, target_save_path, max_memory=None):\n",
        "    \"\"\"\n",
        "    Load and save model using device map for very large models.\n",
        "\n",
        "    Args:\n",
        "        checkpoint_path: Path to the checkpoint directory\n",
        "        target_save_path: Where to save the final model\n",
        "        max_memory: Dict specifying maximum memory for each device\n",
        "    \"\"\"\n",
        "    if max_memory is None:\n",
        "        max_memory = {\n",
        "            0: \"24GiB\",  # Adjust based on your GPU memory\n",
        "            \"cpu\": \"64GiB\"  # Adjust based on your RAM\n",
        "        }\n",
        "\n",
        "    try:\n",
        "        # Load with device map\n",
        "        model = AutoModelForCausalLM.from_pretrained(\n",
        "            checkpoint_path,\n",
        "            device_map=\"auto\",\n",
        "            max_memory=max_memory,\n",
        "            torch_dtype=torch.float16,\n",
        "            low_cpu_mem_usage=True\n",
        "        )\n",
        "\n",
        "        tokenizer = AutoTokenizer.from_pretrained(checkpoint_path)\n",
        "\n",
        "        # Save model and tokenizer\n",
        "        model.save_pretrained(target_save_path)\n",
        "        tokenizer.save_pretrained(target_save_path)\n",
        "\n",
        "        return True, \"Model successfully loaded and saved using device map\"\n",
        "\n",
        "    except Exception as e:\n",
        "        return False, f\"Error: {str(e)}\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "piFiE0pS8tTb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 228
        },
        "outputId": "2188fe6d-d526-42eb-ee86-c79862fd53c2"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "load_and_save_model_from_checkpoint() got an unexpected keyword argument 'load_in_4bit'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-20-0a016c05007f>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0msave_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"/content/drive/MyDrive/LLM_training_20November/final_model\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m success, message = load_and_save_model_from_checkpoint(\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mcheckpoint_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0msave_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: load_and_save_model_from_checkpoint() got an unexpected keyword argument 'load_in_4bit'"
          ]
        }
      ],
      "source": [
        "checkpoint_path = \"/content/drive/MyDrive/LLM_training_19Nov_test/emergency_checkpoint\"\n",
        "save_path = \"/content/drive/MyDrive/LLM_training_20November/final_model\"\n",
        "\n",
        "success, message = load_and_save_model_from_checkpoint(\n",
        "    checkpoint_path,\n",
        "    save_path,\n",
        "    load_in_8bit=True  # Use 8-bit precision to reduce memory usage\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5liekk1V8q5K",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fa30dc19-f557-4d79-ba5b-5fa4dc30af13"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ],
      "source": [
        "success"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H9MN6DEq_uDU",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "b6591744-ce29-4d0e-bc7d-cde0640472b0"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Model successfully loaded and saved'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 19
        }
      ],
      "source": [
        "message"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "heeIiN0-pzIW"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vaLCCeW7R4v_"
      },
      "outputs": [],
      "source": [
        "# output_dir = \"/content/drive/MyDrive/LLM_training_4hours_16 November\"\n",
        "# train_model(dataset_dict, tokenizer, output_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eArslRiaqrLD"
      },
      "outputs": [],
      "source": [
        "# !pip install --upgrade peft"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8giSwXoaA2dM"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "import torch\n",
        "\n",
        "def load_model_for_inference(model_path, device='cuda'):\n",
        "    \"\"\"\n",
        "    Load model and tokenizer for inference\n",
        "    \"\"\"\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        model_path,\n",
        "        torch_dtype=torch.float16 if device == 'cuda' else torch.float32,\n",
        "        device_map='auto'\n",
        "    )\n",
        "    model.eval()  # Set to evaluation mode\n",
        "    return model, tokenizer\n",
        "\n",
        "def generate_response(\n",
        "    model,\n",
        "    tokenizer,\n",
        "    prompt,\n",
        "    max_length=128,\n",
        "    temperature=0.7,\n",
        "    top_p=0.9,\n",
        "    top_k=50,\n",
        "    num_return_sequences=1,\n",
        "    do_sample=True\n",
        "):\n",
        "    \"\"\"\n",
        "    Generate response from the model\n",
        "    \"\"\"\n",
        "    # Encode the prompt\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
        "    inputs = inputs.to(model.device)\n",
        "\n",
        "    # Generate\n",
        "    outputs = model.generate(\n",
        "        **inputs,\n",
        "        max_length=max_length,\n",
        "        temperature=temperature,\n",
        "        top_p=top_p,\n",
        "        top_k=top_k,\n",
        "        num_return_sequences=num_return_sequences,\n",
        "        do_sample=do_sample,\n",
        "        pad_token_id=tokenizer.pad_token_id,\n",
        "        eos_token_id=tokenizer.eos_token_id,\n",
        "    )\n",
        "\n",
        "    # Decode and return responses\n",
        "    responses = [tokenizer.decode(output, skip_special_tokens=True) for output in outputs]\n",
        "    return responses\n",
        "\n",
        "# Example function for interactive chat\n",
        "def chat_with_model(model, tokenizer):\n",
        "    \"\"\"\n",
        "    Interactive chat loop with the model\n",
        "    \"\"\"\n",
        "    print(\"Chat started (type 'quit' to exit)\")\n",
        "    while True:\n",
        "        user_input = input(\"\\nYou: \")\n",
        "        if user_input.lower() == 'quit':\n",
        "            break\n",
        "\n",
        "        responses = generate_response(\n",
        "            model,\n",
        "            tokenizer,\n",
        "            user_input,\n",
        "            max_length=256  # Adjust based on your needs\n",
        "        )\n",
        "\n",
        "        print(\"\\nModel:\", responses[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aWZg2uRlAZx0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "d11735e7d99d4e46a0cef28a97e42c18",
            "88365a49ff0e468dba014622519d9185",
            "24a75d54ad014799aaa813c9dca0013f",
            "e821f68a40ec4667bb12b23b6bc3385e",
            "0722aa305d924d4faa57f00e5c05b6bc",
            "65897c21d0aa41c4b5946430dd1ccdd7",
            "d62e8d99259b443f8e3f4b16085a6797",
            "82778d5d838e44c6a0fe44ef0ecc7d4b",
            "6c32e62d4d0d48f1b5c85a45838edecf",
            "4c217f25a90b4be1a69e87d05775f428",
            "f00d373ee5a84a0bb938a1ddd0afe1cd"
          ]
        },
        "outputId": "d22c8b81-6b4a-48ed-bb0e-9216a3066c9a"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d11735e7d99d4e46a0cef28a97e42c18"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "model_path = \"/content/drive/MyDrive/LLM_training_17November/final_model\"  # Adjust path\n",
        "model, tokenizer = load_model_for_inference(model_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i6g3AhBKBIGs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f54ac9f8-a710-414f-dd53-2a74dd677176"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "What is artificial intelligence?\n",
            "\n",
            "Artificial intelligence (AI) refers to the development of computer systems that can perform tasks that typically require human intelligence. These tasks include problem-solving, learning, planning, understanding natural language, recognizing patterns, and making decisions. AI aims to create machines that can act and react intelligently, often mimicking human behavior.\n",
            "\n",
            "\n",
            "### Instruction 2 (Much more difficult with at least 4 more constraints) Artificial intelligence (AI) encompasses the creation of computer systems capable of performing tasks that necessitate human-\n"
          ]
        }
      ],
      "source": [
        "prompt = \"What is artificial intelligence?\"\n",
        "responses = generate_response(model, tokenizer, prompt)\n",
        "print(responses[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I356jmP1aZss"
      },
      "outputs": [],
      "source": [
        "# from transformers import AutoProcessor, AutoModelForVisionTextDualEncoding\n",
        "import torch\n",
        "from PIL import Image\n",
        "\n",
        "def setup_image_qa_model(model_path):\n",
        "    \"\"\"\n",
        "    Set up model and processor for image question-answering\n",
        "    \"\"\"\n",
        "    try:\n",
        "        processor = AutoProcessor.from_pretrained(model_path)\n",
        "        model = AutoModelForVisionTextDualEncoding.from_pretrained(\n",
        "            model_path,\n",
        "            torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,\n",
        "            device_map=\"auto\"\n",
        "        )\n",
        "        return model, processor\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading model: {str(e)}\")\n",
        "        return None, None\n",
        "\n",
        "def ask_question_about_image(\n",
        "    model,\n",
        "    processor,\n",
        "    image_path,\n",
        "    question,\n",
        "    max_length=128,\n",
        "    temperature=0.7\n",
        "):\n",
        "    \"\"\"\n",
        "    Ask a question about an image and get a response\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Load and preprocess the image\n",
        "        image = Image.open(image_path)\n",
        "\n",
        "        # Prepare inputs\n",
        "        inputs = processor(\n",
        "            images=image,\n",
        "            text=question,\n",
        "            return_tensors=\"pt\"\n",
        "        )\n",
        "\n",
        "        # Move inputs to the same device as model\n",
        "        inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
        "\n",
        "        # Generate response\n",
        "        outputs = model.generate(\n",
        "            **inputs,\n",
        "            max_length=max_length,\n",
        "            temperature=temperature,\n",
        "            do_sample=True\n",
        "        )\n",
        "\n",
        "        # Decode response\n",
        "        response = processor.decode(outputs[0], skip_special_tokens=True)\n",
        "        return response\n",
        "\n",
        "    except Exception as e:\n",
        "        return f\"Error processing image or generating response: {str(e)}\"\n",
        "\n",
        "def interactive_image_qa(model, processor, image_path):\n",
        "    \"\"\"\n",
        "    Interactive Q&A session about an image\n",
        "    \"\"\"\n",
        "    print(f\"Image loaded from: {image_path}\")\n",
        "    print(\"Ask questions about the image (type 'quit' to exit)\")\n",
        "\n",
        "    while True:\n",
        "        question = input(\"\\nYour question: \")\n",
        "        if question.lower() == 'quit':\n",
        "            break\n",
        "\n",
        "        response = ask_question_about_image(model, processor, image_path, question)\n",
        "        print(\"\\nResponse:\", response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RRIMADifiqqy"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vg7nPf3M2rK8"
      },
      "outputs": [],
      "source": [
        "# !pip install --upgrade transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P_7fBchdh2CO"
      },
      "outputs": [],
      "source": [
        "# dataset_dict"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cYyoVto2aHB-"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UUh_lsCUR6ym"
      },
      "source": [
        "***Training Finishes***\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VgojIdZLc2mg"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pG_vP8L2aIqG"
      },
      "source": [
        "#Testing Date 1st Nov 2024"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6xequ3ePb_ue",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f4635cc6-4274-4e73-e947-cf57799f0f86"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/84.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.0/84.0 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip -q install evaluate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2KMDzVI0aXWq"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "from evaluate import load\n",
        "import json\n",
        "from PIL import Image\n",
        "from accelerate import Accelerator\n",
        "\n",
        "# Load your fine-tuned model and required processors\n",
        "def load_fine_tuned_models():\n",
        "    # Quantize the model to 4-bit using bitsandbytes\n",
        "    bnb_config = BitsAndBytesConfig(\n",
        "        load_in_8bit=True,\n",
        "        llm_int8_threshold=6.0,\n",
        "        llm_int8_has_fp16_weight=False,\n",
        "        bnb_4bit_compute_dtype=torch.float16,\n",
        "        bnb_4bit_quant_type=\"nf4\",\n",
        "        bnb_4bit_use_double_quant=True,\n",
        "    )\n",
        "\n",
        "    # Load the model directly onto the device and enable gradient checkpointing\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        \"/content/drive/MyDrive/LLM_training_output_1Nov2024_V3/final_model\",\n",
        "        quantization_config=bnb_config,\n",
        "        trust_remote_code=True  # Required for loading custom models\n",
        "    )\n",
        "    model.gradient_checkpointing_enable()\n",
        "\n",
        "    tokenizer = AutoTokenizer.from_pretrained(\"microsoft/Phi-3-mini-128k-instruct\")\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "    # Additional processor/loading functions here if needed\n",
        "    return model, tokenizer\n",
        "\n",
        "# Function to handle multimodal inference\n",
        "def multimodal_inference(model, tokenizer, input_text=None, image_embedding=None, audio_embedding=None, device=\"cuda\"):\n",
        "    # Construct the input prompt with roles\n",
        "    prompt_parts = [\"You are a knowledgeable assistant.\"]\n",
        "    if input_text:\n",
        "        prompt_parts.append(f\"User question: {input_text}\")\n",
        "    if image_embedding is not None:\n",
        "        prompt_parts.append(\"An image is provided.\")\n",
        "    if audio_embedding is not None:\n",
        "        prompt_parts.append(\"Audio information is provided.\")\n",
        "\n",
        "    # Combine all parts to form the final prompt\n",
        "    prompt = \" \".join(prompt_parts)\n",
        "\n",
        "    # Tokenize the prompt for the model\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
        "\n",
        "    # Pass embeddings as model inputs if available\n",
        "    inputs_embeds = {}\n",
        "    if image_embedding is not None:\n",
        "        inputs_embeds[\"image_embedding\"] = image_embedding.unsqueeze(0).to(device)\n",
        "    if audio_embedding is not None:\n",
        "        inputs_embeds[\"audio_embedding\"] = audio_embedding.unsqueeze(0).to(device)\n",
        "\n",
        "    # Generate response from the model\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(\n",
        "            input_ids=inputs[\"input_ids\"],\n",
        "            max_new_tokens=100,\n",
        "            do_sample=True,\n",
        "            temperature=0.7,\n",
        "            top_p=0.9,\n",
        "            **inputs_embeds  # add embeddings to the generate function call\n",
        "        )\n",
        "\n",
        "    # Decode and clean up the generated response\n",
        "    response = tokenizer.decode(outputs[0], skip_special_tokens=True).strip()\n",
        "    return response\n",
        "\n",
        "# Function to evaluate a single test example\n",
        "def evaluate_example(example, model, tokenizer, device=\"cuda\"):\n",
        "    # Get inputs from the test example\n",
        "    input_text = example.get(\"text\")\n",
        "    image_embedding = example.get(\"image_embedding\")\n",
        "    audio_embedding = example.get(\"audio_embedding\")\n",
        "    expected_response = example.get(\"expected_response\")\n",
        "\n",
        "    # Run inference\n",
        "    generated_response = multimodal_inference(model, tokenizer, input_text, image_embedding, audio_embedding, device)\n",
        "\n",
        "    # Print results\n",
        "    print(\"Generated Response:\", generated_response)\n",
        "    print(\"Expected Response:\", expected_response)\n",
        "\n",
        "    # Optionally return evaluation metrics if there is an expected response\n",
        "    return {\"generated_response\": generated_response, \"expected_response\": expected_response}\n",
        "\n",
        "# Full evaluation over a test set\n",
        "def evaluate_test_set(test_set_path, model, tokenizer, device=\"cuda\"):\n",
        "    with open(test_set_path, 'r') as f:\n",
        "        test_set = json.load(f)\n",
        "\n",
        "    results = []\n",
        "    for example in test_set:\n",
        "        result = evaluate_example(example, model, tokenizer, device)\n",
        "        results.append(result)\n",
        "\n",
        "    return results\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6zzviKhnaXIM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5347e1b0-9a35-4e2c-f8bd-4992b983e365"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['input_ids', 'attention_mask', 'labels', 'image_embeddings'],\n",
              "    num_rows: 36142\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ],
      "source": [
        "dataset_dict['test']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cbEph9BijhFG"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
        "import torch\n",
        "import json\n",
        "\n",
        "# Load your fine-tuned model and required processors\n",
        "def load_fine_tuned_models():\n",
        "    # Quantize the model to 4-bit using bitsandbytes\n",
        "    bnb_config = BitsAndBytesConfig(\n",
        "        load_in_4bit=True,\n",
        "        bnb_4bit_quant_type=\"nf4\",\n",
        "        bnb_4bit_use_double_quant=True,\n",
        "    )\n",
        "\n",
        "    # Load the model directly onto the device and enable gradient checkpointing\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        \"/content/drive/MyDrive/LLM_training_output_1Nov2024_V3/final_model\",\n",
        "        quantization_config=bnb_config,\n",
        "        trust_remote_code=True  # Required for loading custom models\n",
        "    )\n",
        "    model.gradient_checkpointing_enable()\n",
        "\n",
        "    tokenizer = AutoTokenizer.from_pretrained(\"microsoft/Phi-3-mini-128k-instruct\")\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "    return model, tokenizer\n",
        "\n",
        "# Function to handle multimodal inference\n",
        "def multimodal_inference(model, tokenizer, input_text=None, image_embedding=None, audio_embedding=None, device=\"cuda\"):\n",
        "    # Construct the input prompt with roles\n",
        "    prompt_parts = [\"You are a knowledgeable assistant.\"]\n",
        "    if input_text:\n",
        "        prompt_parts.append(f\"User question: {input_text}\")\n",
        "    if image_embedding is not None:\n",
        "        prompt_parts.append(\"An image is provided.\")\n",
        "    if audio_embedding is not None:\n",
        "        prompt_parts.append(\"Audio information is provided.\")\n",
        "\n",
        "    # Combine all parts to form the final prompt\n",
        "    prompt = \" \".join(prompt_parts)\n",
        "\n",
        "    # Tokenize the prompt for the model\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
        "\n",
        "    # Handle embedding inputs\n",
        "    if image_embedding is not None:\n",
        "        image_embedding = image_embedding.unsqueeze(0).to(device)\n",
        "    if audio_embedding is not None:\n",
        "        audio_embedding = audio_embedding.unsqueeze(0).to(device)\n",
        "\n",
        "    # Generate response from the model with custom embeddings\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(\n",
        "            #input_ids=inputs[\"input_ids\"],\n",
        "            **inputs,\n",
        "            max_new_tokens=100,\n",
        "            do_sample=True,\n",
        "            temperature=0.7,\n",
        "            top_p=0.9,\n",
        "        )\n",
        "\n",
        "    # Decode and clean up the generated response\n",
        "    response = tokenizer.decode(outputs[0], skip_special_tokens=True).strip()\n",
        "    return response\n",
        "\n",
        "# Function to evaluate a single test example\n",
        "def evaluate_example(example, model, tokenizer, device=\"cuda\"):\n",
        "    # Get inputs from the test example\n",
        "    input_text = example.get(\"text\")\n",
        "    image_embedding = example.get(\"image_embedding\")\n",
        "    audio_embedding = example.get(\"audio_embedding\")\n",
        "    expected_response = example.get(\"expected_response\")\n",
        "\n",
        "    # Run inference\n",
        "    generated_response = multimodal_inference(model, tokenizer, input_text, image_embedding, audio_embedding, device)\n",
        "\n",
        "    # Print results\n",
        "    print(\"Generated Response:\", generated_response)\n",
        "    print(\"Expected Response:\", expected_response)\n",
        "\n",
        "    # Optionally return evaluation metrics if there is an expected response\n",
        "    return {\"generated_response\": generated_response, \"expected_response\": expected_response}\n",
        "\n",
        "# Full evaluation over a test set\n",
        "def evaluate_test_set(test_set_path, model, tokenizer, device=\"cuda\"):\n",
        "    with open(test_set_path, 'r') as f:\n",
        "        test_set = json.load(f)\n",
        "\n",
        "    results = []\n",
        "    for example in test_set:\n",
        "        try:\n",
        "            result = evaluate_example(example, model, tokenizer, device)\n",
        "            results.append(result)\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing example {example}: {e}\")\n",
        "            continue  # Skip any problematic examples\n",
        "\n",
        "    return results\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vzIQHC7bjhBd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "a033e2a7d3164866a112174783fc9e2b",
            "e6dff8cd54584478bce37e69fb6b52d9",
            "bcc92e9686794fa48a2d338eb50d42db",
            "596c98baaf674db9baacffb3a6355273",
            "ea8371e12adb4184a9d56437573a1e6e",
            "bf6f386c6068461a979facff868b5104",
            "af045d878fa5432a92cf9ec634c49231",
            "0f89c66647944811adb285bdb534c265",
            "d0bfd853ba8e4fa4867b22d8b870b47c",
            "f7e142d668e04ed18f07d72b78d1f976",
            "8942582682a44abab711b9fee8bf6b18",
            "077b112e05bc4f49a53a832e58596d49",
            "05c57802f87b43d7a9e5e7356702d0ba",
            "d32fa8217f824f78b991d4fbc6d939ab",
            "cb5ba2b9963a45229ee40bdfee1b8188",
            "c5b3efdd635b4e889de0fba11329a93b",
            "b3e87413791a4d8f9dc9c7451124d482",
            "f24b1eef8cbc461a82289101dea6b522",
            "4370d76f368a4b5aa76e2b6624d1cb82",
            "ee74cfda88954a739c4262fe5d80c525",
            "7aa9c85fba9c4b0799aad64536d4b8cc",
            "8e4785d1fb924b70b0ad231fe8f6cff7",
            "2c52db0e9029419dbccfa1ff90a4c008",
            "8266ded3fc364ac0b74c86fa741dec8a",
            "b4f0d4a00ca7424986661c32a862cfbd",
            "d146359e3fc945daa7b803857cb8922e",
            "d60a5a316c19479eb4c1ad01008f97c6",
            "6bb74baf484b43dbaad159b8bc8a8d50",
            "d4451334dd84476ab3bd426ec3d2c5e3",
            "cf1c2cecb8834ee1abaa6e06cdffa928",
            "14c53d48014345578220caff0e81ca43",
            "3cd0bad0755f44458d363af9b74bebb9",
            "fd2536710e9041aa9dba4ec8ac1c5499"
          ]
        },
        "outputId": "f0153e24-94bd-4e65-d7e1-ccad3c380802"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "configuration_phi3.py:   0%|          | 0.00/11.2k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a033e2a7d3164866a112174783fc9e2b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "A new version of the following files was downloaded from https://huggingface.co/microsoft/Phi-3-mini-128k-instruct:\n",
            "- configuration_phi3.py\n",
            ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "modeling_phi3.py:   0%|          | 0.00/73.2k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "077b112e05bc4f49a53a832e58596d49"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "A new version of the following files was downloaded from https://huggingface.co/microsoft/Phi-3-mini-128k-instruct:\n",
            "- modeling_phi3.py\n",
            ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n",
            "WARNING:transformers_modules.microsoft.Phi-3-mini-128k-instruct.a90b62ae09941edff87a90ced39ba5807e6b2ade.modeling_phi3:`flash-attention` package not found, consider installing for better performance: No module named 'flash_attn'.\n",
            "WARNING:transformers_modules.microsoft.Phi-3-mini-128k-instruct.a90b62ae09941edff87a90ced39ba5807e6b2ade.modeling_phi3:Current `flash-attention` does not support `window_size`. Either upgrade or use `attn_implementation='eager'`.\n",
            "`low_cpu_mem_usage` was None, now default to True since model is quantized.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "2c52db0e9029419dbccfa1ff90a4c008"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The `seen_tokens` attribute is deprecated and will be removed in v4.41. Use the `cache_position` model input instead.\n",
            "`get_max_cache()` is deprecated for all Cache classes. Use `get_max_cache_shape()` instead. Calling `get_max_cache()` will raise error from v4.48\n",
            "WARNING:transformers_modules.microsoft.Phi-3-mini-128k-instruct.a90b62ae09941edff87a90ced39ba5807e6b2ade.modeling_phi3:You are not running the flash-attention implementation, expect numerical differences.\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/nn/modules.py:452: UserWarning: Input type into Linear4bit is torch.float16, but bnb_4bit_compute_dtype=torch.float32 (default). This will lead to slow inference or training speed.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated Response: You are a knowledgeable assistant. User question: \n",
            " You are a helpful assistant.<|endofsystem|> What is the color of the thermometer in the image? The color of the thermometer in the image is white and red.      An image is provided. The image contains a thermometer, a mug, and a notebook. The thermometer has a white body and a red mercury column. The mug is a ceramic cup with a blue stripe and a handle. The notebook has a black cover and is open with a yellow pen on it. What color is the thermometer'rád akkor, hogyan érthetőek?\n",
            "\n",
            "The thermometer in the image is white with a red\n",
            "Expected Response: \n",
            " You are a helpful assistant.<|endofsystem|> What is the color of the thermometer in the image? The color of the thermometer in the image is white and red.     \n",
            "Generated Response: You are a knowledgeable assistant. User question: \n",
            " You are a helpful assistant.<|endofsystem|> Are the stuffed bears in the image wearing various outfits? Yes, many of the stuffed bears are wearing different costumes or clothes, which gives each bear a distinct appearance.      An image is provided. Analyze the image and answer the following question: In the image, are there any stuffed bears that are wearing glasses? If so, please list all the bears' names and the type of glasses they are wearing.\n",
            "\n",
            "\n",
            "# Answer: \n",
            "I'm sorry, but I can't access images or provide analysis of them. However, in a hypothetical scenario where I could analyze an image, I would describe the process as follows:\n",
            "Expected Response: \n",
            " You are a helpful assistant.<|endofsystem|> Are the stuffed bears in the image wearing various outfits? Yes, many of the stuffed bears are wearing different costumes or clothes, which gives each bear a distinct appearance.     \n",
            "Generated Response: You are a knowledgeable assistant. User question: \n",
            " You are a helpful assistant.<|endofsystem|> What is the condition of the fruits and vegetables? The fruits and vegetables appear fresh, with the exception of the baby carrots, which are in a bag. They are lying on a white surface, making a colorful and appealing arrangement.      An image is provided. Can you answer the following question based on the image: How are the fruits and vegetables arranged in the image? The fruits and vegetables in the image are arranged in a colorful and appealing manner on a white surface. Specifically, the fruits are in a large basket, while the vegetables are neatly arranged in a small, clear plastic container. The baby carrots are in a bag, separate from the other produce. The overall arrangement seems to be carefully cur\n",
            "Expected Response: \n",
            " You are a helpful assistant.<|endofsystem|> What is the condition of the fruits and vegetables? The fruits and vegetables appear fresh, with the exception of the baby carrots, which are in a bag. They are lying on a white surface, making a colorful and appealing arrangement.     \n",
            "Generated Response: You are a knowledgeable assistant. User question: \n",
            " You are a helpful assistant.<|endofsystem|> What kind of environment is the zebra's enclosure designed to resemble? The zebra's enclosure is designed to resemble a natural habitat, as it contains trees, plants, and rocky terrain. This zoo-like environment attempts to provide the zebra with a setting that mimics its natural living conditions while keeping it safely in captivity.      An image is provided. Can you tell if the image depicts a zebra in its enclosure? I'm sorry, but as a text-based AI, I can't process images. However, I can guide you on how to identify a zebra in its enclosure in a picture. Look for an animal with black and white stripes on its body, a long neck, and a mane. The enclosure should contain trees, rocks, and other elements typical of the z\n",
            "Expected Response: \n",
            " You are a helpful assistant.<|endofsystem|> What kind of environment is the zebra's enclosure designed to resemble? The zebra's enclosure is designed to resemble a natural habitat, as it contains trees, plants, and rocky terrain. This zoo-like environment attempts to provide the zebra with a setting that mimics its natural living conditions while keeping it safely in captivity.     \n",
            "Generated Response: You are a knowledgeable assistant. User question: \n",
            " You are a helpful assistant.<|endofsystem|> Is the group simply standing around or actively preparing food? The group is actively preparing food, with the children reaching for ingredients and the adults participating in the process. They are not simply standing around the table.      An image is provided. Does the image contain the object 'banana peel'? No, the image does not contain a 'banana peel'. The focus of the image is the group of people cooking together. \n",
            " Q: Is the following statement correct based on the document'首页's description? The '首页' depicts a scene of culinary preparation with various ingredients present.\n",
            " A: Yes, the statement is correct. The '首页' indeed dep\n",
            "Expected Response: \n",
            " You are a helpful assistant.<|endofsystem|> Is the group simply standing around or actively preparing food? The group is actively preparing food, with the children reaching for ingredients and the adults participating in the process. They are not simply standing around the table.     \n",
            "Generated Response: You are a knowledgeable assistant. User question: \n",
            " You are a helpful assistant.<|endofsystem|> What are the key elements in this picture? The scene features a busy intersection in a city with numerous cars awaiting the green light. There is a total of 8 cars visible on the crowded street, with some of them closely packed together. Some of these vehicles are positioned closer to the front while others are farther back. \n",
            "\n",
            "Three people can be seen in the area as well, either walking or standing by the road. Various traffic lights are also present in the image; one lit with red is easily noticeable, forcing cars to stop at the intersection.\n",
            "\n",
            "Palm trees can be spotted near the intersection, indicating a possible warm climate. Furthermore, there is a sign for a Chevron gas station at the side of the road, suggesting a nearby refueling point for vehicles. A fire hydrant is also visible on the sidewalk, ensuring fire safety in the city.      An image is provided. Describe the image. In the image, we can observe a serene beach scene during the early morning hours. The beach is relatively empty, with only a few scattered people walking along the shoreline. There is a solitary lighthouse standing tall in the distance, casting a long, thin shadow over the water. The sea is calm and clear, reflecting the sky above. The sand is soft and white, gently sloping towards the water'utoriously blue. The\n",
            "Expected Response: \n",
            " You are a helpful assistant.<|endofsystem|> What are the key elements in this picture? The scene features a busy intersection in a city with numerous cars awaiting the green light. There is a total of 8 cars visible on the crowded street, with some of them closely packed together. Some of these vehicles are positioned closer to the front while others are farther back. \n",
            "\n",
            "Three people can be seen in the area as well, either walking or standing by the road. Various traffic lights are also present in the image; one lit with red is easily noticeable, forcing cars to stop at the intersection.\n",
            "\n",
            "Palm trees can be spotted near the intersection, indicating a possible warm climate. Furthermore, there is a sign for a Chevron gas station at the side of the road, suggesting a nearby refueling point for vehicles. A fire hydrant is also visible on the sidewalk, ensuring fire safety in the city.     \n",
            "Generated Response: You are a knowledgeable assistant. User question: \n",
            " You are a helpful assistant.<|endofsystem|> What activities can someone engage in at this location? At this location by the harbor, someone can engage in various activities. They can visit the nearby stores and businesses along the docks or take a leisurely walk along the water. The available benches offer a place to sit, enjoy the serene waterfront view, and watch the boats as they come and go. Visitors may also interact with locals or observe the daily life of the town. Additionally, they might be able to rent a boat or join a guided tour, depending on the services offered in the area. This waterfront location provides a combination of recreational activities, shopping, and sightseeing opportunities for visitors and residents alike.      An image is provided. Can you tell me the color of the sky in the image? I'm sorry, but I can't assist with that as I can't access or interpret images. However, I can provide some general information about the sky. Typically, the sky's color can vary based on the time of day and weather conditions. During the day, the sky usually appears blue due to the scattering of sunlight by the atmosphere. At sunrise and sunset, the sky often\n",
            "Expected Response: \n",
            " You are a helpful assistant.<|endofsystem|> What activities can someone engage in at this location? At this location by the harbor, someone can engage in various activities. They can visit the nearby stores and businesses along the docks or take a leisurely walk along the water. The available benches offer a place to sit, enjoy the serene waterfront view, and watch the boats as they come and go. Visitors may also interact with locals or observe the daily life of the town. Additionally, they might be able to rent a boat or join a guided tour, depending on the services offered in the area. This waterfront location provides a combination of recreational activities, shopping, and sightseeing opportunities for visitors and residents alike.     \n",
            "Generated Response: You are a knowledgeable assistant. User question: \n",
            " You are a helpful assistant.<|endofsystem|> What is the color of the clock tower in the image? The color of the clock tower in the image is blue.      An image is provided. On the image, a person is wearing a red hat and a blue shirt. Another person is holding a green umbrella. What color is the hat of the person on the left? The person on the left is wearing a red hat.  \n",
            "What is the color of the shirt of the person on the right? The person on the right is wearing a blue shirt.  \n",
            "What color is the umbrella that the second person is holding? The um\n",
            "Expected Response: \n",
            " You are a helpful assistant.<|endofsystem|> What is the color of the clock tower in the image? The color of the clock tower in the image is blue.     \n",
            "Generated Response: You are a knowledgeable assistant. User question: \n",
            " You are a helpful assistant.<|endofsystem|> What is the facial expression of the woman in the image? The woman is smiling in the image, showing a positive and happy expression.      An image is provided. The image shows a group of people gathered in a park. The woman in the image is standing in the center of the group. She is smiling and her eyes are sparkling. She is wearing a blue dress and has a pearl necklace around her neck. The man in the image is standing next to her, looking at her with a loving gaze. The children in the image are playing with a frisbee and a dog in the background.\n",
            "Expected Response: \n",
            " You are a helpful assistant.<|endofsystem|> What is the facial expression of the woman in the image? The woman is smiling in the image, showing a positive and happy expression.     \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:root:Internal Python error in the inspect module.\n",
            "Below is the traceback from this internal error.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\n",
            "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
            "  File \"<ipython-input-36-cf8612b29d91>\", line 31, in <cell line: 31>\n",
            "    test_results = evaluate_test_set_hf(dataset_dict['test'], model, tokenizer, device=\"cuda\")\n",
            "  File \"<ipython-input-36-cf8612b29d91>\", line 20, in evaluate_test_set_hf\n",
            "    result = evaluate_example(\n",
            "  File \"<ipython-input-35-402e26cdfefd>\", line 73, in evaluate_example\n",
            "    generated_response = multimodal_inference(model, tokenizer, input_text, image_embedding, audio_embedding, device)\n",
            "  File \"<ipython-input-35-402e26cdfefd>\", line 51, in multimodal_inference\n",
            "    outputs = model.generate(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py\", line 2215, in generate\n",
            "    result = self._sample(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py\", line 3206, in _sample\n",
            "    outputs = self(**model_inputs, return_dict=True)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py\", line 170, in new_forward\n",
            "    output = module._old_forward(*args, **kwargs)\n",
            "  File \"/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-128k-instruct/a90b62ae09941edff87a90ced39ba5807e6b2ade/modeling_phi3.py\", line 1243, in forward\n",
            "    outputs = self.model(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py\", line 170, in new_forward\n",
            "    output = module._old_forward(*args, **kwargs)\n",
            "  File \"/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-128k-instruct/a90b62ae09941edff87a90ced39ba5807e6b2ade/modeling_phi3.py\", line 1121, in forward\n",
            "    layer_outputs = decoder_layer(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py\", line 170, in new_forward\n",
            "    output = module._old_forward(*args, **kwargs)\n",
            "  File \"/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-128k-instruct/a90b62ae09941edff87a90ced39ba5807e6b2ade/modeling_phi3.py\", line 855, in forward\n",
            "    hidden_states = self.mlp(hidden_states)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py\", line 170, in new_forward\n",
            "    output = module._old_forward(*args, **kwargs)\n",
            "  File \"/root/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-128k-instruct/a90b62ae09941edff87a90ced39ba5807e6b2ade/modeling_phi3.py\", line 235, in forward\n",
            "    return self.down_proj(up_states)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/peft/tuners/lora/bnb.py\", line 467, in forward\n",
            "    result = self.base_layer(x, *args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py\", line 170, in new_forward\n",
            "    output = module._old_forward(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/bitsandbytes/nn/modules.py\", line 486, in forward\n",
            "    out = out.to(inp_dtype)\n",
            "KeyboardInterrupt\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 2099, in showtraceback\n",
            "    stb = value._render_traceback_()\n",
            "AttributeError: 'KeyboardInterrupt' object has no attribute '_render_traceback_'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 1101, in get_records\n",
            "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 248, in wrapped\n",
            "    return f(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 281, in _fixed_getinnerframes\n",
            "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
            "  File \"/usr/lib/python3.10/inspect.py\", line 1662, in getinnerframes\n",
            "    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n",
            "  File \"/usr/lib/python3.10/inspect.py\", line 1624, in getframeinfo\n",
            "    lines, lnum = findsource(frame)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 170, in findsource\n",
            "    file = getsourcefile(object) or getfile(object)\n",
            "  File \"/usr/lib/python3.10/inspect.py\", line 829, in getsourcefile\n",
            "    module = getmodule(object, filename)\n",
            "  File \"/usr/lib/python3.10/inspect.py\", line 868, in getmodule\n",
            "    for modname, module in sys.modules.copy().items():\n",
            "KeyboardInterrupt\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "object of type 'NoneType' has no len()",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
            "\u001b[0;32m<ipython-input-36-cf8612b29d91>\u001b[0m in \u001b[0;36m<cell line: 31>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;31m# Step 3: Run evaluation on dataset_dict['test']\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m \u001b[0mtest_results\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate_test_set_hf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'test'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"cuda\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-36-cf8612b29d91>\u001b[0m in \u001b[0;36mevaluate_test_set_hf\u001b[0;34m(test_dataset, model, tokenizer, device)\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0;31m# Run evaluation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m         result = evaluate_example(\n\u001b[0m\u001b[1;32m     21\u001b[0m             \u001b[0;34m{\u001b[0m\u001b[0;34m\"text\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0minput_text\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"image_embedding\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mimage_embedding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"expected_response\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mexpected_response\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-35-402e26cdfefd>\u001b[0m in \u001b[0;36mevaluate_example\u001b[0;34m(example, model, tokenizer, device)\u001b[0m\n\u001b[1;32m     72\u001b[0m     \u001b[0;31m# Run inference\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m     \u001b[0mgenerated_response\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmultimodal_inference\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_text\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage_embedding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maudio_embedding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-35-402e26cdfefd>\u001b[0m in \u001b[0;36mmultimodal_inference\u001b[0;34m(model, tokenizer, input_text, image_embedding, audio_embedding, device)\u001b[0m\n\u001b[1;32m     50\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m         outputs = model.generate(\n\u001b[0m\u001b[1;32m     52\u001b[0m             \u001b[0;31m#input_ids=inputs[\"input_ids\"],\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mctx_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   2214\u001b[0m             \u001b[0;31m# 12. run sample (it degenerates to greedy search when `generation_config.do_sample=False`)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2215\u001b[0;31m             result = self._sample(\n\u001b[0m\u001b[1;32m   2216\u001b[0m                 \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py\u001b[0m in \u001b[0;36m_sample\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   3205\u001b[0m             \u001b[0;31m# forward pass to get next token\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3206\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mmodel_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3207\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py\u001b[0m in \u001b[0;36mnew_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    169\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 170\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_old_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    171\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_hf_hook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpost_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-128k-instruct/a90b62ae09941edff87a90ced39ba5807e6b2ade/modeling_phi3.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1242\u001b[0m         \u001b[0;31m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1243\u001b[0;31m         outputs = self.model(\n\u001b[0m\u001b[1;32m   1244\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py\u001b[0m in \u001b[0;36mnew_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    169\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 170\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_old_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    171\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_hf_hook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpost_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-128k-instruct/a90b62ae09941edff87a90ced39ba5807e6b2ade/modeling_phi3.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1120\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1121\u001b[0;31m                 layer_outputs = decoder_layer(\n\u001b[0m\u001b[1;32m   1122\u001b[0m                     \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py\u001b[0m in \u001b[0;36mnew_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    169\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 170\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_old_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    171\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_hf_hook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpost_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-128k-instruct/a90b62ae09941edff87a90ced39ba5807e6b2ade/modeling_phi3.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, **kwargs)\u001b[0m\n\u001b[1;32m    854\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpost_attention_layernorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 855\u001b[0;31m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmlp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    856\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresidual\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresid_mlp_dropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py\u001b[0m in \u001b[0;36mnew_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    169\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 170\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_old_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    171\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_hf_hook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpost_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-128k-instruct/a90b62ae09941edff87a90ced39ba5807e6b2ade/modeling_phi3.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states)\u001b[0m\n\u001b[1;32m    234\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 235\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdown_proj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mup_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    236\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/peft/tuners/lora/bnb.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, *args, **kwargs)\u001b[0m\n\u001b[1;32m    466\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 467\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    468\u001b[0m                 \u001b[0;31m# As per Tim Dettmers, for 4bit, we need to defensively clone here.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py\u001b[0m in \u001b[0;36mnew_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    169\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 170\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_old_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    171\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_hf_hook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpost_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/bitsandbytes/nn/modules.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    485\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 486\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minp_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    487\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mshowtraceback\u001b[0;34m(self, exc_tuple, filename, tb_offset, exception_only, running_compiled_code)\u001b[0m\n\u001b[1;32m   2098\u001b[0m                         \u001b[0;31m# in the engines. This should return a list of strings.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2099\u001b[0;31m                         \u001b[0mstb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_render_traceback_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2100\u001b[0m                     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'KeyboardInterrupt' object has no attribute '_render_traceback_'",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mshowtraceback\u001b[0;34m(self, exc_tuple, filename, tb_offset, exception_only, running_compiled_code)\u001b[0m\n\u001b[1;32m   2099\u001b[0m                         \u001b[0mstb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_render_traceback_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2100\u001b[0m                     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2101\u001b[0;31m                         stb = self.InteractiveTB.structured_traceback(etype,\n\u001b[0m\u001b[1;32m   2102\u001b[0m                                             value, tb, tb_offset=tb_offset)\n\u001b[1;32m   2103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mstructured_traceback\u001b[0;34m(self, etype, value, tb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[1;32m   1365\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1366\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1367\u001b[0;31m         return FormattedTB.structured_traceback(\n\u001b[0m\u001b[1;32m   1368\u001b[0m             self, etype, value, tb, tb_offset, number_of_lines_of_context)\n\u001b[1;32m   1369\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mstructured_traceback\u001b[0;34m(self, etype, value, tb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[1;32m   1265\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbose_modes\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1266\u001b[0m             \u001b[0;31m# Verbose modes need a full traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1267\u001b[0;31m             return VerboseTB.structured_traceback(\n\u001b[0m\u001b[1;32m   1268\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0metype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtb_offset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumber_of_lines_of_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1269\u001b[0m             )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mstructured_traceback\u001b[0;34m(self, etype, evalue, etb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[1;32m   1122\u001b[0m         \u001b[0;34m\"\"\"Return a nice text document describing the traceback.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1124\u001b[0;31m         formatted_exception = self.format_exception_as_a_whole(etype, evalue, etb, number_of_lines_of_context,\n\u001b[0m\u001b[1;32m   1125\u001b[0m                                                                tb_offset)\n\u001b[1;32m   1126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mformat_exception_as_a_whole\u001b[0;34m(self, etype, evalue, etb, number_of_lines_of_context, tb_offset)\u001b[0m\n\u001b[1;32m   1080\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1081\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1082\u001b[0;31m         \u001b[0mlast_unique\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecursion_repeat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfind_recursion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0morig_etype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1083\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1084\u001b[0m         \u001b[0mframes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat_records\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecords\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlast_unique\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecursion_repeat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mfind_recursion\u001b[0;34m(etype, value, records)\u001b[0m\n\u001b[1;32m    380\u001b[0m     \u001b[0;31m# first frame (from in to out) that looks different.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    381\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_recursion_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0metype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 382\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    383\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    384\u001b[0m     \u001b[0;31m# Select filename, lineno, func_name to track frames with\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: object of type 'NoneType' has no len()"
          ]
        }
      ],
      "source": [
        "# Step 1: Load the fine-tuned model and tokenizer\n",
        "model, tokenizer = load_fine_tuned_models()\n",
        "\n",
        "# Step 2: Define the evaluation function over the Dataset object\n",
        "def evaluate_test_set_hf(test_dataset, model, tokenizer, device=\"cuda\"):\n",
        "    results = []\n",
        "    for example in test_dataset:\n",
        "        # Prepare inputs for evaluation\n",
        "        input_text = example.get(\"input_ids\")  # Adapt as needed for your input text format\n",
        "        image_embedding = example.get(\"image_embeddings\")\n",
        "        expected_response = example.get(\"labels\")  # Assuming labels hold expected response text or IDs\n",
        "\n",
        "        # Convert input_text and labels to text using tokenizer if they are token IDs\n",
        "        if input_text is not None:\n",
        "            input_text = tokenizer.decode(input_text, skip_special_tokens=True)\n",
        "        if expected_response is not None:\n",
        "            expected_response = tokenizer.decode(expected_response, skip_special_tokens=True)\n",
        "\n",
        "        # Run evaluation\n",
        "        result = evaluate_example(\n",
        "            {\"text\": input_text, \"image_embedding\": image_embedding, \"expected_response\": expected_response},\n",
        "            model,\n",
        "            tokenizer,\n",
        "            device\n",
        "        )\n",
        "        results.append(result)\n",
        "\n",
        "    return results\n",
        "\n",
        "# Step 3: Run evaluation on dataset_dict['test']\n",
        "test_results = evaluate_test_set_hf(dataset_dict['test'], model, tokenizer, device=\"cuda\")\n",
        "\n",
        "# Print the results\n",
        "for result in test_results:\n",
        "    print(\"Generated Response:\", result[\"generated_response\"])\n",
        "    print(\"Expected Response:\", result[\"expected_response\"])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uL1U5tw2_omX"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
        "import torch\n",
        "import json\n",
        "\n",
        "# Load your fine-tuned model and required processors\n",
        "def load_fine_tuned_models():\n",
        "    # Quantize the model to 4-bit using bitsandbytes\n",
        "    bnb_config = BitsAndBytesConfig(\n",
        "        load_in_4bit=True,\n",
        "        bnb_4bit_quant_type=\"nf4\",\n",
        "        bnb_4bit_use_double_quant=True,\n",
        "    )\n",
        "\n",
        "    # Load the model directly onto the device and enable gradient checkpointing\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        \"/content/drive/MyDrive/LLM_training_output_1Nov2024\",\n",
        "        quantization_config=bnb_config,\n",
        "        trust_remote_code=True  # Required for loading custom models\n",
        "    )\n",
        "    model.gradient_checkpointing_enable()\n",
        "\n",
        "    tokenizer = AutoTokenizer.from_pretrained(\"microsoft/Phi-3-mini-128k-instruct\")\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "    return model, tokenizer\n",
        "\n",
        "# Function to handle multimodal inference\n",
        "def multimodal_inference(model, tokenizer, input_text=None, image_embedding=None, audio_embedding=None, device=\"cuda\"):\n",
        "    # Construct the input prompt with roles\n",
        "    prompt_parts = [\"You are a knowledgeable assistant.\"]\n",
        "    if input_text:\n",
        "        prompt_parts.append(f\"User question: {input_text}\")\n",
        "    if image_embedding is not None:\n",
        "        prompt_parts.append(\"An image is provided.\")\n",
        "    if audio_embedding is not None:\n",
        "        prompt_parts.append(\"Audio information is provided.\")\n",
        "\n",
        "    # Combine all parts to form the final prompt\n",
        "    prompt = \" \".join(prompt_parts)\n",
        "\n",
        "    # Tokenize the prompt for the model\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
        "\n",
        "    # Handle embedding inputs\n",
        "    if image_embedding is not None:\n",
        "        image_embedding = image_embedding.unsqueeze(0).to(device)\n",
        "    if audio_embedding is not None:\n",
        "        audio_embedding = audio_embedding.unsqueeze(0).to(device)\n",
        "\n",
        "    # Generate response from the model with custom embeddings\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(\n",
        "            input_ids=inputs[\"input_ids\"],\n",
        "            max_new_tokens=100,\n",
        "            do_sample=True,\n",
        "            temperature=0.7,\n",
        "            top_p=0.9,\n",
        "        )\n",
        "\n",
        "    # Decode and clean up the generated response\n",
        "    response = tokenizer.decode(outputs[0], skip_special_tokens=True).strip()\n",
        "    return response\n",
        "\n",
        "# Function to evaluate a single test example\n",
        "def evaluate_example(example, model, tokenizer, device=\"cuda\"):\n",
        "    # Get inputs from the test example\n",
        "    input_text = example.get(\"text\")\n",
        "    image_embedding = example.get(\"image_embedding\")\n",
        "    audio_embedding = example.get(\"audio_embedding\")\n",
        "    expected_response = example.get(\"expected_response\")\n",
        "\n",
        "    # Run inference\n",
        "    generated_response = multimodal_inference(model, tokenizer, input_text, image_embedding, audio_embedding, device)\n",
        "\n",
        "    # Print results\n",
        "    print(\"Generated Response:\", generated_response)\n",
        "    print(\"Expected Response:\", expected_response)\n",
        "\n",
        "    # Optionally return evaluation metrics if there is an expected response\n",
        "    return {\"generated_response\": generated_response, \"expected_response\": expected_response}\n",
        "\n",
        "# Full evaluation over a test set\n",
        "def evaluate_test_set(test_set_path, model, tokenizer, device=\"cuda\"):\n",
        "    with open(test_set_path, 'r') as f:\n",
        "        test_set = json.load(f)\n",
        "\n",
        "    results = []\n",
        "    for example in test_set:\n",
        "        try:\n",
        "            result = evaluate_example(example, model, tokenizer, device)\n",
        "            results.append(result)\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing example {example}: {e}\")\n",
        "            continue  # Skip any problematic examples\n",
        "\n",
        "    return results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PCrIXiNqjg8W"
      },
      "outputs": [],
      "source": [
        "# Step 1: Load the fine-tuned model and tokenizer\n",
        "model, tokenizer = load_fine_tuned_models()\n",
        "\n",
        "# Step 2: Define the evaluation function over the Dataset object\n",
        "def evaluate_test_set_hf(test_dataset, model, tokenizer, device=\"cuda\"):\n",
        "    results = []\n",
        "    for example in test_dataset:\n",
        "        # Prepare inputs for evaluation\n",
        "        input_text = example.get(\"input_ids\")  # Adapt as needed for your input text format\n",
        "        image_embedding = example.get(\"image_embeddings\")\n",
        "        expected_response = example.get(\"labels\")  # Assuming labels hold expected response text or IDs\n",
        "\n",
        "        # Convert input_text and labels to text using tokenizer if they are token IDs\n",
        "        if input_text is not None:\n",
        "            input_text = tokenizer.decode(input_text, skip_special_tokens=True)\n",
        "        if expected_response is not None:\n",
        "            expected_response = tokenizer.decode(expected_response, skip_special_tokens=True)\n",
        "\n",
        "        # Run evaluation\n",
        "        result = evaluate_example(\n",
        "            {\"text\": input_text, \"image_embedding\": image_embedding, \"expected_response\": expected_response},\n",
        "            model,\n",
        "            tokenizer,\n",
        "            device\n",
        "        )\n",
        "        results.append(result)\n",
        "\n",
        "    return results\n",
        "\n",
        "# Step 3: Run evaluation on dataset_dict['test']\n",
        "test_results = evaluate_test_set_hf(dataset_dict['test'], model, tokenizer, device=\"cuda\")\n",
        "\n",
        "# Print the results\n",
        "for result in test_results:\n",
        "    print(\"Generated Response:\", result[\"generated_response\"])\n",
        "    print(\"Expected Response:\", result[\"expected_response\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6gfkZODmjg5V"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XRbzciWDjg2N"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i2mFnRomjgzV"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VKoP9FNrjgsl"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DuDjbscbaW96"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7eff_C-MaIm1"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0XiZxRgNaIjl"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OMVAK3NHaIOt"
      },
      "source": [
        "# Inference starts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "56BXGWgqEi3_"
      },
      "outputs": [],
      "source": [
        "output_dir = \"/content/drive/MyDrive/LLM_training_4hours_16 November/final_model\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UPRIPaHH09To"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, CLIPProcessor, CLIPModel\n",
        "from transformers import WhisperProcessor, WhisperForConditionalGeneration\n",
        "from PIL import Image\n",
        "import gc\n",
        "\n",
        "# Define device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "def load_models(output_dir):\n",
        "    \"\"\"\n",
        "    Load all required models and processors, including your fine-tuned model\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Load your fine-tuned model and tokenizer directly\n",
        "        model = AutoModelForCausalLM.from_pretrained(\n",
        "            output_dir,\n",
        "            torch_dtype=torch.float16,\n",
        "            device_map=\"auto\"\n",
        "        )\n",
        "        tokenizer = AutoTokenizer.from_pretrained(output_dir)\n",
        "\n",
        "        # Load CLIP models\n",
        "        clip_processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
        "        clip_model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\").to(device)\n",
        "\n",
        "        # Load Whisper models\n",
        "        whisper_processor = WhisperProcessor.from_pretrained(\"openai/whisper-small\")\n",
        "        whisper_model = WhisperForConditionalGeneration.from_pretrained(\"openai/whisper-small\").to(device)\n",
        "\n",
        "        # Set model to eval mode\n",
        "        model.eval()\n",
        "        torch.set_grad_enabled(False)\n",
        "\n",
        "        return tokenizer, model, clip_processor, clip_model, whisper_processor, whisper_model\n",
        "\n",
        "    except Exception as e:\n",
        "        raise Exception(f\"Error loading models: {str(e)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BhAZXcEvsAIj"
      },
      "outputs": [],
      "source": [
        "from PIL import Image\n",
        "import gc\n",
        "import torch\n",
        "\n",
        "def generate_response(\n",
        "    model,\n",
        "    tokenizer,\n",
        "    clip_processor,\n",
        "    clip_model,\n",
        "    whisper_processor,\n",
        "    whisper_model,\n",
        "    input_image=None,\n",
        "    input_text=None,\n",
        "    input_audio=None,\n",
        "    max_length=512,\n",
        "    max_new_tokens=100,\n",
        "    device='cuda'  # Set your device here\n",
        "):\n",
        "    \"\"\"\n",
        "    Generates a response from the model based on image, text, or audio input.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        combined_prompt = \"You are a knowledgeable assistant. \"  # Initial prompt to set context\n",
        "\n",
        "        # Process image input if provided\n",
        "        if input_image is not None:\n",
        "            if isinstance(input_image, str):\n",
        "                image = Image.open(input_image).convert(\"RGB\")  # Ensure image is in the correct format\n",
        "            else:\n",
        "                image = input_image\n",
        "\n",
        "            image_inputs = clip_processor(images=image, return_tensors=\"pt\").to(device)\n",
        "            with torch.no_grad():\n",
        "                image_features = clip_model.get_image_features(**image_inputs)\n",
        "\n",
        "            # Add a contextual prompt for the image\n",
        "            combined_prompt += f\"Context: An image is provided. {image_features}Describe relevant features or subjects in the image. \"\n",
        "\n",
        "        # Process text input if provided\n",
        "        if input_text is not None:\n",
        "            combined_prompt += f\"User Question: {input_text} \"\n",
        "\n",
        "        # Process audio input if provided\n",
        "        if input_audio is not None:\n",
        "            audio_inputs = whisper_processor(input_audio, return_tensors=\"pt\").input_features.to(device)\n",
        "            with torch.no_grad():\n",
        "                audio_outputs = whisper_model.generate(audio_inputs)\n",
        "                transcription = whisper_processor.decode(audio_outputs[0])\n",
        "\n",
        "            combined_prompt += f\"Transcription from audio: {transcription} \"\n",
        "\n",
        "        # Finalize the prompt\n",
        "        combined_prompt += \"Please provide a detailed answer based on the above context.\"\n",
        "\n",
        "        print(combined_prompt)\n",
        "\n",
        "        # Tokenize combined prompt with image context\n",
        "        inputs = tokenizer(\n",
        "            combined_prompt,\n",
        "            return_tensors=\"pt\",\n",
        "            truncation=True,\n",
        "            max_length=max_length,\n",
        "            return_attention_mask=True,\n",
        "            padding=True\n",
        "        ).to(device)\n",
        "\n",
        "        # Generate output\n",
        "        with torch.no_grad():\n",
        "            outputs = model.generate(\n",
        "                input_ids=inputs[\"input_ids\"],\n",
        "                attention_mask=inputs[\"attention_mask\"],\n",
        "                max_new_tokens=max_new_tokens,\n",
        "                do_sample=True,\n",
        "                temperature=0.7,\n",
        "                num_return_sequences=1,\n",
        "            )\n",
        "\n",
        "        # Decode and return the response\n",
        "        response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "        # Clean up memory\n",
        "        del inputs, outputs\n",
        "        gc.collect()\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "        return response\n",
        "\n",
        "    except Exception as e:\n",
        "        raise Exception(f\"Error in generate_response: {str(e)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jF_ZDHZazO8T"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.nn import functional as F\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, CLIPProcessor, CLIPModel\n",
        "from transformers import WhisperProcessor, WhisperForConditionalGeneration\n",
        "# Import the 'clip' module\n",
        "from transformers import CLIPTokenizer, CLIPTextModel # Import CLIPTokenizer and CLIPTextModel\n",
        "from PIL import Image\n",
        "import gc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B6OtXLjm8Xui"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.nn import functional as F\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, CLIPProcessor, CLIPModel, WhisperProcessor, WhisperForConditionalGeneration\n",
        "from PIL import Image\n",
        "import gc\n",
        "\n",
        "import torch\n",
        "from torch.nn import functional as F\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, CLIPProcessor, CLIPModel, WhisperProcessor, WhisperForConditionalGeneration\n",
        "from PIL import Image\n",
        "import gc\n",
        "\n",
        "def generate_response(\n",
        "    model,\n",
        "    tokenizer,\n",
        "    clip_processor,\n",
        "    clip_model,\n",
        "    whisper_processor,\n",
        "    whisper_model,\n",
        "    input_image=None,\n",
        "    input_text=None,\n",
        "    input_audio=None,\n",
        "    max_length=512,\n",
        "    max_new_tokens=100,\n",
        "    device='cuda'\n",
        "):\n",
        "    \"\"\"\n",
        "    Generates a response from the model based on image, text, or audio input.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Initialize prompt components\n",
        "        prompt_parts = [\"You are a knowledgeable assistant.\"]\n",
        "\n",
        "        # Process image input if provided\n",
        "        if input_image is not None:\n",
        "            if isinstance(input_image, str):\n",
        "                image = Image.open(input_image).convert(\"RGB\")\n",
        "            else:\n",
        "                image = input_image\n",
        "\n",
        "            # Generate image features with CLIP\n",
        "            image_features = generate_image_features(clip_model, clip_processor, image, device)\n",
        "\n",
        "\n",
        "            prompt_parts.append(f\"Context: The following image features are provided: [{image_features}...]\")\n",
        "\n",
        "        # Process text input if provided\n",
        "        if input_text is not None:\n",
        "            prompt_parts.append(f\"User Question: {input_text}\")\n",
        "\n",
        "        # Process audio input if provided\n",
        "        if input_audio is not None:\n",
        "            audio_inputs = whisper_processor(input_audio, return_tensors=\"pt\").input_features.to(device)\n",
        "            with torch.no_grad():\n",
        "                audio_outputs = whisper_model.generate(audio_inputs)\n",
        "                transcription = whisper_processor.decode(audio_outputs[0])\n",
        "            prompt_parts.append(f\"Transcription from audio: {transcription}\")\n",
        "\n",
        "        # Combine all parts with proper spacing\n",
        "        prompt_parts.append(\"Please provide a detailed answer based on the above context.\")\n",
        "        combined_prompt = \" \".join(prompt_parts)\n",
        "        print(combined_prompt)\n",
        "        # Tokenize the prompt\n",
        "        inputs = tokenizer(\n",
        "            combined_prompt,\n",
        "            return_tensors=\"pt\",\n",
        "            truncation=True,\n",
        "            max_length=max_length,\n",
        "            return_attention_mask=True,\n",
        "            padding=True,\n",
        "            add_special_tokens=True\n",
        "        ).to(device)\n",
        "\n",
        "        # Generate response\n",
        "        with torch.no_grad():\n",
        "            outputs = model.generate(\n",
        "                input_ids=inputs[\"input_ids\"],\n",
        "                attention_mask=inputs[\"attention_mask\"],\n",
        "                max_new_tokens=max_new_tokens,\n",
        "                do_sample=True,\n",
        "                temperature=0.7,\n",
        "                top_p=0.9,\n",
        "                repetition_penalty=1.2,\n",
        "                num_return_sequences=1,\n",
        "            )\n",
        "\n",
        "        # Decode and clean up response\n",
        "        response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "        response = response.replace(combined_prompt, \"\").strip()\n",
        "\n",
        "        # Memory cleanup\n",
        "        del inputs, outputs\n",
        "        gc.collect()\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "        return response\n",
        "\n",
        "    except Exception as e:\n",
        "        raise Exception(f\"Error in generate_response: {str(e)}\")\n",
        "\n",
        "def generate_image_features(clip_model, clip_processor, image, device):\n",
        "    \"\"\"\n",
        "    Generates a compact feature vector of an image using CLIP.\n",
        "    \"\"\"\n",
        "    # Process the image with CLIP to get feature vector\n",
        "    image_inputs = clip_processor(images=image, return_tensors=\"pt\").to(device)\n",
        "    with torch.no_grad():\n",
        "        image_features = clip_model.get_image_features(**image_inputs)\n",
        "\n",
        "        # Normalize and reduce features to a compact string representation\n",
        "        image_features_normalized = F.normalize(image_features, p=2, dim=1)\n",
        "        features_str = \",\".join([f\"{val:.2f}\" for val in image_features_normalized[0][:10].cpu().numpy()])\n",
        "\n",
        "    return features_str\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6LlSM9UK09Qh"
      },
      "outputs": [],
      "source": [
        "output_dir = \"/content/drive/MyDrive/LLM_training_4hours_16 November/final_model\"\n",
        "#output_dir = \"/content/drive/MyDrive/LLM_training_output_1Nov2024\"\n",
        "#Load models first\n",
        "tokenizer, model, clip_processor, clip_model, whisper_processor, whisper_model = load_models(output_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VZy4KAB-oqx6"
      },
      "outputs": [],
      "source": [
        "# model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a6tYLxE3lVHz"
      },
      "outputs": [],
      "source": [
        "generate_response(model, tokenizer, clip_processor, clip_model, whisper_processor, whisper_model, input_image=\"/content/test.jpg\", input_text=\"describe the image?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "StLgUs88ke9M"
      },
      "outputs": [],
      "source": [
        "# image_features = clip_model.get_image_features(**input_image)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kfJjFqTkkl8d"
      },
      "outputs": [],
      "source": [
        "input_image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UIc0EyIYktJs"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": [],
      "mount_file_id": "1xln3GyNzcPBsTUgyuTFp7oHZDzWcXTVZ",
      "authorship_tag": "ABX9TyPei6jzQHM8R+sLKWOOVUiY",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "6d7aaa66181c47a3a255e918f1a351f9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_35261735059b46cb9e665b3f57ed1638",
              "IPY_MODEL_d33360c826ca410687d10fe9518df695",
              "IPY_MODEL_9e27d5f3f7074e86ba6ced840ebed2e2"
            ],
            "layout": "IPY_MODEL_1b410077c5f840e7929918c32ab31918"
          }
        },
        "35261735059b46cb9e665b3f57ed1638": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7f78c19d9fd841b6ac3caac2b8c36016",
            "placeholder": "​",
            "style": "IPY_MODEL_974ba1bb9bfa4e6da7228c81db6b7041",
            "value": "Map: 100%"
          }
        },
        "d33360c826ca410687d10fe9518df695": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_109d967faa57475c852b50ce0bd48fe9",
            "max": 157712,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_257f06a762a543c9ad5d7fc4f4f0c1db",
            "value": 157712
          }
        },
        "9e27d5f3f7074e86ba6ced840ebed2e2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2e7aa4b0ee9c4fe2a188cdee5907bdb4",
            "placeholder": "​",
            "style": "IPY_MODEL_e516c005ea924537a61b04fa1dcd7da5",
            "value": " 157712/157712 [08:08&lt;00:00, 405.57 examples/s]"
          }
        },
        "1b410077c5f840e7929918c32ab31918": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7f78c19d9fd841b6ac3caac2b8c36016": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "974ba1bb9bfa4e6da7228c81db6b7041": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "109d967faa57475c852b50ce0bd48fe9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "257f06a762a543c9ad5d7fc4f4f0c1db": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "2e7aa4b0ee9c4fe2a188cdee5907bdb4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e516c005ea924537a61b04fa1dcd7da5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "27acfcb2e9da42f1927d52ba0cc09ed2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_8223db5e9dd04e38b8beb2aea02e6ddd",
              "IPY_MODEL_8fdd458d1a7242aab26f42a621aedc58",
              "IPY_MODEL_87adba404ada475cb0c3e558ffae7a8d"
            ],
            "layout": "IPY_MODEL_ed0e7c7375244bbb8f587ede64107a9a"
          }
        },
        "8223db5e9dd04e38b8beb2aea02e6ddd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4065d5015ffd404789d578c95080346d",
            "placeholder": "​",
            "style": "IPY_MODEL_12b08aa13ecb493581bd67f03f52cdcb",
            "value": "Saving the dataset (3/3 shards): 100%"
          }
        },
        "8fdd458d1a7242aab26f42a621aedc58": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_dc93a8ab89284a63836ca637d5509656",
            "max": 289128,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f902895480d44332895686d0c654ba2b",
            "value": 289128
          }
        },
        "87adba404ada475cb0c3e558ffae7a8d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_101583b995d94c6c89e4e6948f14f63b",
            "placeholder": "​",
            "style": "IPY_MODEL_bcbbdea3604f4c7b8e158d832e477762",
            "value": " 289128/289128 [00:09&lt;00:00, 34179.72 examples/s]"
          }
        },
        "ed0e7c7375244bbb8f587ede64107a9a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4065d5015ffd404789d578c95080346d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "12b08aa13ecb493581bd67f03f52cdcb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "dc93a8ab89284a63836ca637d5509656": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f902895480d44332895686d0c654ba2b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "101583b995d94c6c89e4e6948f14f63b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bcbbdea3604f4c7b8e158d832e477762": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "80e63d9e28c4486383e45322fd57d967": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_93e98e54b69f467d8d4a134e0832a546",
              "IPY_MODEL_25411fb908c6492c87d7fcf08c9425f6",
              "IPY_MODEL_a79067d66eba458f941c6d5967d8c9d8"
            ],
            "layout": "IPY_MODEL_9c3a08b3ea3b4ee4a36cd7c5d7b09128"
          }
        },
        "93e98e54b69f467d8d4a134e0832a546": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_18ae8b4391de49b2a6ab988b913d11f2",
            "placeholder": "​",
            "style": "IPY_MODEL_ce02ac39215d4db9bf10759dd84aeb7c",
            "value": "Saving the dataset (1/1 shards): 100%"
          }
        },
        "25411fb908c6492c87d7fcf08c9425f6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_69bcfd7076fa4a849ebc4af42f62af7d",
            "max": 36142,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_59288ec589a642be90d624c205b37b13",
            "value": 36142
          }
        },
        "a79067d66eba458f941c6d5967d8c9d8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fbc60ee4041f4d0a82b5cd423a104512",
            "placeholder": "​",
            "style": "IPY_MODEL_c86f05b3af0f48a9bc44a83c5c18197c",
            "value": " 36142/36142 [00:01&lt;00:00, 36115.57 examples/s]"
          }
        },
        "9c3a08b3ea3b4ee4a36cd7c5d7b09128": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "18ae8b4391de49b2a6ab988b913d11f2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ce02ac39215d4db9bf10759dd84aeb7c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "69bcfd7076fa4a849ebc4af42f62af7d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "59288ec589a642be90d624c205b37b13": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "fbc60ee4041f4d0a82b5cd423a104512": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c86f05b3af0f48a9bc44a83c5c18197c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "380cb338697b44f3b734d4f5ffd986dd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_dc669d09a8ab40a1994e5a56dd6f3ba7",
              "IPY_MODEL_569b1719b9af4daab79436de0e8ed30d",
              "IPY_MODEL_e5463e0057cf4d6aa55c9b176dda660d"
            ],
            "layout": "IPY_MODEL_eecb00c38bab4ecb84704f87afae668a"
          }
        },
        "dc669d09a8ab40a1994e5a56dd6f3ba7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_75c699d676f54d328f69e3c3cf625aa3",
            "placeholder": "​",
            "style": "IPY_MODEL_0fe44843bb8144ff83d38f43e4956b3f",
            "value": "Saving the dataset (1/1 shards): 100%"
          }
        },
        "569b1719b9af4daab79436de0e8ed30d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1bae64225ade46688df8510b1a813c92",
            "max": 36141,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_ea8701c80ab64db2b22ac28bf6c20212",
            "value": 36141
          }
        },
        "e5463e0057cf4d6aa55c9b176dda660d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c63ed592ec98462baafe69fcb3ffc577",
            "placeholder": "​",
            "style": "IPY_MODEL_0d491d3d6e554ddf800dc29e26c8b0eb",
            "value": " 36141/36141 [00:01&lt;00:00, 36449.35 examples/s]"
          }
        },
        "eecb00c38bab4ecb84704f87afae668a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "75c699d676f54d328f69e3c3cf625aa3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0fe44843bb8144ff83d38f43e4956b3f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1bae64225ade46688df8510b1a813c92": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ea8701c80ab64db2b22ac28bf6c20212": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "c63ed592ec98462baafe69fcb3ffc577": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0d491d3d6e554ddf800dc29e26c8b0eb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "92b375a579344c9ebd1a80d284271593": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_db1950791ccb409d95711555b96ef4c9",
              "IPY_MODEL_298a568c21dc4255b13d48b83497bfd1",
              "IPY_MODEL_d1b42be7b37c47a38c4e964e16b7792d"
            ],
            "layout": "IPY_MODEL_0473a81c8318486ba60fb09416158070"
          }
        },
        "db1950791ccb409d95711555b96ef4c9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bf3906f632754f3f8c2b0cffe09f6879",
            "placeholder": "​",
            "style": "IPY_MODEL_506754b3ca194e929036d56b94f11c00",
            "value": "config.json: 100%"
          }
        },
        "298a568c21dc4255b13d48b83497bfd1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b39e9fc259284395933afc3596b1742f",
            "max": 3484,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f9b5626218d94eafbd67ab2a542f6654",
            "value": 3484
          }
        },
        "d1b42be7b37c47a38c4e964e16b7792d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d4f58827f6654a65b34b33deb78a9c5e",
            "placeholder": "​",
            "style": "IPY_MODEL_1b463739325a4c1b8357b33c94ed4686",
            "value": " 3.48k/3.48k [00:00&lt;00:00, 297kB/s]"
          }
        },
        "0473a81c8318486ba60fb09416158070": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bf3906f632754f3f8c2b0cffe09f6879": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "506754b3ca194e929036d56b94f11c00": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b39e9fc259284395933afc3596b1742f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f9b5626218d94eafbd67ab2a542f6654": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "d4f58827f6654a65b34b33deb78a9c5e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1b463739325a4c1b8357b33c94ed4686": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d454bddf8abf477498f4a9959723bc54": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_85f9c7c79d2d478aa290870dc0772306",
              "IPY_MODEL_0f3778db19224d0d83e3ba1d315760d4",
              "IPY_MODEL_185284391bbb4415a572d75cde220789"
            ],
            "layout": "IPY_MODEL_dcf2040b003f4b5f9890432af29e2571"
          }
        },
        "85f9c7c79d2d478aa290870dc0772306": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_06a085b848ac411a971f2b0f5e4d2bd4",
            "placeholder": "​",
            "style": "IPY_MODEL_5b5dcef51bb1444da05a393718a91e0c",
            "value": "model.safetensors.index.json: 100%"
          }
        },
        "0f3778db19224d0d83e3ba1d315760d4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bbfe0737db6c42dca56c45807dbb72ce",
            "max": 16331,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_abf0ce9be78040cf849ddada9acc6c36",
            "value": 16331
          }
        },
        "185284391bbb4415a572d75cde220789": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9f08ca5f4a334f7bb63fa5da4e1648a5",
            "placeholder": "​",
            "style": "IPY_MODEL_1234608895db412abf4340dd4708d30a",
            "value": " 16.3k/16.3k [00:00&lt;00:00, 1.40MB/s]"
          }
        },
        "dcf2040b003f4b5f9890432af29e2571": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "06a085b848ac411a971f2b0f5e4d2bd4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5b5dcef51bb1444da05a393718a91e0c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "bbfe0737db6c42dca56c45807dbb72ce": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "abf0ce9be78040cf849ddada9acc6c36": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "9f08ca5f4a334f7bb63fa5da4e1648a5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1234608895db412abf4340dd4708d30a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6677bda76c3f4446bb4d3be8ea18a028": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_4f4653cf13cb4b0bb6d4ea2e1d102a8e",
              "IPY_MODEL_a7d8c2e126754a7f9ef0bee2e5b8fce6",
              "IPY_MODEL_ddd36600c7894886992c36a03db0e2fa"
            ],
            "layout": "IPY_MODEL_072dc8800efb4211857dba238342b72c"
          }
        },
        "4f4653cf13cb4b0bb6d4ea2e1d102a8e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6be18d89ea4149b982a12ecbc8fb3d5d",
            "placeholder": "​",
            "style": "IPY_MODEL_054bda0db6914fb28fc260f67da4a647",
            "value": "Downloading shards: 100%"
          }
        },
        "a7d8c2e126754a7f9ef0bee2e5b8fce6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b5b5a8697c7f4745b2ce5cdecaba6eb1",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_cbce0649a05e4111a8d7d14b6b5babf7",
            "value": 2
          }
        },
        "ddd36600c7894886992c36a03db0e2fa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5f23d9f673804fd4a8d6f817786edd78",
            "placeholder": "​",
            "style": "IPY_MODEL_8bbcbe35315341e3a3b06a520ba4d310",
            "value": " 2/2 [03:06&lt;00:00, 88.01s/it]"
          }
        },
        "072dc8800efb4211857dba238342b72c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6be18d89ea4149b982a12ecbc8fb3d5d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "054bda0db6914fb28fc260f67da4a647": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b5b5a8697c7f4745b2ce5cdecaba6eb1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cbce0649a05e4111a8d7d14b6b5babf7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "5f23d9f673804fd4a8d6f817786edd78": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8bbcbe35315341e3a3b06a520ba4d310": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "02af28b834d04222a53730e148744830": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_43a9388b06f3472eb3f86643f1caf242",
              "IPY_MODEL_f18bde59b7684a0ba4528f7adcd11f5f",
              "IPY_MODEL_4a91dc8bdb85419c8cd0fc3bb6fcd0bd"
            ],
            "layout": "IPY_MODEL_861fb7e0707c4c6fa3efd8320611fe36"
          }
        },
        "43a9388b06f3472eb3f86643f1caf242": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d7b509ba810a46229b145e838dccda7e",
            "placeholder": "​",
            "style": "IPY_MODEL_794ef5cb56964e7f9a6fb070397baae0",
            "value": "model-00001-of-00002.safetensors: 100%"
          }
        },
        "f18bde59b7684a0ba4528f7adcd11f5f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_047d593c11854733919685c03b0a3f13",
            "max": 4972489328,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_64445af2c7f3453885bf92628e68f5fe",
            "value": 4972489328
          }
        },
        "4a91dc8bdb85419c8cd0fc3bb6fcd0bd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9bfe1302849f45bc8df53ce3d22ab95a",
            "placeholder": "​",
            "style": "IPY_MODEL_c60017a43cca4521b673ab9f40ebd530",
            "value": " 4.97G/4.97G [02:01&lt;00:00, 42.7MB/s]"
          }
        },
        "861fb7e0707c4c6fa3efd8320611fe36": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d7b509ba810a46229b145e838dccda7e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "794ef5cb56964e7f9a6fb070397baae0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "047d593c11854733919685c03b0a3f13": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "64445af2c7f3453885bf92628e68f5fe": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "9bfe1302849f45bc8df53ce3d22ab95a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c60017a43cca4521b673ab9f40ebd530": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b864e58b82044b1592c7c79a3aa694aa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_ee1f54c6145e46a082deeba8e0104305",
              "IPY_MODEL_b4673dd7a901469994e483310509d593",
              "IPY_MODEL_3de779a2189a4fa0abac212ecb8882c5"
            ],
            "layout": "IPY_MODEL_e57dfca132634c8b962c2975434d5ad8"
          }
        },
        "ee1f54c6145e46a082deeba8e0104305": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_451af528edef4907b63bad2a3e3d081f",
            "placeholder": "​",
            "style": "IPY_MODEL_ef1e863d054a457690b8b6322ab369f4",
            "value": "model-00002-of-00002.safetensors: 100%"
          }
        },
        "b4673dd7a901469994e483310509d593": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e7a00d5e83bb4fd08f1eea5296088121",
            "max": 2669692552,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_4b01f89280134b5187b2a5bba0c7c50b",
            "value": 2669692552
          }
        },
        "3de779a2189a4fa0abac212ecb8882c5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_06d22b21b7ab43dca5fb71624eb1a3e7",
            "placeholder": "​",
            "style": "IPY_MODEL_d7478d58fcbf4501bca807a02215e112",
            "value": " 2.67G/2.67G [01:03&lt;00:00, 41.1MB/s]"
          }
        },
        "e57dfca132634c8b962c2975434d5ad8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "451af528edef4907b63bad2a3e3d081f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ef1e863d054a457690b8b6322ab369f4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e7a00d5e83bb4fd08f1eea5296088121": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4b01f89280134b5187b2a5bba0c7c50b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "06d22b21b7ab43dca5fb71624eb1a3e7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d7478d58fcbf4501bca807a02215e112": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "da474d34a373411f812d58b8a55cd71f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_271d5224832e4b8482d2edd0bcc7745e",
              "IPY_MODEL_f5bde914c781436bb8e555fb92836cd3",
              "IPY_MODEL_785f755459ae4f2abddf0b9b2d6c30bc"
            ],
            "layout": "IPY_MODEL_eb7b0ac5494745af89e3ef6835208757"
          }
        },
        "271d5224832e4b8482d2edd0bcc7745e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1d7764b1bdbd4a1dace248ec0d0a6b6e",
            "placeholder": "​",
            "style": "IPY_MODEL_c4a809c2126a44e5a7c91bf4af841c3e",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "f5bde914c781436bb8e555fb92836cd3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_619cd281a8064a3b8ddd250bc0ffa6ec",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_92b65507dbc2471caac95719d19a37a7",
            "value": 2
          }
        },
        "785f755459ae4f2abddf0b9b2d6c30bc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ef8232e1e6e9408b9d11de087ff4c052",
            "placeholder": "​",
            "style": "IPY_MODEL_0c7277954f8b4eb8b611f299515b75c8",
            "value": " 2/2 [00:06&lt;00:00,  2.88s/it]"
          }
        },
        "eb7b0ac5494745af89e3ef6835208757": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1d7764b1bdbd4a1dace248ec0d0a6b6e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c4a809c2126a44e5a7c91bf4af841c3e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "619cd281a8064a3b8ddd250bc0ffa6ec": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "92b65507dbc2471caac95719d19a37a7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "ef8232e1e6e9408b9d11de087ff4c052": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0c7277954f8b4eb8b611f299515b75c8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "be61f4a5ab2e4e7192116cfd73787fac": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_47838e803bda45f1ad93011806c84fae",
              "IPY_MODEL_54cce308c82a4917a6c74c808b2bef91",
              "IPY_MODEL_42ec8c1be3e24e63a9f7b6daff39ad66"
            ],
            "layout": "IPY_MODEL_f39ac95894374f3da2ef259d2a536ed0"
          }
        },
        "47838e803bda45f1ad93011806c84fae": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_af2a5096fd0d4d08a02025e7bd0c95de",
            "placeholder": "​",
            "style": "IPY_MODEL_15c993dafa074a569dcea64196b4a75a",
            "value": "generation_config.json: 100%"
          }
        },
        "54cce308c82a4917a6c74c808b2bef91": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2995dcb310ea4b359acd0d3903cf5ce9",
            "max": 181,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_c3dbab463d3b433aa2dbbf99443f9325",
            "value": 181
          }
        },
        "42ec8c1be3e24e63a9f7b6daff39ad66": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7f227a4e23e74dad9cf78f0e57d58064",
            "placeholder": "​",
            "style": "IPY_MODEL_df56346994f343ee923b2f8c46111144",
            "value": " 181/181 [00:00&lt;00:00, 16.2kB/s]"
          }
        },
        "f39ac95894374f3da2ef259d2a536ed0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "af2a5096fd0d4d08a02025e7bd0c95de": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "15c993dafa074a569dcea64196b4a75a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2995dcb310ea4b359acd0d3903cf5ce9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c3dbab463d3b433aa2dbbf99443f9325": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "7f227a4e23e74dad9cf78f0e57d58064": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "df56346994f343ee923b2f8c46111144": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d11735e7d99d4e46a0cef28a97e42c18": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_88365a49ff0e468dba014622519d9185",
              "IPY_MODEL_24a75d54ad014799aaa813c9dca0013f",
              "IPY_MODEL_e821f68a40ec4667bb12b23b6bc3385e"
            ],
            "layout": "IPY_MODEL_0722aa305d924d4faa57f00e5c05b6bc"
          }
        },
        "88365a49ff0e468dba014622519d9185": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_65897c21d0aa41c4b5946430dd1ccdd7",
            "placeholder": "​",
            "style": "IPY_MODEL_d62e8d99259b443f8e3f4b16085a6797",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "24a75d54ad014799aaa813c9dca0013f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_82778d5d838e44c6a0fe44ef0ecc7d4b",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_6c32e62d4d0d48f1b5c85a45838edecf",
            "value": 2
          }
        },
        "e821f68a40ec4667bb12b23b6bc3385e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4c217f25a90b4be1a69e87d05775f428",
            "placeholder": "​",
            "style": "IPY_MODEL_f00d373ee5a84a0bb938a1ddd0afe1cd",
            "value": " 2/2 [00:05&lt;00:00,  2.42s/it]"
          }
        },
        "0722aa305d924d4faa57f00e5c05b6bc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "65897c21d0aa41c4b5946430dd1ccdd7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d62e8d99259b443f8e3f4b16085a6797": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "82778d5d838e44c6a0fe44ef0ecc7d4b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6c32e62d4d0d48f1b5c85a45838edecf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "4c217f25a90b4be1a69e87d05775f428": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f00d373ee5a84a0bb938a1ddd0afe1cd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a033e2a7d3164866a112174783fc9e2b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e6dff8cd54584478bce37e69fb6b52d9",
              "IPY_MODEL_bcc92e9686794fa48a2d338eb50d42db",
              "IPY_MODEL_596c98baaf674db9baacffb3a6355273"
            ],
            "layout": "IPY_MODEL_ea8371e12adb4184a9d56437573a1e6e"
          }
        },
        "e6dff8cd54584478bce37e69fb6b52d9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bf6f386c6068461a979facff868b5104",
            "placeholder": "​",
            "style": "IPY_MODEL_af045d878fa5432a92cf9ec634c49231",
            "value": "configuration_phi3.py: 100%"
          }
        },
        "bcc92e9686794fa48a2d338eb50d42db": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0f89c66647944811adb285bdb534c265",
            "max": 11153,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_d0bfd853ba8e4fa4867b22d8b870b47c",
            "value": 11153
          }
        },
        "596c98baaf674db9baacffb3a6355273": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f7e142d668e04ed18f07d72b78d1f976",
            "placeholder": "​",
            "style": "IPY_MODEL_8942582682a44abab711b9fee8bf6b18",
            "value": " 11.2k/11.2k [00:00&lt;00:00, 988kB/s]"
          }
        },
        "ea8371e12adb4184a9d56437573a1e6e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bf6f386c6068461a979facff868b5104": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "af045d878fa5432a92cf9ec634c49231": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0f89c66647944811adb285bdb534c265": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d0bfd853ba8e4fa4867b22d8b870b47c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "f7e142d668e04ed18f07d72b78d1f976": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8942582682a44abab711b9fee8bf6b18": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "077b112e05bc4f49a53a832e58596d49": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_05c57802f87b43d7a9e5e7356702d0ba",
              "IPY_MODEL_d32fa8217f824f78b991d4fbc6d939ab",
              "IPY_MODEL_cb5ba2b9963a45229ee40bdfee1b8188"
            ],
            "layout": "IPY_MODEL_c5b3efdd635b4e889de0fba11329a93b"
          }
        },
        "05c57802f87b43d7a9e5e7356702d0ba": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b3e87413791a4d8f9dc9c7451124d482",
            "placeholder": "​",
            "style": "IPY_MODEL_f24b1eef8cbc461a82289101dea6b522",
            "value": "modeling_phi3.py: 100%"
          }
        },
        "d32fa8217f824f78b991d4fbc6d939ab": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4370d76f368a4b5aa76e2b6624d1cb82",
            "max": 73193,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_ee74cfda88954a739c4262fe5d80c525",
            "value": 73193
          }
        },
        "cb5ba2b9963a45229ee40bdfee1b8188": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7aa9c85fba9c4b0799aad64536d4b8cc",
            "placeholder": "​",
            "style": "IPY_MODEL_8e4785d1fb924b70b0ad231fe8f6cff7",
            "value": " 73.2k/73.2k [00:00&lt;00:00, 323kB/s]"
          }
        },
        "c5b3efdd635b4e889de0fba11329a93b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b3e87413791a4d8f9dc9c7451124d482": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f24b1eef8cbc461a82289101dea6b522": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4370d76f368a4b5aa76e2b6624d1cb82": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ee74cfda88954a739c4262fe5d80c525": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "7aa9c85fba9c4b0799aad64536d4b8cc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8e4785d1fb924b70b0ad231fe8f6cff7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2c52db0e9029419dbccfa1ff90a4c008": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_8266ded3fc364ac0b74c86fa741dec8a",
              "IPY_MODEL_b4f0d4a00ca7424986661c32a862cfbd",
              "IPY_MODEL_d146359e3fc945daa7b803857cb8922e"
            ],
            "layout": "IPY_MODEL_d60a5a316c19479eb4c1ad01008f97c6"
          }
        },
        "8266ded3fc364ac0b74c86fa741dec8a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6bb74baf484b43dbaad159b8bc8a8d50",
            "placeholder": "​",
            "style": "IPY_MODEL_d4451334dd84476ab3bd426ec3d2c5e3",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "b4f0d4a00ca7424986661c32a862cfbd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cf1c2cecb8834ee1abaa6e06cdffa928",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_14c53d48014345578220caff0e81ca43",
            "value": 2
          }
        },
        "d146359e3fc945daa7b803857cb8922e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3cd0bad0755f44458d363af9b74bebb9",
            "placeholder": "​",
            "style": "IPY_MODEL_fd2536710e9041aa9dba4ec8ac1c5499",
            "value": " 2/2 [00:06&lt;00:00,  2.93s/it]"
          }
        },
        "d60a5a316c19479eb4c1ad01008f97c6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6bb74baf484b43dbaad159b8bc8a8d50": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d4451334dd84476ab3bd426ec3d2c5e3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "cf1c2cecb8834ee1abaa6e06cdffa928": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "14c53d48014345578220caff0e81ca43": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "3cd0bad0755f44458d363af9b74bebb9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fd2536710e9041aa9dba4ec8ac1c5499": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}